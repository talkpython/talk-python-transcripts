WEBVTT

00:00:01.079 --> 00:00:11.460
Hello, YouTube. Hello world. Hi, Vincent here. Also a very active on this year platform YouTube.

00:00:11.610 --> 00:00:16.280
We're going to talk a bit about that as we kick off the show. Vincent, are you ready to do the

00:00:16.340 --> 00:00:22.080
episode? Have I ever been? Yes. No, we're looking forward to it. Yeah, I'm looking forward to

00:00:22.200 --> 00:00:25.360
catching up with you and talking about this stuff some more. It's gonna be great.

00:00:26.680 --> 00:00:28.580
Sorry, the audio thing because I was watching the YouTube.

00:00:28.740 --> 00:00:29.420
Yes, hi, we're back.

00:00:29.660 --> 00:00:30.220
Hi, sorry.

00:00:30.460 --> 00:00:32.259
Hey, welcome to Talk Python To Me.

00:00:33.280 --> 00:00:33.520
Hi.

00:00:34.500 --> 00:00:34.880
Welcome back.

00:00:35.200 --> 00:00:35.720
Pleasure to be here.

00:00:36.060 --> 00:00:39.940
Yeah, it's been like, this is our second Talk Python interview, I think.

00:00:40.120 --> 00:00:41.920
We've interacted before also on socials and all that.

00:00:42.120 --> 00:00:43.080
But like, yeah, second time.

00:00:43.340 --> 00:00:43.420
Yay.

00:00:44.040 --> 00:00:44.380
Happy to be here.

00:00:44.460 --> 00:00:44.920
Yay, yeah.

00:00:45.300 --> 00:00:47.180
It's very exciting to have you back.

00:00:48.200 --> 00:00:49.180
You know, I'm trying to remember.

00:00:49.240 --> 00:00:56.260
I did a blog post on the most popular episodes of last year.

00:00:56.700 --> 00:00:59.040
And I think, if I do recall correctly.

00:01:00.400 --> 00:01:04.580
Yeah, the number one last year was our interaction, I think, on space.

00:01:04.640 --> 00:01:06.200
Yep, that's not terrible, is it?

00:01:06.920 --> 00:01:08.720
Yeah, it was a bragging right for a day.

00:01:09.720 --> 00:01:10.560
Yeah, yeah, it's pretty cool.

00:01:10.720 --> 00:01:35.800
So your episode, and double bragging rights, as your episode was released in, I don't know when that was, something like October or something, almost exactly a year ago to the day. And still, it was the number one most downloaded for the whole year. So fantastic. And I think that might be an indication of where we're going to another very exciting topic, you know?

00:01:36.940 --> 00:01:40.500
Yeah, like ML and AI only went up, right? So, so.

00:01:42.360 --> 00:01:53.440
You know what, I would love to get your thoughts on this. I, I'm seeing out on the internet, And I want to put this up as kind of a, to counter this thought or whatever.

00:01:54.300 --> 00:01:59.140
But I'm seeing a lot of people get really frustrated that AI is a thing.

00:02:00.340 --> 00:02:12.380
You know, OpenAI just released their Atlas browser, which I'm not super excited about, but it's like their version of what an AI browser wrapping Chromium looks like.

00:02:12.540 --> 00:02:12.980
You know what I mean?

00:02:13.220 --> 00:02:13.780
It's fine.

00:02:14.280 --> 00:02:18.300
but the comments in Ars Technica, which are usually pretty balanced,

00:02:18.520 --> 00:02:22.360
are just like people losing it over the fact that it has AI.

00:02:22.920 --> 00:02:24.080
Are people tired of this stuff?

00:02:24.420 --> 00:02:27.940
Or why do you think that's the reaction, I guess, is really what I want to ask you.

00:02:30.380 --> 00:02:42.400
I mean, I guess I have a couple of perspectives, but I think I could say we are all tired of every tech product out there is trying its best to find a way to put AI in the product.

00:02:43.020 --> 00:02:47.060
oh, any Gmail, any calendar app, let's put AI in there.

00:02:47.180 --> 00:02:47.940
And it gets a bit tiring.

00:02:48.060 --> 00:02:52.700
I wouldn't be too surprised if fire hoses and refrigerators also come with AI features these days.

00:02:52.840 --> 00:02:54.040
It feels a bit nuts.

00:02:54.360 --> 00:02:54.760
I know.

00:02:54.800 --> 00:02:58.000
It's like, what is my refrigerator AI for?

00:02:58.400 --> 00:03:01.160
Yeah, or like soccer ball, whatever object.

00:03:01.360 --> 00:03:03.240
It feels like people are overdoing the AI thing.

00:03:03.260 --> 00:03:06.000
So, okay, that can be a legitimate source of frustration.

00:03:07.200 --> 00:03:23.900
But then it also becomes a little bit personal because I can also imagine if you put your heart and soul to being a good engineer and you want to take the internet serious, you consider it something that's somewhat sacred because it helps you so much in your career, then I definitely can also imagine, oh my god, please don't turn that into a slop.

00:03:24.200 --> 00:03:25.400
I built a career on that.

00:03:25.940 --> 00:03:27.440
So that might be the source of all these things.

00:03:28.100 --> 00:03:35.220
I guess my main way of dealing with it is more along the lines of, okay, this genie's not going to go back into the bottle in a way.

00:03:36.140 --> 00:03:47.840
But if we're able to do more things with these LLMs, if they're able to automate more boilerplate and that sort of a thing, And then it seems that then I can try and get better at natural intelligence as well instead of this artificial intelligence.

00:03:48.080 --> 00:03:50.620
So like, OK, I can Vibe code all these things now.

00:03:51.240 --> 00:03:55.700
So the bottleneck is just do I have good ideas and maybe I should invest in good ideas.

00:03:56.020 --> 00:04:00.780
And oh, maybe I can actually learn about JavaScript, but then I should not focus on the syntax.

00:04:00.880 --> 00:04:03.980
But I shouldn't focus maybe on the browser and like how to CSS work.

00:04:05.900 --> 00:04:15.220
oh, I should actually maybe do a little bit of stuff with flashcards just so I know the syntax just enough so I can review. Okay. I try to be very conscious about it that way. That's kind of

00:04:15.240 --> 00:04:30.620
my approach more. It's easy to get really lazy, right? And just push the button and say, do the next thing and not use it as an opportunity. Like, oh, it did something I didn't know. Let me have a conversation and try to understand this thing I didn't know. I think the key word here is you want

00:04:30.620 --> 00:04:48.200
to be deliberate about it. I think if you can sort of say, okay, I'm deliberate about the way I use this stuff, and I'm also learning as I'm doing this. One thing I really like to do is just give some sort of bonkers front-end task to the LLM that I have no idea how I would solve it, and then study the output. That's a thing I actually do once in a while.

00:04:50.150 --> 00:04:58.580
But yeah, I do get it that people have mixed emotions about it, and that's totally cool and fine. It's just that for me, in my situation, this is how I deal with it.

00:04:59.620 --> 00:05:25.480
Yeah, I think that's fair. I think also I definitely have long since been tired of like every email app having to rewrite this with AI. And it usually destroys the email, like removes all the formatting. And then you just, you know, there's always the cartoon of like, I wrote an email as bullet points. I asked AI to expand it. I sent it to you. You're like, what a long email. I'm going to ask AI to summarize this. We should just send the bullet points.

00:05:26.880 --> 00:05:31.360
Yeah, I mean, I do fear the amount of slop, though.

00:05:32.260 --> 00:05:40.340
It almost feels like every good tech YouTuber, there aren't a lot of good tech YouTubers anymore because they're all doing AI stuff instead of doing actual really great programming.

00:05:40.500 --> 00:05:41.040
There's still a few.

00:05:41.920 --> 00:05:44.180
Anthony writes code is a really good one still.

00:05:44.560 --> 00:05:45.080
He's awesome.

00:05:45.820 --> 00:05:47.600
Definitely follow him if you want to learn Python.

00:05:47.760 --> 00:05:48.580
He does cool stuff.

00:05:50.460 --> 00:06:01.300
But yeah, the main thing, I guess, from my selfish perspective, YouTube used to be better because nowadays it's all about the AI and you've always got this thumbnail of a guy pulling his hair out like, oh my god, this changes everything.

00:06:01.880 --> 00:06:03.960
I would love to see a video that's maybe not that.

00:06:04.740 --> 00:06:07.000
Have you heard that there's an AI bubble that's going to burst?

00:06:09.120 --> 00:06:10.680
We'll see exactly how far that goes.

00:06:10.680 --> 00:06:11.520
I don't think so, actually.

00:06:11.870 --> 00:06:14.940
I mean, we can speculate.

00:06:15.520 --> 00:06:17.540
I can argue that it's been over-invested.

00:06:17.620 --> 00:06:19.180
I can also argue there's still so much potential.

00:06:19.740 --> 00:06:25.500
The one thing I will say, though, we have a lot of kinds of tools and we're all looking forward to the vibe coding.

00:06:25.590 --> 00:06:27.040
Oh, it's never been easier and that sort of thing.

00:06:27.530 --> 00:06:31.100
But I would have expected an explosion of awesome apps to be going along with it.

00:06:31.150 --> 00:06:36.680
And it doesn't necessarily feel like we have much better software, even though we supposedly have way better tools.

00:06:36.810 --> 00:06:43.840
So something about that is making me a little bit suspicious, but it might also just be that the apps that are being built that I'm not the target audience for.

00:06:44.440 --> 00:06:45.420
I can imagine that.

00:06:45.540 --> 00:06:52.300
Because let's say you have something awesome for dentists that can still be an awesome crud app that you could build with Claude, but I'm not a dentist.

00:06:53.640 --> 00:06:57.700
Wow, there's a lot of new cool dentistry management apps.

00:06:57.870 --> 00:06:58.280
You're right.

00:06:58.660 --> 00:06:59.460
I haven't noticed those.

00:06:59.460 --> 00:07:00.720
If a dentist can outprogram, right?

00:07:00.740 --> 00:07:06.840
Like, I do believe in the story that, oh, as a dentist, you might be best equipped to write an app for dentists, right?

00:07:07.340 --> 00:07:17.440
And if they're now more empowered to maybe do some stuff with code, I mean, there's a story there that every single niche profession will have someone who can do enough clod to make the app for that profession.

00:07:18.620 --> 00:07:18.980
Yeah.

00:07:19.360 --> 00:07:19.420
Yeah.

00:07:19.860 --> 00:07:20.640
Well, time will tell.

00:07:20.660 --> 00:07:29.320
It used to be that you have to have a programming skill and a little bit of, let's say, dentistry experience to build the right sort of specialized vertical.

00:07:29.580 --> 00:07:30.820
And now I think it maybe is reversed.

00:07:31.960 --> 00:07:36.340
You need a lot of dentistry and a little bit of coding skill to go along with an AI.

00:07:37.020 --> 00:07:37.160
Yeah.

00:07:38.240 --> 00:07:40.840
The character sheet used to be 80 points here, 20 points there.

00:07:40.980 --> 00:07:43.080
And now it's 80 points there and 20 points here.

00:07:43.240 --> 00:07:43.560
Yes, exactly.

00:07:43.940 --> 00:07:45.020
That's exactly what I was thinking.

00:07:45.160 --> 00:07:47.780
I think maybe that's actually shifted until you said that.

00:07:47.860 --> 00:07:49.760
I've never really thought about it, but it just may be.

00:07:50.300 --> 00:08:01.420
I do want to point out for people listening, we're not going to really talk too much about using agentic tools to write code for us.

00:08:01.520 --> 00:08:04.380
We have been speculating about this theoretical dentist.

00:08:05.860 --> 00:08:25.080
But what instead we're going to do is we're going to talk to you, Vincent, about how can I use an LLM like a library or an API to add functionality, features, behaviors to an existing data science tool or to an existing web app or whatever it is we're building, right?

00:08:25.820 --> 00:08:26.080
Yes.

00:08:27.640 --> 00:08:34.000
Also, people might probably know it, but I made a course for Talk Python, of course, and we're going to talk about some of those details.

00:08:34.520 --> 00:08:39.400
But the idea for that course was not necessarily how can I use LLMs to build me software.

00:08:39.680 --> 00:08:42.840
It's more, oh, how can I build software that also uses LLMs under the hood?

00:08:43.039 --> 00:08:46.940
If I have an app that makes a summary, How can I make sure the summary is reliable?

00:08:47.240 --> 00:08:50.640
And basically get the ball rolling on those building blocks.

00:08:50.800 --> 00:08:51.680
That's what that course is about.

00:08:51.840 --> 00:08:57.600
That's also probably going to be the main bones of this topic as opposed to a dentist Bob and his ambitions to make a new tech startup.

00:08:58.380 --> 00:08:58.740
Exactly.

00:08:59.180 --> 00:09:01.360
We're going to grow new teeth if we just could get the software right.

00:09:01.900 --> 00:09:03.940
Now, yeah, so how do you do that?

00:09:03.990 --> 00:09:05.120
And we're going to talk about that before.

00:09:05.470 --> 00:09:11.300
I just want to give you a chance before we really dive too far into the details of how we make that work.

00:09:11.620 --> 00:09:15.760
is just give people a sense of what you've been up to lately.

00:09:15.940 --> 00:09:18.300
You've done a lot of cool stuff with CalmCode.io.

00:09:20.699 --> 00:09:24.400
I can see your YouTube game is growing strong here.

00:09:25.080 --> 00:09:27.960
And you've been doing a lot of data science at Marimo.

00:09:28.640 --> 00:09:30.080
Yeah, what's-- Right.

00:09:30.880 --> 00:09:33.420
Yeah, we haven't spoken in a year, so maybe we should catch up.

00:09:34.200 --> 00:09:37.180
So long story short, CalmCode is still a project that I maintain.

00:09:37.540 --> 00:09:42.400
It's basically 99% free educational content on Python.

00:09:42.630 --> 00:09:44.280
That thing is just going to maintain itself.

00:09:45.800 --> 00:09:47.100
I'm super happy to keep maintaining it.

00:09:47.520 --> 00:09:54.220
The main thing I'm doing with that nowadays is just every two months I switch cloud providers just because I can, and then I can sort of see what it's like on the other side.

00:09:55.120 --> 00:09:55.840
So that's the thing that I do.

00:09:56.640 --> 00:10:06.440
From Calm Cult, though, I figured I might also start a YouTube channel, and that ended up being a YouTube channel where I still talk about Python stuff, but I mainly talk about these fancy ergonomic keyboards because I had a couple of RSI issues.

00:10:07.160 --> 00:10:10.460
In a year's time, I went from 100 subscribers to 5,000 something.

00:10:10.680 --> 00:10:12.120
So this thing kind of took off.

00:10:12.860 --> 00:10:15.320
I actually have sponsors now.

00:10:15.360 --> 00:10:19.780
So I actually have a couple of companies in Asia sending me their custom boards for me to review.

00:10:19.860 --> 00:10:21.400
So that's been a really fun journey.

00:10:22.460 --> 00:10:24.820
But since last time we spoke, I also switched employers.

00:10:25.080 --> 00:10:28.340
So people might have heard of Jupyter before.

00:10:28.640 --> 00:10:31.320
That's an environment in Python where you can do interactive things.

00:10:31.920 --> 00:10:36.540
And I work now for a company that makes Maremo, which is an alternative, a very likable one.

00:10:36.720 --> 00:10:38.020
It does work completely differently.

00:10:38.620 --> 00:10:44.780
One of the main things that attracts people to it is the fact that these kinds of notebooks are Python files under the hood.

00:10:45.240 --> 00:10:46.520
They're also a little bit more interactive.

00:10:46.660 --> 00:10:49.020
You can do more rapid prototyping with UI.

00:10:49.340 --> 00:10:50.780
You can blend Python with it very nicely.

00:10:52.060 --> 00:10:59.960
So I will say all of my rapid prototyping, especially with LLMs, I do that in Marimo nowadays just so you can blend Python with UIs very easily.

00:11:00.980 --> 00:11:03.220
And there's demos on the site that people can go ahead and check out.

00:11:04.200 --> 00:11:07.760
But that's also the second YouTube channel I made this year.

00:11:08.500 --> 00:11:09.680
I do a lot of stuff for Marimo.

00:11:10.220 --> 00:11:15.200
I'm a little bit more on the growth side of things than the hardcore engineering side of things.

00:11:15.220 --> 00:11:17.040
I still make a lot of plugins for Marimo, of course.

00:11:17.760 --> 00:11:22.860
But that's a little bit more of what I do nowadays, making sure that people learn about Marimo.

00:11:24.440 --> 00:11:26.620
You can do things in a notebook now that you couldn't think of before.

00:11:27.620 --> 00:11:33.340
I write my command line apps in a notebook nowadays because it's actually not just because I can, but because it's convenient too.

00:11:33.700 --> 00:11:39.180
So we'll talk a little bit about that later when we talk about LLMs, but that's effectively the short story of what I've been doing last year.

00:11:39.420 --> 00:11:39.740
Nice.

00:11:40.820 --> 00:11:41.100
All right.

00:11:41.820 --> 00:11:43.500
I am a fan of Marimo.

00:11:43.820 --> 00:11:44.920
It really...

00:11:44.920 --> 00:11:45.700
Happy to hear it.

00:11:46.420 --> 00:11:47.020
Yeah, yeah, yeah.

00:11:48.240 --> 00:11:53.820
I did a data science course this year, just enough Python and software engineering for data scientists.

00:11:54.540 --> 00:12:02.000
What could you bring from the software engineering side to make your engineering, data science stuff a little more reliable?

00:12:02.600 --> 00:12:05.700
But I was right on the fence of should I use this or should I not?

00:12:07.740 --> 00:12:10.140
Because I think it's clearly better.

00:12:10.860 --> 00:12:13.900
But at the same time, I also want to use what people are using.

00:12:14.180 --> 00:12:15.680
So I didn't quite go for it.

00:12:15.780 --> 00:12:18.280
But the UI is fantastic.

00:12:18.800 --> 00:12:28.820
The reproducibility, reliability of it, where it uses the abstract syntax tree or concrete, whatever, to understand relationships across cells so they don't get out of sync, potentially.

00:12:30.140 --> 00:12:31.660
Yeah, I think it's really nice.

00:12:31.880 --> 00:12:35.880
I had Akshay on the show, who you work with also, to talk about it.

00:12:36.880 --> 00:12:43.860
Well, so the one thing in Jupyter that's, I mean, let me maybe start by saying, Jupyter is still a really cool project.

00:12:44.030 --> 00:12:58.920
Like, if I think back of the last decade, like, all the good that that project has brought to my career, not just directly as a user, but also indirectly, all the algorithms that got invented simply because we had a good enough, like, interactive environment suddenly, which we never had before, it's done wonders.

00:12:59.010 --> 00:13:00.820
So, like, we should definitely be thankful.

00:13:01.620 --> 00:13:02.880
I'm not bashing on it either.

00:13:03.330 --> 00:13:09.200
Yeah, but I do always want to make sure that I give credit where credit is due because the project had a lot of impact.

00:13:10.000 --> 00:13:11.620
But there is this really annoying thing with Jupyter, though.

00:13:11.730 --> 00:13:22.520
Besides, you can be a bit grumpy about the file format, sure, but the one thing that's very annoying is you can write down X is equal to 10 and then have all sorts of cells below it that depend on X.

00:13:23.140 --> 00:13:26.360
You can then delete the cell, and the notebook will not complain to you about it.

00:13:27.600 --> 00:13:31.160
Even though if anyone else tries to rerun the notebook, X is gone.

00:13:31.420 --> 00:13:34.400
it won't run and your notebook is broken and you can't share the thing anymore.

00:13:35.320 --> 00:13:46.460
And nowadays, fast forward many years later and we've got stuff like uv now so we can add metadata to a Python file to add dependencies and, oh, wait, because Marimo is a Python file, we can add dependencies to the notebook.

00:13:46.590 --> 00:13:47.800
That makes it super reproducible.

00:13:48.560 --> 00:13:55.060
There's just all this stuff that you can rethink now simply because we have a good notebook format that is still a Python file.

00:13:55.340 --> 00:13:56.840
That's really the killer feature here.

00:13:58.580 --> 00:14:02.580
We can talk more about it if you like, but I can talk for ages about this topic, just warning you.

00:14:03.839 --> 00:14:07.420
One final thing, maybe also for the software engineering side of things, because you didn't mention it.

00:14:08.060 --> 00:14:18.900
Just to give an example of something we added recently, if you have a cell in Marimo, and there's a function in there that starts with test underscore, we will automatically assume it's a pytest.

00:14:20.220 --> 00:14:30.900
So you can actually add unit tests to your notebook as well, and then if you say python notebook.py, then pytest will just run all the tests for you, even though you can also run the tests in line in your CI/CD as well.

00:14:31.080 --> 00:14:35.620
So there's all sorts of these Python specific things that we can add, again, because it's just a Python file.

00:14:36.320 --> 00:14:36.680
Yeah.

00:14:36.880 --> 00:14:42.900
It's sort of the second mover advantage or nth mover advantage or n is greater than one, where you see, OK, that was awesome.

00:14:43.240 --> 00:14:45.420
Maybe this was a little bit of a rough edge.

00:14:45.470 --> 00:14:48.460
And what would we do to work around that or smooth it out, right?

00:14:49.140 --> 00:14:53.800
Well, and also we're also lucky that we're born in the age of uv, I got to say.

00:14:53.920 --> 00:14:57.280
a lot of quality of life improvements to a notebook.

00:14:58.520 --> 00:15:01.980
A lot of that is also due to the fact that uv is around. That's definitely a huge help as well.

00:15:03.759 --> 00:15:06.240
Yeah. I saw on, gosh, where was it?

00:15:06.460 --> 00:15:12.320
Speaking of Reddit, I saw on slash R learn Python or slash R Python.

00:15:12.920 --> 00:15:15.680
One of the slash R's with a Python substring.

00:15:16.900 --> 00:15:20.120
Someone asks, what Python package manager

00:15:20.220 --> 00:15:23.880
would you use now? And it was just like, how many times

00:15:23.900 --> 00:15:27.200
When you say uv, the feedback comments.

00:15:27.420 --> 00:15:29.500
I mean, it was someone new who wasn't unsure, right?

00:15:29.880 --> 00:15:30.900
They've seen all the different ones.

00:15:31.779 --> 00:15:32.420
And yeah.

00:15:33.240 --> 00:15:36.800
I mean, the coolest comparison I've seen-- I think it was a tweet someone posted.

00:15:36.940 --> 00:15:40.200
But definitely, suppose you're in the data field right now.

00:15:40.260 --> 00:15:41.620
Like, what are the tools 10 years ago?

00:15:41.800 --> 00:15:42.520
What are the tools now?

00:15:42.960 --> 00:15:45.780
It definitely does feel like, OK, before we had pip, now we have uv.

00:15:46.660 --> 00:15:48.580
Before we had Pandas, now we have Polars.

00:15:49.600 --> 00:15:51.360
Before we had Matplotlib, now we have Altair.

00:15:51.360 --> 00:15:52.880
And before we had Jupyter, and now we've got Maremo.

00:15:53.060 --> 00:15:54.880
You can kind of see a generational shift,

00:15:55.000 --> 00:15:56.240
not just on the notebook side of things,

00:15:56.290 --> 00:15:58.720
but on the package manager, on the data frame library.

00:15:59.720 --> 00:16:02.720
We're all learning from the previous generation, and it kind of feels like.

00:16:04.000 --> 00:16:04.880
Yeah, absolutely.

00:16:05.240 --> 00:16:07.940
It's an amazing time.

00:16:08.500 --> 00:16:11.400
Every single day in Python is just more exciting than the day before.

00:16:12.360 --> 00:16:16.560
Yeah, although I should also mention, like especially with the Polar thing, there's a fair bit of rust too.

00:16:17.180 --> 00:16:18.840
That also makes a difference.

00:16:19.320 --> 00:16:20.740
Yeah, yeah, of course.

00:16:22.300 --> 00:16:28.020
So before Flyby, you talked a bit about your YouTube channel.

00:16:28.250 --> 00:16:30.240
You review the ergonomic keyboards.

00:16:30.650 --> 00:16:35.420
You have, for those people who are just listening, not watching, your background is super nice.

00:16:36.160 --> 00:16:37.700
Your bona fides are solid here.

00:16:38.360 --> 00:16:39.720
But you've got a bunch of different ones.

00:16:39.920 --> 00:16:44.000
And I personally also, I don't know, you might even just kick me out of the club

00:16:44.170 --> 00:16:44.780
if I should do that.

00:16:44.820 --> 00:16:46.420
That's the Microsoft Sculpt, right?

00:16:46.680 --> 00:16:48.460
The Microsoft Sculpt ergonomic.

00:16:48.490 --> 00:16:55.860
And I like it so much because I can take this little bottom thing off and it's like razor thin if I could rotate and reverse and jam it in my backpack and take it with me.

00:16:56.020 --> 00:16:59.340
But I've also had RSI issues for 25 years or something.

00:17:00.660 --> 00:17:01.860
And it was really bad.

00:17:02.240 --> 00:17:02.640
And I switched.

00:17:03.020 --> 00:17:04.500
I used to type them on like laptops and stuff.

00:17:04.560 --> 00:17:06.400
And I switched to just something like this.

00:17:07.220 --> 00:17:09.740
And I could type 10 hours a day and it's no problem.

00:17:09.900 --> 00:17:17.439
Whereas before, if I were to force to type full on a laptop, I would probably be like unable to work in weeks, if not a week.

00:17:17.880 --> 00:17:18.319
It's so bad.

00:17:18.800 --> 00:17:20.620
We had an offsite in San Francisco.

00:17:20.800 --> 00:17:21.720
I was there like two weeks ago.

00:17:22.410 --> 00:17:24.939
A lot of fun, but I'm not going to bring...

00:17:25.520 --> 00:17:30.420
Airport security is going to look at this and wonder what kind of weird alien device this is, right?

00:17:30.880 --> 00:17:33.220
So I figured, okay, I'll leave those at home and just bring my normal laptop.

00:17:33.310 --> 00:17:35.420
And after a week, I'm feeling it in my wrist again.

00:17:35.860 --> 00:17:36.380
It's not good.

00:17:36.920 --> 00:17:38.200
Yeah, it's something that takes serious.

00:17:38.310 --> 00:17:47.060
Although I will also say, because the kid was sick and the wife was sick, so I wasn't able to do a whole lot of exercise at home, also do exercise.

00:17:47.300 --> 00:17:48.660
That also totally helps.

00:17:48.740 --> 00:17:53.460
If I had to like, sure, buy an ergonomic keyboard if you like, programmatic keyboards, they're great.

00:17:54.000 --> 00:17:56.880
But if you're going to do that as an excuse not to do exercise, you're doing it wrong.

00:17:57.200 --> 00:17:58.860
That's the one thing I do want to add to that.

00:17:58.940 --> 00:18:00.160
There's lots of stretches you can do.

00:18:00.420 --> 00:18:01.120
Taking breaks matters.

00:18:01.640 --> 00:18:02.260
Exercise matters.

00:18:03.160 --> 00:18:06.020
But I think those keyboards are an essential feature.

00:18:07.520 --> 00:18:08.820
They do totally help.

00:18:08.870 --> 00:18:11.400
And also sometimes it's not so much a healing measure.

00:18:11.600 --> 00:18:12.660
It's more of a preventative measure.

00:18:13.420 --> 00:18:14.140
Yes, 100%.

00:18:14.450 --> 00:18:20.720
And I would like to put this message out just to everyone listening who is young, absolutely indestructible.

00:18:22.520 --> 00:18:22.860
Please.

00:18:23.660 --> 00:18:26.760
I know they're a bit of a pain in the butt with the weird curviness.

00:18:28.140 --> 00:18:32.980
It is so worth it to just say, I've never had any RSI issues.

00:18:33.700 --> 00:18:36.200
I just use this weird keyboard and people make fun of me, but I don't care.

00:18:36.440 --> 00:18:39.780
That is a better thing than like, I'm working on it.

00:18:40.220 --> 00:18:40.620
Yeah.

00:18:42.540 --> 00:18:45.920
Well, another thing is also, especially now that we've got stuff like Claude, right?

00:18:46.060 --> 00:18:48.320
If I can point out one thing on that front.

00:18:49.420 --> 00:18:52.660
So these ergonomic keyboards, you can often program them as you see fit.

00:18:52.840 --> 00:18:55.100
So you can say, if I hit this key, it's K.

00:18:55.220 --> 00:18:58.000
But if I hold it down, it becomes command or whatever you like.

00:18:58.740 --> 00:19:00.540
But it also means that you can map macros or shortcuts.

00:19:00.820 --> 00:19:04.340
So whenever I hit this button, an app called Mac Whisper boots up.

00:19:04.800 --> 00:19:06.360
I love Mac Whisper.

00:19:07.020 --> 00:19:09.080
And then there's alternative variants for it.

00:19:09.080 --> 00:19:10.520
I'm sure you've got stuff for Linux as well.

00:19:10.580 --> 00:19:13.060
But the main thing it does is just really good speech to text.

00:19:13.680 --> 00:19:17.880
And then whenever I'm done talking, it's just immediately going to paste whatever I'm in.

00:19:18.580 --> 00:19:28.960
So, oh, that's actually very nice because now the keyboard shortcut is just me holding my thumb down instead of some sort of weird claw I got to eject to my keyboard.

00:19:30.120 --> 00:19:32.600
And suddenly it doesn't necessarily become a convenience thing.

00:19:32.700 --> 00:19:35.260
It also becomes kind of a power user thing.

00:19:35.400 --> 00:19:37.120
You can really customize your computer experience

00:19:37.560 --> 00:19:40.860
if you can really map everything to your keyboard just the way that you like.

00:19:41.040 --> 00:19:43.480
It's like having Vim, but for all the apps out there.

00:19:44.320 --> 00:20:07.880
okay that's very neat yeah i've remapped caps lock to my activate mac whisper so if i hit caps lock and hold this down then i can dictate and i know computers have had dictation for a long time but it's been really bad right the built-in dictation to mac os or windows isn't great especially when you try to talk acronyms like hey do pi pi and it's like yeah mac whisper no but mac whisper is

00:20:08.000 --> 00:20:13.560
it's it's not like all the way there but i will say it's surprised surprised me in quality a few

00:20:13.580 --> 00:20:30.860
definitely. Yeah, yeah. And that app, you can go in and actually set replacements. Like if you write, if you ever think you're going to write this, write that instead. So I've done that with like Talk Python because it always just turns it into a camel case combined. I'm like, no, those are two separate words, you know, whatever. You can sort of customize it a bit.

00:20:31.840 --> 00:20:50.060
Yeah. And I think my favorite one, I think Mac Whisper actually gets this one wrong still, but whenever I write scikit-learn, which is a very popular site, like data science package, it always translates it's to psychologists learn which you know they should put that as a correction

00:20:50.160 --> 00:21:01.040
in there like i never want you to ever write this series of words because if you if i have to i'll just type it the one time it's not my common thing well so i'm not it's it's one of these

00:21:01.180 --> 00:21:28.940
moments where actually i type scikit-learn often enough where it's like almost becoming the issue so i'm like at the verge of adding these rules manually for all these weird open source packages that I interact with. But yeah, it's, yeah, take ergonomics serious people. That's the one thing I want to say, you don't always have to buy a super expensive keyboard. If you want to explore like programmatic keyboards, because you can customize things, that's an excellent reason. But like, take a break and do exercises and just, you know, be healthy. That's the that's how you win a

00:21:29.120 --> 00:21:42.960
marathon. Yes, that's for sure. Now, this is not the Vincent's keyboard review marathon. But let's wrap it up with, if you could take any keyboard hanging around on your wall there, which one would you use?

00:21:43.580 --> 00:21:47.480
Well, so there's four pairs of these, so it's probably this one.

00:21:48.980 --> 00:21:57.220
So there's a couple of other boards that are great too. This board is not the board I would recommend to everyone, but if you have serious RSI issues, I do think the GloVe 80 is your best bet.

00:21:58.780 --> 00:22:02.880
In terms of the shape, it is probably the most ergonomic shape for most hand sizes.

00:22:04.919 --> 00:22:05.720
Okay. Awesome.

00:22:07.500 --> 00:22:11.500
All right, well, let's switch over to talking about programming.

00:22:12.460 --> 00:22:12.680
Yes.

00:22:12.980 --> 00:22:15.740
LLMs with LLMs, not programming LLMs.

00:22:16.360 --> 00:22:21.700
And I guess, like you already called out, you wrote this course called LLM Building Blocks for Python.

00:22:22.960 --> 00:22:24.300
Super fun course, really neat.

00:22:24.500 --> 00:22:25.660
It's pretty short and concise.

00:22:26.720 --> 00:22:32.140
And it really talks about how can you reliably add some kind of LLM into your code.

00:22:34.880 --> 00:22:43.900
I guess what you're talking about in this course really applies regardless of whether it's a self-hosted one or it's OpenAI or Anthropic, right?

00:22:44.480 --> 00:22:50.060
There's some choices you can make on which LLM to use, right?

00:22:50.980 --> 00:23:02.900
So the main idea I had with the course was like-- an LLM is a building block at some point, but it's very unlike a normal building block when you're dealing with code.

00:23:03.000 --> 00:23:07.160
Because normally with code, you put something into a function and like one thing comes out.

00:23:07.600 --> 00:23:09.600
But in this particular case, you put a thing into a function.

00:23:09.760 --> 00:23:11.680
You have no idea up front what's going to come out.

00:23:12.240 --> 00:23:15.680
And not only that, but you put the same thing in twice and something else might come out as well.

00:23:16.300 --> 00:23:21.080
So that means that you're going to want to think about this tool a bit more defensively.

00:23:21.160 --> 00:23:22.100
It's a weird building block.

00:23:22.260 --> 00:23:26.640
It's like you have to put a moat around it because otherwise the building block is going to do stuff you don't want it to do.

00:23:27.380 --> 00:23:27.580
Almost.

00:23:28.940 --> 00:23:30.020
And like some of that is syntax.

00:23:30.220 --> 00:23:32.020
Some of that is how do you think about these Lego bricks.

00:23:32.620 --> 00:23:39.600
And some of it is also just what is good methodology in general to statistically test if the thing is doing roughly what you want it to do.

00:23:40.780 --> 00:23:41.180
Nice.

00:23:42.179 --> 00:23:43.120
That's the gist of it.

00:23:43.800 --> 00:23:51.840
Yeah, I pulled out a couple of ideas, concepts, and tools that you talked about throughout the course.

00:23:52.460 --> 00:23:55.080
And you don't have to have taken the course to have these things be useful.

00:23:55.260 --> 00:23:59.480
I just thought it might be fun to riff on some of the things you touched on here.

00:24:00.840 --> 00:24:03.840
The main thing I think will be fun is it's been half a year, I think.

00:24:04.110 --> 00:24:05.900
It will be fun how much of it is still intact.

00:24:06.030 --> 00:24:14.020
I think most of it still definitely is, but it might be fun to sort of see if we can find anything that might be dated just to see if the world has moved on quickly.

00:24:14.160 --> 00:24:17.260
I think there's only one thing, but I'm just kind of curious.

00:24:17.900 --> 00:24:18.320
Yeah, all right.

00:24:18.370 --> 00:24:22.100
Well, let's keep our radar up for that.

00:24:22.180 --> 00:24:30.660
It's definitely something that's more changing quicker and has a higher likelihood of being dated, but I think it holds up pretty well.

00:24:31.300 --> 00:25:12.600
yeah like okay so we like one of the things I remember emphasizing is you want to do some stuff like caching so like let's say you've got a function and you use an LLM for it and let's let's keep it simple let's say we're just making summaries so talk Python episode paragraph goes in single sentence is supposed to come out something like that okay well you might have a loop and you're you know you're going to do like maybe one pass try one LLM with one type of setting, try another LLM with different type of settings to generate all this data, it would be a shame that you're going to use an LLM, which is kind of an expensive compute thing, if you put the same input in by accident and then you incur the cost twice. That would really stink.

00:25:13.180 --> 00:25:16.080
So one of the things you always want to do is think a little bit about caching.

00:25:17.180 --> 00:25:24.320
There's a Python library called disk cache that I've always loved to use and I highly recommend people have a look at it. I think, Michael, you've also used it in one of your courses before.

00:25:24.900 --> 00:25:26.900
Oh my gosh, we have to talk about this. It's so

00:25:27.040 --> 00:25:28.320
good. It is so good.

00:25:28.760 --> 00:25:30.840
It is SQLite and is so good.

00:25:30.920 --> 00:25:32.740
It is even better than SQLite.

00:25:32.780 --> 00:25:34.820
It is unbelievably good.

00:25:34.900 --> 00:25:35.940
And I have you to think.

00:25:36.100 --> 00:25:37.940
I knew about it, but like, ah, whatever.

00:25:38.240 --> 00:25:40.460
And then after I saw you use it, I'm like, genius.

00:25:41.400 --> 00:25:42.120
This is genius.

00:25:42.340 --> 00:25:42.580
It is so good.

00:25:43.040 --> 00:25:43.160
Yes.

00:25:43.360 --> 00:25:48.860
No, so it's like having the LRO disk cache, but it's also on disk.

00:25:48.980 --> 00:25:52.540
So if you were to restart Python, you still have everything in cache, basically.

00:25:52.980 --> 00:25:57.480
And it's a SQLite database, so you can always inspect all the stuff that's in there.

00:25:57.880 --> 00:26:04.060
If you wanted to, you can also do fancy things like add time to live to every single object.

00:26:04.260 --> 00:26:06.380
And this is something you could do in a Docker container for a web app.

00:26:07.100 --> 00:26:14.860
But the main thing that's always nice when you're dealing with LLMs is you always want to be able to say in hindsight, like, okay, how did this LLM compare to that one?

00:26:15.000 --> 00:26:15.840
You want to compare outputs.

00:26:16.140 --> 00:26:21.360
And then just writing a little bit of a decorator on top of a function is the way to put it in SQLite.

00:26:21.460 --> 00:26:22.860
And you're just done with that concern.

00:26:23.700 --> 00:26:25.240
That is just amazing.

00:26:25.620 --> 00:26:27.820
And we're using this cache directly.

00:26:28.260 --> 00:26:35.380
If you're using LLM by Simon Willison from the command line, there's also a mechanism there so that you can get it into SQLite if you wanted to.

00:26:35.440 --> 00:26:37.020
So that's also a feature you could consider.

00:26:37.640 --> 00:26:39.380
Of course, he's going to put something in SQLite.

00:26:39.540 --> 00:26:41.620
He couldn't write a library that doesn't put something in SQLite,

00:26:41.720 --> 00:26:43.280
given his dataset project.

00:26:43.760 --> 00:26:44.500
It's Simon Willison.

00:26:44.680 --> 00:26:46.560
He'll put SQLite in SQLite if it's a Sunday.

00:26:50.179 --> 00:26:58.760
But if you haven't used this cache before, it definitely feels like one of these libraries is that because I have it in my back pocket, it just feels like I can tackle more problems.

00:26:59.080 --> 00:27:00.480
That's the short story of it.

00:27:00.620 --> 00:27:13.120
And again, the course uses it in a very sort of basic fashion, but knowing that everything you do in an LLM only needs to happen once, if you're interested in using it once, that just saves you so much money.

00:27:13.960 --> 00:27:15.060
Yeah, and it's so useful.

00:27:15.370 --> 00:27:24.520
So one of the things you did in the course is you said, all right, the key for the value that we're going to store is going to be the model, the model settings, and the prompt.

00:27:25.600 --> 00:27:26.800
as a tuple, right?

00:27:27.110 --> 00:27:27.800
Something along those lines.

00:27:28.040 --> 00:27:29.520
And then you use that as the key.

00:27:29.610 --> 00:27:32.380
So if any of those variations change, does the model change?

00:27:32.730 --> 00:27:33.660
Do the settings change?

00:27:33.690 --> 00:27:36.100
Or like anything, that's a totally different request.

00:27:36.270 --> 00:27:51.900
And then you just store the response and then boom, if you ask the exact same question of the same model with the same prompt with the same settings, why do you need to go and wait 10 seconds and burn money and environmental badness when you could literally within a microsecond get the answer back?

00:27:52.420 --> 00:27:53.900
Yeah, and there's also a fancy thing.

00:27:54.000 --> 00:27:55.340
Is there a trick you can do on top?

00:27:55.390 --> 00:27:59.380
So sometimes you want to say, well, there's a statistical thing also happening.

00:27:59.580 --> 00:28:05.440
So sometimes I want to have one input and actually store maybe 10 outputs so I can look at all these outputs, maybe annotate that later.

00:28:06.000 --> 00:28:09.860
And the way you solve that is you just add an integer to the tuple, basically.

00:28:10.480 --> 00:28:12.980
And then you're also able to store many outputs, if you really like.

00:28:13.320 --> 00:28:13.660
Right, right.

00:28:13.710 --> 00:28:14.540
This is the first one.

00:28:14.730 --> 00:28:15.560
The eighth one, whatever.

00:28:16.460 --> 00:28:16.960
Yeah, yeah, cool.

00:28:18.000 --> 00:28:18.360
Flexible.

00:28:18.980 --> 00:28:19.340
Yeah.

00:28:20.580 --> 00:28:28.360
And it does a whole bunch of neat things that are really, really wild. So it looks like just, hey, I put this value into it, right? And it stores it.

00:28:28.840 --> 00:28:43.220
It's really powerful because across application executions. So maybe if you're caching that response in your notebook and whatever you're doing some testing, if you come back later, you start the notebook back up or restart the kernel or whatever, it's not like an LRU cache.

00:28:43.290 --> 00:29:02.080
It remembers because it's stored somewhere in temporary storage in a local SQLite file, which is amazing. It also has interesting ideas. I'm not sure really where they are, but like it has different kinds of caches as well. So maybe you're storing a ton of stuff in there.

00:29:02.270 --> 00:29:15.120
And so it'll do basically built-in sharding across multiple SQLite files. And it's really, really good. This is a deep library. This is not just, oh yeah, it's like LRU went to desk.

00:29:15.520 --> 00:29:18.440
So the cool thing about that library is it really does go deep.

00:29:19.200 --> 00:29:22.060
But if you really just want to use it as a dictionary, you can.

00:29:22.660 --> 00:29:24.140
That's the thing that I really love about it.

00:29:24.620 --> 00:29:27.340
For all intents and purposes, you just treat it as a dictionary and you're good.

00:29:27.470 --> 00:29:28.960
Or use it as a decorator on a function.

00:29:29.110 --> 00:29:30.120
And again, you're good.

00:29:33.560 --> 00:29:40.880
So again, it's one of those little hacks where, oh, if you just know about the library, you just become more productive at typical Python stuff.

00:29:41.820 --> 00:29:42.180
Yeah.

00:29:43.500 --> 00:29:44.980
I'll give you a place where I'm using it, actually.

00:29:45.320 --> 00:29:50.180
I use it on some LLM stuff, like where I'm programming against an LLM for exactly the reason you did.

00:29:50.320 --> 00:29:52.760
Because if you have the same inputs, don't ask the question again.

00:29:53.160 --> 00:29:53.980
You just hear the answer.

00:29:54.130 --> 00:29:54.820
It's real, real fast.

00:29:55.400 --> 00:29:58.600
But if you go over to like Talk Python, let's see here.

00:29:59.620 --> 00:30:01.360
Go over to the guests, for example, right?

00:30:01.390 --> 00:30:02.260
So there's a bunch of guests.

00:30:02.590 --> 00:30:03.360
And here we have Vincent.

00:30:04.160 --> 00:30:04.700
Not that Vincent.

00:30:05.120 --> 00:30:05.640
Not that Vincent.

00:30:06.380 --> 00:30:06.860
There you are.

00:30:06.910 --> 00:30:07.240
That Vincent.

00:30:07.300 --> 00:30:07.920
All these Vincents.

00:30:09.400 --> 00:30:10.400
They're all over the...

00:30:10.900 --> 00:30:12.820
I have like 560 guests or something.

00:30:12.900 --> 00:30:13.200
There's a lot.

00:30:13.720 --> 00:30:52.500
but in here you'll notice that if you go into the view the source on this thing like all over the place anytime there's a picture it'll have this little cache busting id on the end and that's fine when it's served locally because you can just look at the file and go just the file still with the same cache at startup but if it comes from like an s3 blob storage you know and the app restarts how do i know what that is like it has to go and it would have to re-download the entire content so the blob it's so check this out yeah so i just added yeah yeah yeah so this feels like

00:30:52.580 --> 00:31:00.280
there's something between the proper cdn and then getting it from s3 like there are these moments when you want to have something that's kind of in between and then this cache could actually be a

00:31:00.290 --> 00:31:08.340
good solution for it exactly so what the site does is it has a disc cache and anytime it has it says hey, I want to refer to a resource that's external.

00:31:09.080 --> 00:31:11.360
It'll download it once, compute the hash,

00:31:11.460 --> 00:31:17.120
and then store it in the disk cache unless you change something behind it.

00:31:17.120 --> 00:31:26.220
And so it's automatically using this, and it makes everything-- there's never stale resources, and it's instantly fast, even if they're served out of something remote like S3.

00:31:26.820 --> 00:31:27.200
A quick one.

00:31:27.700 --> 00:31:29.000
And do you also do a time to live?

00:31:29.000 --> 00:31:29.680
It's so good, man.

00:31:29.760 --> 00:31:30.240
It's so good.

00:31:30.900 --> 00:31:31.300
Yes.

00:31:31.680 --> 00:31:32.020
I do--

00:31:32.120 --> 00:31:35.320
Time to live is also-- like every day, it's allowed to refresh once or something like that, I suppose?

00:31:36.600 --> 00:31:44.180
Yeah, for the S3 stuff, I don't because I've set up all the admin functions that if I ever change one through the admin, it deletes it out of the cache.

00:31:44.560 --> 00:31:45.400
Gotcha, gotcha.

00:31:45.540 --> 00:31:47.320
So it's like it's internally consistent.

00:31:47.960 --> 00:31:58.300
But for other things, like if it parses something out of, say, the description, which sits set in the dictionary, that stuff has just got a time to live,

00:31:58.380 --> 00:32:00.920
like a day or something, and there's a bunch of those.

00:32:00.950 --> 00:32:03.900
So I'm using all these different places, and wow, it's so good.

00:32:04.000 --> 00:32:06.060
I just wanted to say thank you because I knew we're going to talk about it today.

00:32:06.600 --> 00:32:13.560
Yeah, no, it's one of the, if it was part of the standard library, I would honestly not be surprised.

00:32:13.920 --> 00:32:15.700
Like that's also the story with that.

00:32:15.720 --> 00:32:17.000
But yeah, SQLite is great.

00:32:17.260 --> 00:32:17.800
Disc cache is great.

00:32:18.700 --> 00:32:21.100
It feels like a dictionary, but it gives you so much more.

00:32:21.540 --> 00:32:21.780
It's great.

00:32:22.060 --> 00:32:23.500
That's the only thing I can say about it.

00:32:25.360 --> 00:32:27.680
So people are probably like, wait, I thought we were talking about LLMs.

00:32:27.860 --> 00:32:28.320
Yeah, we are.

00:32:28.500 --> 00:32:34.320
I think this is one of the interesting things is because there's all these interesting tools, And it's not even about using agentic AI.

00:32:34.620 --> 00:32:42.280
And it's not-- like there's really cool libraries and tools that you can just apply to this LLM problem, but also apply everywhere else, right?

00:32:43.100 --> 00:32:44.160
I think it's neat.

00:32:44.720 --> 00:32:48.200
So it's one thing I have found with LLMs in general if you're building software with it.

00:32:48.560 --> 00:32:55.160
On the one end, I think it can be very helpful if you're a proper senior engineer kind of a person, because then you know about things like, oh, I want a cache.

00:32:55.530 --> 00:32:56.500
And what's a pragmatic cache?

00:32:56.680 --> 00:32:58.120
And you can also pick Redis, by the way.

00:32:58.180 --> 00:32:59.980
If you would have used Redis for this, that could have also worked.

00:33:00.380 --> 00:33:01.200
Sure, it would have been fine.

00:33:01.980 --> 00:33:04.120
You know what I like is there's no servers.

00:33:04.620 --> 00:33:05.640
I don't have to deal with servers.

00:33:06.220 --> 00:33:07.880
It's just a file.

00:33:08.760 --> 00:33:09.360
Very true.

00:33:09.420 --> 00:33:10.280
It just keeps it simpler.

00:33:11.320 --> 00:33:11.760
Totally true.

00:33:12.020 --> 00:33:18.760
But the point I wanted to get out here, your previous experience as a proper engineer will still help you write good LLM software.

00:33:19.440 --> 00:33:30.280
However, from a similar perspective, I also think that we do have this generation of data scientists and maybe data engineer, data analyst kind of person, thinking analytically, being quite critical of the output of an algorithm.

00:33:30.860 --> 00:33:35.000
That's also a good bone to have in this day and age, I would say.

00:33:37.920 --> 00:33:40.020
I've built a couple of recommenders back in my day.

00:33:40.360 --> 00:33:48.960
It's been a decade now, but one of the things you do learn when you're building a recommender is that you're stuck with this problem of, hey, I gave this user this recommendation and they clicked it.

00:33:49.480 --> 00:33:52.680
Would they have clicked this other thing if I would have recommended it to them?

00:33:53.130 --> 00:34:01.500
And suddenly you're dealing with a system that's really stochastic and hard to predict, and you have to be kind of strict about the way you test and compare these different algorithms.

00:34:01.860 --> 00:34:04.600
And you want to think twice about the way you AB test these things.

00:34:04.820 --> 00:34:17.280
And, oh, actually, just like disk cache is useful as a tool, having a little bit of methodology statistically in your mind will also help you because comparing LLMs, a lot of it is doing evaluations.

00:34:18.260 --> 00:34:18.399
Sure.

00:34:18.580 --> 00:34:20.120
And being kind of strict about that.

00:34:20.149 --> 00:34:21.540
And that's also what I try to do in the course.

00:34:21.629 --> 00:34:32.000
I try to just show you that if you're really strict about the evaluations, then you can also learn that for some problems, you're still better off using scikit-learn because you just evaluate it learn that the number is better on the site.

00:34:32.010 --> 00:34:32.820
I could learn side of things.

00:34:34.119 --> 00:34:37.020
Yeah, that's when you feel like, oh, maybe I did it wrong.

00:34:37.080 --> 00:34:41.620
You paid a bunch of money to run expensive slow LLMs, and you're like, I would just use a predictor.

00:34:41.659 --> 00:34:43.020
Well, so it's funny you say that.

00:34:43.120 --> 00:34:44.980
So I've actually been talking to a bunch of people

00:34:45.179 --> 00:34:47.600
that do LLMs at companies here in the Netherlands.

00:34:47.830 --> 00:34:52.919
And you go to a PyData, you go to a conference, and you give them just enough beer so they're honest

00:34:53.320 --> 00:34:54.560
in that kind of a situation.

00:34:55.740 --> 00:34:58.220
The NDA curtain opens just a little or whatever.

00:34:59.000 --> 00:35:01.300
Plus, more deniability is the name of the game in this one, yes.

00:35:02.700 --> 00:35:14.120
But then the stories you hear, which I did find somewhat encouraging, is they do all kind of go, well, there's budget now to do AI stuff, which means that we try out all the different LLMs, and we invest in evals and that sort of thing.

00:35:14.680 --> 00:35:20.320
And funnily enough, we also put some base models in there as a standard benchmark that we should beat.

00:35:20.880 --> 00:36:01.000
And I've heard a story a bunch of times now that because of the hype around LLMs and AI, after it was implemented, after they did all the benchmarks, it turns out that AI is the reason that scikit-learn is now in production in a bunch of places and it's also the same thing with like spaCy because what a lot of people do learn is that hey if the spaCy model or like the lightweight model so to say is like somewhat equivalent to the LLM model after you give it like enough training data which you do want to have either anyway because you need to need that for evaluations well typically those models are more lightweight and they will always produce the same output same thing goes in same prediction will always come out and you can really fence it off, have it on like a normal VM.

00:36:01.280 --> 00:36:02.220
That's also totally fine.

00:36:02.780 --> 00:36:06.420
Oh, and you know, another benefit, it's super lightweight to run.

00:36:06.550 --> 00:36:12.460
You just need a Lambda function and you're good as opposed to like a GPU or like a huge cloud bill or something like that.

00:36:12.640 --> 00:36:26.320
So some of the stories that I'm hearing do suggest that, okay, these LLMs are also helping out like standard data science work, if it were, if only because management now really does want to be serious about the investment and really wants to do the experiments.

00:36:28.060 --> 00:36:50.940
yeah very interesting because you could say we're going to use your mandate is to add ai and then you go use like but we got to see how well it's working so we tested it and then it turns out to compare something of a benchmark exactly the benchmark's fast and effectively free and uh about as good so uh we're good we're just gonna do that you know i think it's yeah i think it's

00:36:50.960 --> 00:36:55.520
some organizations need like mandate from above in order to get something done.

00:36:55.880 --> 00:37:00.840
And this LLM craze, if nothing else, does seem to have caused the mandate from above.

00:37:01.280 --> 00:37:02.180
I'm sure that.

00:37:03.110 --> 00:37:07.660
So that's something I would keep in the back of your mind as well, dear listener.

00:37:07.810 --> 00:37:11.140
Like sometimes that mandate can also give you permission to do other things.

00:37:11.820 --> 00:37:16.740
Yeah, not working for a large enterprise type company for quite a while.

00:37:17.260 --> 00:37:24.100
I imagine I'm pretty blind to how that is transforming directives from the top.

00:37:24.240 --> 00:37:28.220
But I'm sure bosses are like, I'm using ChatGPT and it's so much better than our software.

00:37:28.340 --> 00:37:29.380
What are we going to do about that?

00:37:30.220 --> 00:37:30.620
Yeah.

00:37:31.520 --> 00:37:33.660
Well, I mean, I'm in the developer tooling space.

00:37:33.820 --> 00:37:35.980
So I still talk to big companies and small companies as well.

00:37:35.980 --> 00:37:38.940
And you do notice that big companies, they work differently than small companies.

00:37:39.060 --> 00:37:39.860
That is certainly true.

00:37:41.480 --> 00:37:42.140
For better or worse.

00:37:42.340 --> 00:37:44.140
There's also good reasons why bigger companies.

00:37:44.220 --> 00:37:49.980
Like if I'm a bank, I'm pretty sure it's a good idea to also have some rules that I have to abide by, which-- MARK MANDEL: Oh, it is.

00:37:50.580 --> 00:37:57.840
No, I'm just thinking more of how disconnected is the higher level management from the product, and how much do they just dictate a thing.

00:37:58.779 --> 00:38:00.580
OK, so let's talk about some of the tools.

00:38:00.720 --> 00:38:08.960
One of the things you primarily focused on was using Simon Willison's LLM library.

00:38:09.230 --> 00:38:09.920
Tell us about this thing.

00:38:10.270 --> 00:38:15.540
SIMON WILLISON: So the funny thing about that library that it's actually kind of more meant as a command line utility.

00:38:15.760 --> 00:38:18.280
I think that's kind of the entry point that he made.

00:38:18.760 --> 00:38:22.200
But then he also just made sure there was some sort of a Python API around it.

00:38:22.860 --> 00:38:34.900
And after looking at that library, and also after playing around with all these other libraries, and I'm about to say this, but this is a compliment, I just found the LLM library by Simon Willison by far to be the most boring.

00:38:35.580 --> 00:38:37.220
And I mean that really in a good way.

00:38:37.420 --> 00:38:40.060
Just unsurprising, only does a few things.

00:38:40.220 --> 00:38:42.580
a few things that it does is just in a very predictable way.

00:38:43.480 --> 00:38:55.920
And especially if you're doing rapid prototyping, what I felt was just kind of nice is that it does feel like you have building blocks that allow you to build very quickly, but it doesn't necessarily feel like you're dealing with abstractions that can sort of wall you in at some point.

00:38:56.860 --> 00:39:04.880
So for a hello world getting started, just do some things in a very Pythonic way, this boring library really did the trick.

00:39:05.060 --> 00:39:07.780
And I'm also happy to report it's still a library that I use.

00:39:08.300 --> 00:39:10.140
Like it's still definitely a library in my tool belt.

00:39:10.220 --> 00:39:14.940
And if I want to do something real quick, that is definitely a tool that I refer to.

00:39:17.619 --> 00:39:23.040
Is it kind of an abstraction across the different LLMs and that kind of stuff?

00:39:23.070 --> 00:39:26.360
Like if I want to talk to Anthropic versus OpenAI, it doesn't matter.

00:39:26.960 --> 00:39:30.360
So the way that this is built is also, I think, with good taste.

00:39:30.850 --> 00:39:36.560
So what you don't want to do as a library is say, I'm going to basically support every library under the sun.

00:39:36.820 --> 00:40:00.340
Because as a single maintainer, in the case of Simon Willis, saying you're just going to drown in all these different providers. So what he went for instead was a plugin ecosystem. Now the downside of a plugin ecosystem is that then you defer the responsibility of maintaining a plugin for a specific source to another maintainer. But you might get someone who works at Mistral to make the Mistral plugin. And you might get someone who works at Open Router to make the Open Router plugin, etc.

00:40:01.240 --> 00:40:12.800
So you do distribute the workload in kind of a nice way. And all the bigger models are definitely So the Anthropic and the OpenAI ones, those are just always in there.

00:40:13.800 --> 00:40:19.680
But you will also definitely find some of the more exotic ones that they will also just have a plug in themselves.

00:40:20.280 --> 00:40:25.100
Now, one thing that also helps under the hood is that OpenAI has a standard under the hood.

00:40:25.180 --> 00:40:27.560
Their SDK has become a bit of a standard across industry.

00:40:28.160 --> 00:40:31.260
So you can also reuse the OpenAI stuff.

00:40:31.420 --> 00:40:38.980
It would not surprise me at all if you were just to change a URL in the setting, But you can also connect to Mistral via the OpenAI objects.

00:40:39.090 --> 00:40:41.080
I would have to double check, but that wouldn't surprise me.

00:40:41.900 --> 00:40:48.300
Yeah, that is a very common way of just like, you know what, we're all going to just adopt Open.

00:40:48.920 --> 00:40:49.800
It's a little like S3.

00:40:50.340 --> 00:40:53.580
Like when I was saying S3 earlier, I was actually talking to DigitalOcean.

00:40:54.500 --> 00:40:55.200
But it doesn't matter.

00:40:55.440 --> 00:40:57.280
I'm just still using Boto3 to talk to it.

00:40:57.840 --> 00:40:59.040
Yeah, it does feel weird.

00:40:59.070 --> 00:41:00.160
Like, oh, you want to use DigitalOcean?

00:41:00.340 --> 00:41:04.320
Well, you have to download an SDK from a competing cloud provider.

00:41:04.700 --> 00:41:05.560
and then you can.

00:41:06.339 --> 00:41:07.280
Exactly. It's so weird.

00:41:08.940 --> 00:41:14.280
But the thing I do find that it's just a little bit funny here is like technically this thing is meant to be used from the command line.

00:41:15.200 --> 00:41:26.260
It just happens that it's written in Python and it just happens that also it has a decent Python API and that's the thing I actually end up using more than stuff from the command line. That's because I do a lot of things in notebooks. So the stuff that I tend

00:41:26.280 --> 00:41:28.420
to use is a little bit more on Python side.

00:41:29.120 --> 00:41:30.380
Sure, of course. Same here.

00:41:31.380 --> 00:41:45.600
Yeah, I don't think I would use it very much on the command line. But you know, I can see using it as a step in a series of things happening on the command line, like some sort of orchestration, like X, Y, ask the LLM, Z, you know?

00:41:45.980 --> 00:41:55.840
Well, the main thing I actually use it for from the command line is you can do a git diff on whatever you're working on and then sort of pipe that through to the LLM to make the commit message.

00:41:56.120 --> 00:42:03.740
There are these little moments like that where you do want to have a little automation, maybe in CI, and being able to do this from the command line is definitely useful.

00:42:04.340 --> 00:42:12.600
it's just not the thing I use it for most if I use it that way it's the thing I automate once and then never really look at it but the interaction really for me is more in the notebook

00:42:17.319 --> 00:42:24.680
very cool library definitely neat for one thing that might be fun to point out because that's something I built I think

00:42:25.380 --> 00:42:33.840
around the same time if you were to type into Google smartfunk and then my GitHub alias Koning

00:42:35.400 --> 00:42:36.000
because it's a little

00:42:36.350 --> 00:42:39.860
yep did it okay you already had it open good man no I'm really good with Google

00:42:40.180 --> 00:42:46.120
actually I use a start page these days but I'm sure I could find it there oh you have an ergonomic keyboard so there you go

00:42:46.400 --> 00:42:55.320
quick typing I made it already yeah no so a thing I was able to build on top of the thing that Simon Willison made and it's something that appears in the course as well but if you could scroll down

00:42:55.840 --> 00:42:55.960
yeah

00:42:57.700 --> 00:43:05.740
and then I think I have an example just below that yeah there you go so Basically what this function does is you can add a decorator.

00:43:08.180 --> 00:43:09.340
You just have a normal Python function.

00:43:09.760 --> 00:43:12.180
The inputs, you just put types on it.

00:43:12.220 --> 00:43:15.480
So you can say, this input is a string, this input is an integer, or what have you.

00:43:16.540 --> 00:43:22.780
You then add a decorator that says what back end you want to use, so GPT-4 or anything that Simon Wilson supports.

00:43:23.300 --> 00:43:26.840
And then the doc string is the prompt you give to the LLM.

00:43:27.160 --> 00:43:31.220
So you can do something like, hey, I have a function that accepts a term called paragraph.

00:43:31.820 --> 00:43:32.240
That's a string.

00:43:32.840 --> 00:43:35.140
And then the doc string of the function says, summarize this.

00:43:35.450 --> 00:43:36.420
And then lo and behold,

00:43:36.670 --> 00:43:38.520
you now have a Python function that can do summaries.

00:43:39.440 --> 00:43:40.000
I see.

00:43:40.460 --> 00:43:44.740
And the doc string can be effectively a Jinja template text, right?

00:43:45.200 --> 00:43:46.700
If you wanted to, it could be a Jinja template.

00:43:47.339 --> 00:43:52.280
Alternatively, what you can also do is you can also say, well, what the function returns is the prompt.

00:43:52.420 --> 00:43:53.680
That's also something you can do.

00:43:54.200 --> 00:43:54.500
Got it.

00:43:55.640 --> 00:43:59.480
But this is just a very quick hack for some of the stuff that I want to do real quick.

00:44:00.540 --> 00:44:06.160
I don't necessarily recommend other people to use this, but this is something you can build on top of the LLM library from Simon Willison.

00:44:06.440 --> 00:44:10.660
So if you want to make your own syntactic sugar, your own abstractions, it's definitely something that you can also do.

00:44:12.000 --> 00:44:14.420
And also, again, the name of the game here is Quick Iteration.

00:44:15.380 --> 00:44:20.020
So also feel free to think of your own tools now and again, but you want the base layer to be as boring as possible.

00:44:20.240 --> 00:44:23.300
And Simon Willison's library, again, does that in a good way.

00:44:23.640 --> 00:44:24.400
Right, right.

00:44:24.700 --> 00:44:26.480
Just don't make me think about the details.

00:44:27.240 --> 00:44:28.880
Let me swap out the model providers,

00:44:29.420 --> 00:44:32.960
which is actually doing quite a bit, but not conceptually.

00:44:33.560 --> 00:44:33.780
Yeah.

00:44:35.300 --> 00:44:44.120
So again, one thing I do think at this point in time, if you've not played around with this software stuff already, I do recommend maybe doing that sooner rather than later.

00:44:44.720 --> 00:44:48.740
And I think Simon Willis' approach, if you're already a Python person, is probably the simplest way to get started.

00:44:48.920 --> 00:44:53.160
But I do want to warn people in the sense of these are just tools.

00:44:53.940 --> 00:44:55.260
Tools are useful. Tools are great.

00:44:56.300 --> 00:45:00.140
But at some point, it's not so much about the tools that you use, but more about how you think about the tools.

00:45:00.420 --> 00:45:08.400
The way you interact with these tools, you can use a hammer in many different ways, but the way you think about a hammer is also equally important.

00:45:10.220 --> 00:45:11.180
Yeah, 100%.

00:45:11.980 --> 00:45:21.620
So speaking of thinking about how this goes, one of the challenges is, so let's just take this example here.

00:45:22.820 --> 00:45:28.520
It says, the example on your smartphone, GitHub Readme, It says generate summary, take some text.

00:45:28.560 --> 00:45:31.900
It says generate a summary of the following text, colon text.

00:45:32.040 --> 00:45:32.240
Yes.

00:45:32.480 --> 00:45:32.640
Yes.

00:45:32.900 --> 00:45:33.480
Super straightforward.

00:45:34.140 --> 00:45:41.660
But what if you, what if you said, change the prompt a little bit, said, give me all of the Python libraries discussed in the following markdown content.

00:45:42.700 --> 00:45:43.100
Right.

00:45:43.480 --> 00:45:53.980
And do you want a novella back, like a novel back when, when really what you want is like, I want a list of strings and URLs or whatever.

00:45:54.060 --> 00:45:58.760
Or how do I get programmability instead of just chatability, I guess?

00:45:59.160 --> 00:46:13.360
Yeah, so the thing that Simon Willison's library does allow you to do is you are able to say, well, I have a prompt over here, but the output that I'm supposed to get out, well, that has to be a JSON object of the following type, and then you can use Pydantic.

00:46:13.600 --> 00:46:21.060
So you can do something like, well, I expect a list of strings to come out over here in the case that you're trying to detect Python libraries or Pokemon names or what have you.

00:46:21.520 --> 00:46:23.220
Or you can say, well, this is more like a classification.

00:46:23.540 --> 00:46:28.040
I believe you can pass a literal with only these values and also constraint.

00:46:28.150 --> 00:46:33.500
And the reason that you can do that is some of these LLMs are actually trained on specific tasks.

00:46:33.800 --> 00:46:35.440
One of these tasks could be tool calling.

00:46:35.960 --> 00:46:42.900
Another one of these tasks that I'm trained for these days is that you have, I have to say the right word here, something, something, output, structured output.

00:46:44.340 --> 00:46:51.820
So that the LLM can sort of give a guarantee that if you declare that you get a list of strings out, that you actually do get a list of strings out.

00:46:51.980 --> 00:46:53.940
And that's something that these things are typically trained for.

00:46:54.120 --> 00:46:56.060
So that's something you can do.

00:46:56.870 --> 00:46:59.920
If you're using open source models, definitely check them out.

00:47:00.060 --> 00:47:00.320
They're cool.

00:47:02.240 --> 00:47:04.380
But quality does vary, is what I will say.

00:47:05.370 --> 00:47:06.880
So that's something that always keeps in the back of your mind.

00:47:06.930 --> 00:47:15.300
And also, the more complex you make your Pydanic objects, like if you make a nested thing of a nested thing of a nested thing, at some point the LLM is going to have a bit of trouble with that.

00:47:16.800 --> 00:47:17.920
Even though Pydanic is, of course, great.

00:47:18.160 --> 00:47:21.900
But you make it harder for the LLM to do the right thing at some point.

00:47:23.260 --> 00:47:41.880
but that's also a thing that definitely you want to use if you're doing stuff with software the really cool thing about PyDonic is you can say I expect something that looks like this to come out and you can force an LLM to also actually do that you will get the right types that come out the contents might still be maybe a bit spooky but at least you get the right types out

00:47:41.970 --> 00:47:49.600
which makes it a lot more convenient to write software you can say I can loop over the things that's sent back and you're not into like vague text parsing

00:47:51.539 --> 00:48:39.120
land you get is, I remember when the first OpenAI models sort of started coming out, I worked back at Explosion. We made this tool called spaCy there. And one of the things that we were really bummed out about was LLMs can only really produce text. So this structural, like if you want to detect a substring in a text, for example, which is the thing that spaCy is really good at, then you really want to guarantee, well, I'm going to select a substring that actually did appear in the text. I want to know exactly where it starts and exactly where it ends. And that's the span that I want to to select. So normally in NLP, like text comes in and structured information comes out. But here come these LLMs and text comes in and text comes out. And then you got to figure out a way to guarantee that structured output actually comes out. So that's, that's the thing that the LLM providers also acknowledge. So that's something that they're actually training these models for.

00:48:39.200 --> 00:48:48.680
But that's, I think it's a lot better these days, but it was definitely a huge part of the problem, like three to four years ago. And when these things just got started, it was super annoying.

00:48:51.500 --> 00:48:57.080
So if you use Pydantic, you define a Pydantic model that drives from Pydantic.basedModel.

00:48:57.360 --> 00:49:00.380
You put the types, type information, fields, etc.

00:49:01.320 --> 00:49:02.460
And then what does it do?

00:49:02.540 --> 00:49:07.740
Does it kind of use the schema generation that Pydantic already has?

00:49:07.880 --> 00:49:09.980
Like people are familiar with FastAPI.

00:49:10.420 --> 00:49:16.420
If you set the response model, it'll do OpenAPI spec, which I believe just comes more or less from Pydantic, right?

00:49:17.320 --> 00:49:20.860
I think it's a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of a bit of JSON standard in the end.

00:49:21.120 --> 00:49:23.020
You know how you can generate JSON schemas as well?

00:49:23.480 --> 00:49:28.720
I think it's that spec that it's using under the hood that is then also giving off to OpenAI or what have you.

00:49:28.780 --> 00:49:31.660
So I think in the end, it's a JSON spec, but I could be wrong.

00:49:31.680 --> 00:49:33.940
But it's driven by the Pydantic model.

00:49:34.160 --> 00:49:35.820
And then that's given as part of the prompt.

00:49:35.980 --> 00:49:36.920
Here's my question.

00:49:37.140 --> 00:49:38.640
Your answer will look like this.

00:49:39.180 --> 00:49:40.240
And it uses the Pydantic.

00:49:40.420 --> 00:49:41.080
Something to that effect.

00:49:41.620 --> 00:49:48.040
Yeah, there's one small caveat there in the sense that you can also add custom types with Pydantic.

00:49:48.140 --> 00:49:51.220
So you can do something like, well, this is a non-negative integer, for example.

00:49:51.940 --> 00:49:59.060
And you can definitely imagine that, especially with these validations that you can write in Python, not everything that you can come up with there is going to be supported by this JSON spec.

00:50:00.060 --> 00:50:05.660
So you are going to get some of these weird edge cases maybe where PyTonic can do more than what the JSON spec can do on your behalf.

00:50:06.540 --> 00:50:09.200
There is a really cool library, though, called Instructor.

00:50:09.779 --> 00:50:11.440
Yes, I was just thinking about that.

00:50:12.920 --> 00:50:14.600
So what they do is actually kind of cute.

00:50:15.240 --> 00:50:25.460
So the impression I have is that they acknowledge that PyDonic can actually do a bit more than what the JSON spec can do, especially because the validator that you write can really be a custom whatever thing you care about.

00:50:26.960 --> 00:50:40.040
So what they do as a trick is they say, well, if the LLM gives me something back that doesn't get validated by PyDonic, we take the error message together with the thing that we got back, make a new prompt to the LLM, and just ask it again.

00:50:41.140 --> 00:50:44.840
And see, maybe now with the hint, it is able to give us the proper structure back.

00:50:45.140 --> 00:50:54.060
It sounds silly, but anyone who's done agentic AI and stuff, like, you know, Claude or whatever, that's a lot of the magic where you say, it tried this.

00:50:54.240 --> 00:50:54.860
Oh, I see.

00:50:55.340 --> 00:50:56.360
I actually forgot an import.

00:50:56.500 --> 00:50:57.100
Oh, I see.

00:50:57.310 --> 00:50:58.320
I passed the wrong command.

00:50:58.330 --> 00:50:59.580
Let me try again a different way.

00:50:59.630 --> 00:51:00.360
Oh, now it's working.

00:51:00.560 --> 00:51:03.260
And, you know, I think there's a lot of potential with that.

00:51:03.840 --> 00:51:05.180
So, well, definitely true.

00:51:05.290 --> 00:51:09.960
The one thing I think is safe to say at this point, although another researcher, right?

00:51:10.090 --> 00:51:11.200
So, like, don't pin me down on this.

00:51:11.660 --> 00:51:17.900
That library did originate from an early era of LLMs when the structured stuff wasn't really being trained for yet.

00:51:18.560 --> 00:51:24.780
And it is my impression that LLMs have gotten a fair bit better at this structured task at this point.

00:51:24.960 --> 00:51:30.160
So you could argue that maybe that library is solving a solved problem with the technique that they've got.

00:51:30.700 --> 00:51:39.820
But I still don't want to discount it because you inevitably, at some point, you might want to go for a very lightweight model that you can run locally that isn't fine-tuned on the structured output task.

00:51:39.900 --> 00:51:43.700
And in that situation, this technique can actually do a lot of good.

00:51:43.840 --> 00:51:52.680
You might be prompting the LLM for three times, but you might be able to get away with that because the model is just so much lighter because it doesn't have to do all those tasks that those big LLMs are trained for.

00:51:52.980 --> 00:51:54.820
Right, especially if you're running on your machine

00:51:55.200 --> 00:51:57.020
or you're self-hosting the model.

00:51:57.220 --> 00:51:59.160
You're not like, oh, it's going to cost us so much money.

00:51:59.380 --> 00:52:01.540
It's just, it'll take a little longer, but it's fine.

00:52:02.020 --> 00:52:04.560
Yeah, so there's a lot of support for Instructored.

00:52:04.980 --> 00:52:08.620
There's also a lot of support for the LLM library from Simon Willison, by the way.

00:52:09.360 --> 00:52:14.060
But yeah, you can definitely imagine this is still going to be relevant with the Ollama stuff going forward.

00:52:14.490 --> 00:52:17.700
And also, it's a reasonable trick if you think about it.

00:52:18.760 --> 00:52:23.860
The main thing that we do is we just add a retry mechanic to the structured outputs by using the PyDonic validator, effectively.

00:52:24.030 --> 00:52:28.020
Yeah, I'm going to think about what you probably do, certainly what I do when I'm working with ChatGPT or something.

00:52:28.420 --> 00:52:30.360
It gives me a wrong answer like, no, this is not what I wanted.

00:52:31.020 --> 00:52:32.640
It's like, oh, okay, I see.

00:52:34.000 --> 00:52:35.760
It's kind of automating that, more or less.

00:52:36.920 --> 00:52:37.980
It's certainly an attempt, yeah.

00:52:39.280 --> 00:52:45.920
In my experience, I have also seen moments when it sends the same error back five times and then just kind of fails.

00:52:46.030 --> 00:52:47.940
So there is also a limit to the retry mechanic.

00:52:48.220 --> 00:52:48.840
Sure, sure.

00:52:49.380 --> 00:52:52.060
It's a little bit like tenacity or stamina.

00:52:52.680 --> 00:52:53.020
Exactly.

00:52:53.030 --> 00:52:54.400
Or APIs or whatever.

00:52:54.980 --> 00:52:56.900
It uses tenacity under the hood, if I'm not mistaken.

00:52:57.070 --> 00:52:59.200
So there's definitely that mechanism in there.

00:53:00.540 --> 00:53:00.860
Nice.

00:53:02.020 --> 00:53:02.320
Didn't work.

00:53:02.410 --> 00:53:02.800
Try it again.

00:53:03.280 --> 00:53:04.580
It's like turn it off and turn it on again.

00:53:04.690 --> 00:53:05.340
See if it works now.

00:53:05.840 --> 00:53:06.460
Yep, pretty much.

00:53:07.140 --> 00:53:11.640
Yeah, a little more smart than that, though, because it does validate and return the error.

00:53:11.860 --> 00:53:13.060
So super cool.

00:53:13.840 --> 00:53:14.000
Yep.

00:53:15.940 --> 00:53:16.080
Okay.

00:53:16.840 --> 00:53:23.700
Have you worked with, I guess, I don't know, higher order programming models?

00:53:23.800 --> 00:53:29.240
So, you know, I saw Samuel Colvin yesterday talking about Pydantic AI.

00:53:29.620 --> 00:53:37.820
I saw on X, I saw Sydney Ruckel talking about some big release, like a 1.0 release or something at LangChain.

00:53:38.680 --> 00:53:42.180
what are your thoughts on some of these higher order orchestration libraries

00:53:44.520 --> 00:54:08.940
I mean the main thing for me at least, this was a demo I did do with the Pydanic AI one I do think types are quite powerful when you think of LLMs there's a live stream you can check on the Marimo YouTube if you're keen to see like a full demo, but let me just sketch you a problem that used to be really hard like six years ago, we used to work at this company called Raza.

00:54:09.080 --> 00:54:10.780
We work on chatbot software.

00:54:11.400 --> 00:54:15.280
A thing that was really hard is you, let's say you're a chatbot and you can order a pizza.

00:54:16.680 --> 00:54:18.900
So you can do things like, yep, that's the one.

00:54:19.440 --> 00:54:21.280
It's like also one of the Pydanic AI engineers.

00:54:21.700 --> 00:54:23.560
And he's also the creator of Starlet, by the way.

00:54:23.760 --> 00:54:24.240
Cool project.

00:54:24.480 --> 00:54:25.540
Marimo's built on top of Starlet.

00:54:26.360 --> 00:54:27.540
He's the main maintainer of that project.

00:54:27.820 --> 00:54:29.660
He deserves to get more high fives from the community.

00:54:29.900 --> 00:54:30.940
So do you want to point it out?

00:54:32.400 --> 00:54:33.840
Well, let's say you're working on a pizza bot.

00:54:35.020 --> 00:54:36.520
And okay, someone is ordering a pizza.

00:54:37.099 --> 00:54:40.520
Suppose you go to the bot and you say, hey, I want to order a pepperoni pizza.

00:54:41.180 --> 00:54:42.480
But you can imagine there's a pizza type.

00:54:43.020 --> 00:54:44.680
And the pizza type says, what's the kind of pizza?

00:54:44.940 --> 00:54:45.840
What's the size of the pizza?

00:54:46.260 --> 00:54:47.260
And do you want to have extra toppings?

00:54:47.600 --> 00:54:49.400
And that has to be a list of ingredients, let's say.

00:54:50.359 --> 00:54:56.440
But if I tell you, hey, you want to have a pepperoni pizza, but I don't know the size yet.

00:54:57.040 --> 00:54:59.620
Well, then I need the LLM to ask a good follow-up question.

00:55:01.960 --> 00:55:03.620
Okay, how are you going to build that logic?

00:55:04.460 --> 00:55:05.980
Because that can get quite gnarly quite quickly.

00:55:06.040 --> 00:55:07.680
Especially if, in this case, the pizza is simple.

00:55:07.760 --> 00:55:10.840
But you can come up with a complicated object, like a complicated form almost.

00:55:11.600 --> 00:55:12.020
And then...

00:55:12.380 --> 00:55:15.740
I only need to ask this question if they've answered that one in that way.

00:55:16.540 --> 00:55:16.640
Right.

00:55:16.840 --> 00:55:18.360
It could get really tricky, yeah.

00:55:18.700 --> 00:55:22.400
Well, and then the question is, how can you actually get the LLM to do this for you?

00:55:22.860 --> 00:55:24.340
And there's a trick that you can do with a type.

00:55:24.460 --> 00:55:30.400
Because what you can say is, well, the response from the LLM has to be of type pizza or bar string.

00:55:32.020 --> 00:55:36.340
And then if the prompt says something along the lines of, well, parse the output of the user such that it is a pizza.

00:55:37.020 --> 00:55:40.980
But then if this validation error occurs, oh, how do you express that logic?

00:55:41.220 --> 00:55:42.320
Well, you can express it with a type.

00:55:42.880 --> 00:55:46.300
You just say this is a string, or it's the type that I'm actually interested in the pizza.

00:55:46.800 --> 00:55:54.020
And then the LLM has to figure out, well, is the response either going to be the string where I'm asking the user for missing information, or is it going to be the pizza object?

00:55:54.160 --> 00:55:56.720
And if it's a pizza object, then the code will actually talk to the backend.

00:55:58.859 --> 00:56:03.480
So you do get to rethink how typing works because we have LLMs at our disposal now.

00:56:03.580 --> 00:56:09.500
You can actually have the LLM handle some of that logic as long as your type is strict about it or well-defined, I should say.

00:56:11.420 --> 00:56:22.640
It's a bit of a weird advanced feature of types and LLMs, but I can definitely imagine some of the Pydanic AI stuff does use this trick in order to perform some of the higher order stuff.

00:56:23.380 --> 00:56:30.060
And maybe you don't need a full framework as long as you are really deliberate about your types.

00:56:30.660 --> 00:56:33.480
That's kind of the mentality I try to have right now.

00:56:33.680 --> 00:56:36.560
Let's not worry about the big framework just yet.

00:56:36.570 --> 00:56:44.520
If we can just focus on the right types and make sure that all that stuff is kind of sane, then you also keep the abstractions at bay, which is, I think, also convenient, especially early on.

00:56:44.760 --> 00:56:49.620
It sounds a little bit like an analogy you might be able to draw with databases.

00:56:50.360 --> 00:56:59.860
You could have a really strong, structured Postgres or relational database where the database thing, aka the AI, is in charge of it.

00:57:00.440 --> 00:57:07.620
Or you could be using an ODM or ORM where the code will only accept responses.

00:57:08.200 --> 00:57:13.360
So for example, you can get away talking to a document database if you have strongly typed models.

00:57:13.440 --> 00:57:18.120
If you're just passing dictionaries, you're going to end up in a bad place real quick because something's not going to match.

00:57:18.720 --> 00:57:23.660
So it's kind of like the Pydantic thing is being a check for if it's not quite perfect.

00:57:24.000 --> 00:57:24.260
I don't know.

00:57:24.500 --> 00:57:25.560
I feel like there's an analogy there.

00:57:25.740 --> 00:57:27.000
There's a bit of an analogy there.

00:57:28.920 --> 00:57:34.600
The main thing with the LLM is like, oh, how much of the logic do you want to control yourself as a guarantee?

00:57:35.320 --> 00:57:36.920
And how much would you like the LLM to do it?

00:57:37.980 --> 00:57:40.440
And oh, that's always kind of a struggle.

00:57:40.640 --> 00:57:42.100
Like what is the right glue for that?

00:57:42.320 --> 00:57:47.080
And it turns out that Pydantic is in a very funny, unique position to say, how about we just use types?

00:57:48.320 --> 00:57:49.500
And that actually just kind of works.

00:57:49.780 --> 00:57:55.420
How ironic is that that that's such a popular thing coming out of a effectively untyped language?

00:57:58.220 --> 00:58:04.880
uh yeah maybe that's why it's so popular you know i think it partly is i mean it's it's my impression

00:58:04.960 --> 00:58:25.000
that there's a lot of this stuff also happening on the javascript side of things i'm honest now for the javascript side of things the argument is of course oh you can also build the web uis a bit easier and the user experience and like that whole story but they also do a lot of this validation stuff i forget the library but there there's also like um i'm learning a little bit of javascript now that like there's actually pedantic things happening in javascript land as well yeah it's

00:58:25.020 --> 00:58:26.100
probably gigantic.

00:58:26.670 --> 00:58:27.440
It's probably what it is.

00:58:28.940 --> 00:58:29.800
Just dantic.

00:58:30.940 --> 00:58:32.880
Just dantic, yeah, JS dantic.

00:58:34.040 --> 00:58:35.280
Or TypeScript or whatever.

00:58:35.450 --> 00:58:40.600
But there's also like ORM libraries for JavaScript that are like type E, but not like super TypeScript

00:58:40.600 --> 00:58:40.980
just yet.

00:58:41.700 --> 00:58:44.620
Yeah, there's a lot of interesting analogies from the JavaScript space for us.

00:58:45.180 --> 00:58:52.340
You know, like the whole TypeScript team just rewrote everything in Go, whereas we're seeing a lot of stuff in Python being modified.

00:58:52.540 --> 00:58:52.800
Rust.

00:58:53.520 --> 00:58:53.960
Yeah, exactly.

00:58:54.480 --> 00:58:55.960
So pretty interesting.

00:58:57.220 --> 00:59:01.700
But let's close this out a little bit with like running some of our own models.

00:59:01.900 --> 00:59:02.000
Okay.

00:59:02.160 --> 00:59:10.060
So clearly there's Anthropic, there's OpenAI, there's, you know, all the big, huge foundational models.

00:59:12.160 --> 00:59:16.960
But some of these things we're talking about is like this identity schema validation type of thing and so on.

00:59:17.640 --> 00:59:20.500
You know, maybe we could, as you said, get away with running a smaller model.

00:59:20.620 --> 00:59:29.080
maybe small enough that we could run it ourselves on our servers or on, on my Mac mini here, M2 pro 32 gigs of Ram.

00:59:29.140 --> 00:59:39.440
I have the,  the open AI 20 billion parameter model running as like my, my default LLM for any of my code that I write that needs to talk to an LLM.

00:59:40.340 --> 00:59:42.200
So,  if you, yeah,

00:59:42.260 --> 00:59:45.940
so we actually have a video on the Maroon channel on that one, but go check that out later.

00:59:46.440 --> 00:59:48.240
But basically I've recorded that link.

00:59:48.240 --> 00:59:48.920
We'll put it in the show notes.

00:59:50.020 --> 00:59:50.080
Yeah.

00:59:50.200 --> 00:59:55.560
I kind of find myself exploring all these different open source LLMs like once every two months or so.

00:59:55.670 --> 01:00:06.500
And then Marimo has a couple of interesting integrations that makes it really easy to either hook into Olama or, and that's like the only thing I might change on the course that we made, there's also this service called Open Router.

01:00:06.800 --> 01:00:07.500
Like I don't know if you've heard.

01:00:07.660 --> 01:00:08.720
Yes, I've heard of it.

01:00:09.800 --> 01:00:12.000
Because I was looking at Klein.bot.

01:00:12.110 --> 01:00:13.200
Are you familiar with Klein.bot?

01:00:13.330 --> 01:00:13.580
Oh, yeah.

01:00:14.080 --> 01:00:14.460
Klein is cool.

01:00:14.720 --> 01:00:15.360
Klein is really cool.

01:00:15.680 --> 01:00:17.000
I've been using that for a bit.

01:00:18.320 --> 01:00:19.440
Tell people what Klein is real quick.

01:00:20.140 --> 01:00:27.640
So Klein is basically co-pilot in 4VS Code, but the one thing that they do is they really let you do any model, like just plug it in.

01:00:28.140 --> 01:00:32.000
And they are really strict about plan mode versus like run mode, if it makes sense.

01:00:32.120 --> 01:00:35.420
So you really got to, and one, the UI is kind of cool.

01:00:35.510 --> 01:00:41.320
So like every time that you're like planning or doing whatever, you kind of see the progress bar of your context.

01:00:41.770 --> 01:00:43.300
And then you also see how much money it costs.

01:00:43.440 --> 01:00:46.960
So you're like always aware of the fact that like, okay, this thing is costing me bucks now.

01:00:47.100 --> 01:00:57.580
and it's just a refreshing take on GitHub Copilot and I will say you can't do sessions in parallel because it is, I think at the moment, still stuck to the VS Code extension ecosystem.

01:00:57.920 --> 01:01:00.680
They just introduced a CLI.

01:01:01.300 --> 01:01:01.960
Ah, there you go.

01:01:02.400 --> 01:01:03.700
I bet you can do it with that.

01:01:04.380 --> 01:01:05.580
Yeah, so that's also it.

01:01:05.660 --> 01:01:13.420
The main thing that drew me in was the whole, oh, they're very strict about plan mode versus go mode, which I think is definitely super useful.

01:01:14.300 --> 01:01:15.980
And on top of that, we have some nice UI.

01:01:16.540 --> 01:01:18.380
Yeah, it's also real similar to Cursor and others.

01:01:19.120 --> 01:01:23.920
And I think it's a big selling point is open source and they don't charge for inference.

01:01:24.010 --> 01:01:26.400
So you just put in your API key and it just passes through.

01:01:26.470 --> 01:01:26.920
But you're right.

01:01:26.930 --> 01:01:31.140
They do have like, as it's thinking, it's got a little tick, tick, tick price of this operator.

01:01:31.220 --> 01:01:33.560
And it's like, oh, it's up to three cents, five cents.

01:01:34.420 --> 01:01:36.460
You know, it's just interesting to watch it go.

01:01:36.700 --> 01:01:38.520
You also, it's kind of like the Fibonacci sequence.

01:01:38.610 --> 01:01:39.800
You kind of realize that it's additive.

01:01:39.990 --> 01:01:42.300
So it's like, oh, I've spent like half a dollar now.

01:01:42.820 --> 01:01:45.300
Oh, and the next problem is going to be half a dollar plus extra.

01:01:45.840 --> 01:01:47.520
The next one's going to be like that extra plus time.

01:01:47.850 --> 01:01:50.380
MARK MIRCHANDANI: Yeah, it makes you manage your context size real well.

01:01:50.490 --> 01:01:52.680
So anyway, OK, that's a bit of a diversion.

01:01:52.750 --> 01:01:54.540
But I think this is like a fun tool to shout out.

01:01:55.230 --> 01:01:55.760
JOHN MUELLER: No, I agree.

01:01:55.760 --> 01:01:57.600
MARK MIRCHANDANI: So the reason I brought this up

01:01:57.670 --> 01:02:00.160
is I saw they were number one on Open Router.

01:02:00.240 --> 01:02:01.320
I'm like, wait, what's Open Router?

01:02:01.840 --> 01:02:07.400
JOHN MUELLER: Yeah, so Open Router is basically you have one API key, and then they will route to any LLM that you like.

01:02:07.920 --> 01:02:15.140
And one thing that's really cool about it is that if there's a new open source model that you see all at Twitter and YouTube sort of rave about, it'll be on Open Router.

01:02:15.400 --> 01:02:16.460
and then you can just give it a spin.

01:02:16.950 --> 01:02:27.940
And if you have, let's say, stuff in your cache, and you've got a couple of functions that you use for evaluation, and you've got a setup for that, then it's just changing a single string to give a new LLM model a try.

01:02:28.120 --> 01:02:30.440
And they will make sure everything is nicely routed.

01:02:31.580 --> 01:02:33.600
That's like the whole service also, basically.

01:02:34.050 --> 01:02:41.440
I think they add a 5% fee hike or something like that, but they've got all sorts of GPU providers competing for the lowest price.

01:02:41.950 --> 01:02:46.620
So they do all the routing to all the LLM models, but also the open source ones.

01:02:46.670 --> 01:02:47.740
And that's the main convenient bit.

01:02:49.140 --> 01:02:51.240
Because I might not have...

01:02:51.240 --> 01:02:56.840
So it's partially that I don't have the biggest GPU for the biggest models, but it's really convenient to just do kind of a for loop.

01:02:57.270 --> 01:03:07.260
Like, okay, take the 7B model from Quinn, the 14B model, and just loop over all of them and see if there's like a sweet spot as far as like quality is concerned and also model size.

01:03:07.420 --> 01:03:12.060
Like those kinds of experiments become a whole lot simpler if you just have one API that has all the models.

01:03:12.130 --> 01:03:13.620
And Open Router kind of gets you there.

01:03:14.280 --> 01:03:14.500
Yeah.

01:03:15.950 --> 01:03:16.040
OK.

01:03:16.360 --> 01:03:16.660
Very neat.

01:03:17.400 --> 01:03:19.040
And it's also why Klein likes using it.

01:03:19.200 --> 01:03:20.580
So they have a direct connection with Klein.

01:03:21.440 --> 01:03:24.060
Just in the user interface, you can say, well, I want to have this model from Open Router.

01:03:24.360 --> 01:03:26.200
Just click, click, click, and it's all sort of ready to go.

01:03:26.540 --> 01:03:26.660
Yeah.

01:03:26.780 --> 01:03:27.580
That's their service.

01:03:28.120 --> 01:03:28.480
OK.

01:03:28.740 --> 01:03:29.100
Very neat.

01:03:30.539 --> 01:03:35.380
So maybe for people who haven't run their own models, there's a couple of options.

01:03:35.720 --> 01:03:37.320
It sounds like you recommend Olama.

01:03:37.860 --> 01:03:39.240
I mean, that's the one that I have used.

01:03:39.740 --> 01:03:41.260
You've used LM Studio, I think?

01:03:41.560 --> 01:03:48.080
I use LM Studio because LM Studio has this UI for discovering and setting them up and playing and configuring them.

01:03:48.110 --> 01:03:49.840
But then you can just say, run in dev mode.

01:03:49.840 --> 01:03:51.740
And it just runs-- and here we go--

01:03:52.040 --> 01:03:56.200
an open AI compatible API endpoint on my local network.

01:03:56.410 --> 01:03:59.200
And so I can just point anything that can do ChatGPT.

01:03:59.210 --> 01:04:01.420
I can point it at this whatever is running there.

01:04:01.420 --> 01:04:04.800
And I was running the GPT OSS 20 billion one last time I tried.

01:04:05.450 --> 01:04:06.020
There you go.

01:04:06.270 --> 01:04:06.380
OK.

01:04:06.720 --> 01:04:08.960
So Ollama also has a very similar thing.

01:04:09.620 --> 01:04:11.920
My impression is that LM Studio in the end is pretty similar.

01:04:11.950 --> 01:04:13.460
It just has a slightly different UI.

01:04:13.990 --> 01:04:15.240
It's a bit more of an app, I suppose.

01:04:15.619 --> 01:04:17.900
OhLama is definitely more of a command line utility.

01:04:19.400 --> 01:04:23.560
Does the job, though, I would say, but it doesn't have that much to offer in the UI department.

01:04:24.500 --> 01:04:30.620
Yeah, I think if I were ever to try to set it up on my server, I'd probably just do OhLama, maybe inside a Docker or something.

01:04:30.670 --> 01:04:31.640
I don't know, something like that.

01:04:32.230 --> 01:04:34.120
Yeah, not true.

01:04:34.140 --> 01:04:37.340
The one sort of mysterious thing with a lot of these services is how do they make money?

01:04:37.980 --> 01:04:43.880
I think I read somewhere that Olama is going to start doing open router kind of stuff as well.

01:04:44.100 --> 01:04:46.240
So it wouldn't surprise me if they ended up going in that direction.

01:04:48.130 --> 01:04:49.520
But I mean, there's all sorts of vendors.

01:04:49.690 --> 01:04:58.500
And there's like another thing I'm also seeing a lot of nowadays that people use like big models that generate a data set such that they can take a super tiny model and fine tune on the data that was generated by the big model.

01:04:59.100 --> 01:05:04.120
And that way, like a future maybe that we might have is that you have a Dutch model that's really good for legal.

01:05:05.240 --> 01:05:07.600
Maybe we'll have like a super bespoke LM for that at some point.

01:05:08.200 --> 01:05:13.660
or like an LLM that's like really good at CSS, but only CSS.

01:05:14.030 --> 01:05:15.600
Like that's also a thing that we might have.

01:05:15.600 --> 01:05:17.880
And then those models can be really tiny and run on your laptop.

01:05:18.110 --> 01:05:20.640
And who knows, this is the age we live in.

01:05:21.299 --> 01:05:23.460
I certainly can foresee a world like that

01:05:23.580 --> 01:05:28.080
where it's like, okay, I'm going to work on a project instead of spinning up a general world knowledge model.

01:05:29.220 --> 01:05:31.280
I'm going to just let it on demand go.

01:05:31.390 --> 01:05:32.820
Okay, I have a CSS question.

01:05:33.100 --> 01:05:34.080
Well, what framework are you using?

01:05:34.160 --> 01:05:35.080
I'm using Tailwind.

01:05:35.220 --> 01:05:39.120
okay, we're going to spend up the Tailwind LLM model and we need to ask it a question.

01:05:39.840 --> 01:05:40.460
And we can shut that.

01:05:40.500 --> 01:05:40.940
You know what I mean?

01:05:40.980 --> 01:05:46.020
Like really focused, but then have some kind of orchestrating thing that says, now I got to talk to this.

01:05:46.140 --> 01:05:47.380
Now let me work with that one.

01:05:47.400 --> 01:05:48.060
But that could be great.

01:05:48.680 --> 01:05:53.740
Yeah, and it might also be a solution to the whole, oh, the time cutoff is January to 2025.

01:05:54.240 --> 01:05:56.700
And like everything that happened after, we have no knowledge about it.

01:05:56.700 --> 01:06:02.860
Like there could also be a way for us to have a system where we just retrain a thing weekly and it's just a diff on top or something like that, right?

01:06:03.540 --> 01:06:03.660
Yeah.

01:06:03.740 --> 01:06:04.620
That's something that could also happen.

01:06:05.080 --> 01:06:07.140
Yeah, and if it were small enough, then it would be okay.

01:06:08.020 --> 01:06:09.420
It would be kind of like SETI at home.

01:06:09.640 --> 01:06:12.340
All of our stuff would just kind of have the fan going.

01:06:14.140 --> 01:06:21.760
I mean, it's a bit of a science fiction thing, but you can imagine at some point every home will have solar panels, and then maybe every home will also just have a GPU.

01:06:22.500 --> 01:06:27.460
And maybe we won't have huge-ass server farms, it's just that we have a network, and everyone has a GPU.

01:06:27.690 --> 01:06:31.200
And if my neighbor suddenly wants to do math, then he can borrow my GPU.

01:06:31.350 --> 01:06:32.500
I don't know. We'll see.

01:06:33.920 --> 01:06:35.200
I mean, it kind of sounds cool, doesn't it?

01:06:35.839 --> 01:06:36.160
Yeah.

01:06:37.100 --> 01:06:41.000
If people like distributed systems, might as well go for it.

01:06:41.320 --> 01:06:49.640
If you want to add to the unpredictability of the LLM, mix in a distributed grid computing that you don't know about too much either,

01:06:49.800 --> 01:06:51.780
and then you'll get some real interesting.

01:06:52.280 --> 01:06:54.700
I do want to give one quick shout out on this one front, though,

01:06:54.800 --> 01:06:56.460
because I do think it's an idea that's worth pursuing.

01:06:56.680 --> 01:07:01.160
So in Holland, we've got this one cloud provider called LeafCloud, and I think they're still around.

01:07:01.920 --> 01:07:05.800
But what they do is they install these server farms like what we're discussing.

01:07:05.910 --> 01:07:10.060
They have like a GPU rack at the bottom of like an apartment building.

01:07:10.860 --> 01:07:18.060
And any excess heat, which, you know, is one of the main cost drivers, that's actually used to preheat the water for the rest of the apartment.

01:07:19.240 --> 01:07:25.080
So on paper, at least, you do have something that's not only like carbon negative, but also more cost effective.

01:07:25.200 --> 01:07:32.220
Because the waste product of the compute is now the heat that someone is willing to pay for as well.

01:07:32.380 --> 01:07:35.380
So on paper, you can also be cheaper if you play your cards right.

01:07:36.620 --> 01:07:41.240
I ran across this, and I was really inspired by it.

01:07:41.300 --> 01:07:43.140
I ended up not using Leaf Cloud for anything.

01:07:43.440 --> 01:07:44.540
But it's a cool idea.

01:07:45.780 --> 01:07:54.940
So the one downside of that approach is you do have to be close to a data center because do you really want to store your private data on disk in an apartment building or in a secure facility?

01:07:55.380 --> 01:07:57.480
So what they do is they just have glass fiber cables.

01:07:57.730 --> 01:08:02.180
Like the disk is in the facility, but the memory and the GPU and the computer is in the apartment building.

01:08:02.340 --> 01:08:03.200
That's sort of the way to do it.

01:08:03.240 --> 01:08:05.960
It's like a mounted volume over fiber.

01:08:06.100 --> 01:08:06.160
Yes.

01:08:06.460 --> 01:08:06.620
Yes.

01:08:06.980 --> 01:08:08.640
Like a network volume over fiber, sure.

01:08:08.870 --> 01:08:11.260
I think that's pretty common in a lot of data center setups.

01:08:11.410 --> 01:08:15.380
It just happens to be spanning an apartment complex, which is unusual.

01:08:16.400 --> 01:08:18.560
It's also only for certain types of compute loads, right?

01:08:18.720 --> 01:08:21.580
You're probably not going to do your web app via that apartment building.

01:08:21.779 --> 01:08:23.060
It's probably not going to be productive.

01:08:23.140 --> 01:08:27.100
But if you're doing big simulation, big deep learning things...

01:08:27.180 --> 01:08:33.779
Everyone knows you can do machine training, machine learning training, and it's all good for serverless.

01:08:33.779 --> 01:08:38.740
You just send the stuff out, it just goes, finds an apartment complex, runs it serverless, and it comes back, right?

01:08:38.940 --> 01:08:39.319
No, just kidding.

01:08:40.299 --> 01:08:41.180
No, it's a pretty neat idea.

01:08:43.280 --> 01:08:44.460
We'll see what the future brings.

01:08:45.460 --> 01:08:51.200
When I did data science, the joke I always made is like, as a data scientist, I know for sure that the future is really hard to predict.

01:08:54.520 --> 01:08:57.540
Maybe the past is easier to predict, and that's what we do in machine learning.

01:08:57.600 --> 01:08:58.540
We try to predict the past.

01:08:58.700 --> 01:09:01.240
And yeah, we'll see what these LLMs do.

01:09:01.420 --> 01:09:06.460
But until then, again, also the repeating lesson from the course, there's some good habits.

01:09:06.980 --> 01:09:08.660
But in the end, it's not so much the LLM.

01:09:08.759 --> 01:09:10.600
It's more the stuff around and how you think about it.

01:09:10.759 --> 01:09:13.000
And also, small shout out to Marimo.

01:09:13.299 --> 01:09:18.900
I do think having some UI in the blend as well, such as you can say, oh, there's a text box here, another text box there.

01:09:18.960 --> 01:09:22.220
You can kind of play around making your own little LLM app.

01:09:22.400 --> 01:09:27.220
The thing that I typically do is I hook up Gemini to my YouTube videos to make summaries for my blog.

01:09:27.660 --> 01:09:33.839
I try to find little excuses to improve your life by automating something with these LLM tools, and that's a nice way to get started.

01:09:34.060 --> 01:09:47.240
Yeah, there's really, I think we all probably have little opportunities to automate some low-stakes thing, but that we spend a lot of time with, or Hath or whatever and let an LLM have at it and write a little Python to make that happen, right?

01:09:47.799 --> 01:09:51.000
Yeah, and also expose yourself to your own slop as well.

01:09:52.400 --> 01:09:57.500
The nice thing about building your own apps is also because you then suddenly start to realize when it's also still good to be a human.

01:09:58.300 --> 01:10:02.280
I'm not going to have an LLM automate messages to my wife and my kid.

01:10:02.780 --> 01:10:05.700
That's just not going to happen because I've seen what these LLMs can output.

01:10:06.900 --> 01:10:08.680
That's going to remain on the human side of the spectrum.

01:10:09.160 --> 01:10:10.000
Yeah, exactly.

01:10:10.470 --> 01:10:14.040
But making a summary for my blog on a video, I mean, that feels in the ballpark.

01:10:16.040 --> 01:10:16.480
100%.

01:10:16.730 --> 01:10:16.980
All right.

01:10:18.260 --> 01:10:21.660
Final word for people interested in this topic they want to build.

01:10:22.900 --> 01:10:25.260
I want to plug in LLMs as part of that building block.

01:10:26.000 --> 01:10:26.380
What do you say?

01:10:27.880 --> 01:10:29.260
Open router is nice and flexible.

01:10:29.640 --> 01:10:30.940
LLM by Simon Willison is great.

01:10:31.500 --> 01:10:35.200
Disk cache is a thing that you do want to disk cache.

01:10:36.940 --> 01:10:39.180
Maybe a final thing that I do maybe want to...

01:10:39.580 --> 01:10:42.120
It's also fine if you just want to learn Python as well.

01:10:42.480 --> 01:10:45.940
There's some of this talk of people saying, oh, we never have to...

01:10:46.880 --> 01:10:48.900
Oh, actually, people from Peru are tuning in.

01:10:49.330 --> 01:10:49.780
Peru is amazing.

01:10:50.200 --> 01:10:51.960
I'm seeing people sort of reply in the comments.

01:10:51.980 --> 01:10:52.560
Yeah, absolutely.

01:10:53.070 --> 01:10:53.460
Oh, sweet.

01:10:54.540 --> 01:10:55.660
Best food of my life, Peru.

01:10:56.050 --> 01:10:59.200
Also, if you're looking for great excuses to learn Python, there's plenty.

01:10:59.250 --> 01:11:01.300
If you're looking for great excuses to go to Peru, food.

01:11:03.760 --> 01:11:10.620
But also, I think in the end, you are going to be better at using LLMs if you can still do things yourself.

01:11:10.980 --> 01:11:15.900
One thing I've started noticing is that it's a bit embarrassing, but like, oh, what's the CSS property of that thing?

01:11:15.910 --> 01:11:16.620
I used to know that.

01:11:16.670 --> 01:11:20.160
But nowadays I just, oh, am I seriously going to ask this to ChatGPT?

01:11:20.280 --> 01:11:22.040
This should be in my memory for Pete's sakes.

01:11:22.120 --> 01:11:22.780
Yes, yes.

01:11:22.810 --> 01:11:28.240
And sometimes it's like I'm going to have a whole conversation with ChatGPT so I can change four characters in a style sheet.

01:11:28.310 --> 01:11:29.000
What is this nonsense?

01:11:29.880 --> 01:11:35.260
So part of me is also thinking like definitely expose yourself to LLMs and if that inspires you, that's great.

01:11:35.740 --> 01:11:39.640
But also try to not overly rely on it either.

01:11:40.540 --> 01:11:42.280
I'm building flashcard apps for myself,

01:11:42.460 --> 01:11:44.960
so I'm still kind of in the loop, if that makes sense.

01:11:46.640 --> 01:11:48.660
You've got to be a guardian of your own mind.

01:11:49.540 --> 01:11:56.400
I can see it so easily that you just become a machine that just says, next, okay, yes, continue.

01:11:56.640 --> 01:11:59.140
And then in six months, you're like, gosh, I can't program anything.

01:11:59.400 --> 01:12:01.120
I just got to ask the chat.

01:12:01.300 --> 01:12:02.260
And it's a problem.

01:12:02.780 --> 01:12:04.240
Yeah, it's something.

01:12:04.400 --> 01:12:07.680
And also, that kind of gets back to the point I made earlier.

01:12:09.320 --> 01:12:13.980
You want to be very mindful that if you have better tools, you should also have better ideas.

01:12:14.180 --> 01:12:16.000
And if that's not the case, then something is wrong.

01:12:17.600 --> 01:12:20.840
Because then you get into the self-learned helplessness territory.

01:12:21.180 --> 01:12:23.240
And that's the one thing I do want to kind of guard against.

01:12:24.240 --> 01:12:29.700
Andre Karpathy gave a great interview a little while ago where he mentioned the worst outcome of humanity.

01:12:29.900 --> 01:12:31.040
Did you see the movie WALL-E?

01:12:31.680 --> 01:12:32.180
Remember seeing that?

01:12:33.280 --> 01:12:35.180
That movie really made me sad.

01:12:36.100 --> 01:12:37.060
It's a good movie, though.

01:12:37.300 --> 01:12:38.640
It's a fantastic movie.

01:12:38.880 --> 01:12:43.000
as an adult it's got a hard undertone to it oh my gosh

01:12:43.220 --> 01:13:03.020
well and especially now because for people who never watched the movie there's a scene in the movie where you see what humanity comes to and it's basically everything has to be convenient so people are overweight they're in these like bikes that move them around they're always able to watch television drink a milkshake and eat a pizza like every convenience is sort of taken care of but you also just see that they're

01:13:03.100 --> 01:13:10.660
not happy and they can't even walk out they can't they're like just they're so uh just dependent on the thing is bad.

01:13:11.260 --> 01:13:12.980
Exactly. It's a sweet show. It's a very sweet show.

01:13:12.980 --> 01:13:15.060
I recommend it. Yeah, it's a sweet movie,

01:13:15.180 --> 01:14:13.500
but if at some point you notice like, hey, oh, learning feels hard, then maybe that's fine because maybe it's just like exercising. It's okay if that's a bit painful. In fact, if it's not painful, then you're maybe just participating in entertainment instead as well. So that's the thing. These are thoughts that I'm sort of battling with. I'm also sort of trying to be really critical of like, okay, I work at Marima now. I've accessed all these LLMs and stuff and can I build more awesome software that I wouldn't have before like one thing I made a while ago is I added gamepad support to Marimo Notebooks so you can interact with Python with the gamepad and stuff and this is just a experiment but I would be really disappointed if I'm the only person making packages that I've never made before that are definitely trying to reach new heights and it does feel a bit lonely because it does feel like more people should do it that's like the one sort of hint I want to give to people try to inspire yourself a bit more and do more inspirational stuff so more cool things happen on my timeline. There's less AI really cool software being built, people learning and all that.

01:14:15.100 --> 01:14:18.460
Hear, hear. I appreciate that. All right. Well, Vincent, thanks for

01:14:18.680 --> 01:14:26.200
being back, sharing all these techniques. And yeah, I hope people go out there and build using AIs, not just with AIs.

01:14:27.360 --> 01:14:29.780
Definitely. Y'all have a good one. Yeah. See you later. Bye.

01:14:57.960 --> 01:14:58.600
Bye.

