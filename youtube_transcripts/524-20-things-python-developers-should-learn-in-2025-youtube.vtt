WEBVTT

00:00:00.679 --> 00:00:02.980
Hello, hello. Peter and Calvin,

00:00:03.320 --> 00:00:05.720
welcome back to Talk Python to both of you.

00:00:06.359 --> 00:00:07.240
It's great to be here.

00:00:07.670 --> 00:00:08.520
Great to be here. Thanks for having us.

00:00:09.059 --> 00:00:13.020
Yeah. I know you both are very passionate technologists

00:00:13.240 --> 00:00:16.160
and Pythonistas and we're going to dive

00:00:16.350 --> 00:00:18.000
into some really exciting things.

00:00:18.480 --> 00:00:23.240
What do people need to know as developers and data scientists in 2025?

00:00:24.360 --> 00:00:26.980
I'm going to take a wild guess and bet that these trends,

00:00:27.180 --> 00:00:30.300
most of them carry over to 2026, which is a few months.

00:00:33.740 --> 00:00:36.740
So let's just really quickly have both of you introduce yourselves,

00:00:37.100 --> 00:00:40.420
just because not everyone has had a chance to listen to every episode.

00:00:40.550 --> 00:00:42.740
And even if they did, they may not remember.

00:00:43.220 --> 00:00:44.460
So Peter, welcome.

00:00:44.490 --> 00:00:44.860
Who are you?

00:00:45.570 --> 00:00:46.600
PETER WANG: Hi, I'm Peter Wang.

00:00:46.760 --> 00:00:51.220
I'm a founder of Anaconda and the creator of the PyData community.

00:00:51.690 --> 00:00:55.500
And I'm sort of leading the advocacy, at least,

00:00:55.720 --> 00:00:59.140
and been at the center of evangelism for the use of Python

00:00:59.230 --> 00:01:00.820
in the data science and machine learning world

00:01:01.440 --> 00:01:04.180
for over 12 years now, I think 13 years at this point.

00:01:05.150 --> 00:01:08.280
But my day job is at Anaconda, I'm the chief AI officer.

00:01:08.790 --> 00:01:12.540
So I work on open source, community projects, innovation,

00:01:13.480 --> 00:01:15.860
looking at AI things and how that impacts our community

00:01:16.060 --> 00:01:20.820
and our users and what good could look like there for us.

00:01:20.850 --> 00:01:23.160
I mean, there's a lot of discussion on AI, of course,

00:01:23.420 --> 00:01:23.940
good, bad, and ugly.

00:01:24.520 --> 00:01:30.780
And I'm really trying to figure out if we as responsible open source community stewards, you know, want to have something meaningful to say here.

00:01:30.840 --> 00:01:31.900
What are the right things to do?

00:01:32.060 --> 00:01:34.120
So that's what I spend a lot of my time focused on.

00:01:34.600 --> 00:01:35.520
Yeah, that's really good work.

00:01:36.140 --> 00:01:37.120
Yeah, it's really good work.

00:01:37.200 --> 00:01:40.120
And congrats with all the access you've had at Anaconda.

00:01:40.340 --> 00:01:41.940
It's made a serious dent.

00:01:42.920 --> 00:01:48.100
You were featured in or you were part of the Python documentary, right?

00:01:48.560 --> 00:01:48.940
That's right.

00:01:49.420 --> 00:01:50.380
Yeah, that was really great.

00:01:50.620 --> 00:01:52.200
I really appreciated your words in there.

00:01:52.520 --> 00:01:52.920
Thank you.

00:01:53.240 --> 00:01:53.520
Thank you.

00:01:53.660 --> 00:01:54.200
Yeah, that was great.

00:01:54.340 --> 00:01:55.960
really honor to be included in that.

00:01:56.500 --> 00:02:01.020
Well, tell people, I haven't technically talked about it on the documentary or the documentary

00:02:01.140 --> 00:02:02.060
on the podcast very much.

00:02:02.240 --> 00:02:05.580
So you just give people a quick rundown on what that is and why they should check it out.

00:02:06.040 --> 00:02:09.520
Well, well, anyone who's listening to this podcast should absolutely watch the documentary

00:02:09.740 --> 00:02:13.520
because it is just got a cast of characters telling the story about how our favorite programming

00:02:13.640 --> 00:02:14.240
language came to be.

00:02:15.230 --> 00:02:20.000
All of the, not all, okay, not all, but some of the travails that have challenged us as

00:02:20.000 --> 00:02:24.020
the community over the period of time since its inception,

00:02:24.440 --> 00:02:25.460
30 years ago at this point.

00:02:26.800 --> 00:02:28.500
It's just a really fun, nice.

00:02:29.020 --> 00:02:32.540
I think it's weird because Python has been around forever,

00:02:33.380 --> 00:02:34.780
and yet in many respects,

00:02:35.080 --> 00:02:37.380
we are still, the world is changing,

00:02:37.580 --> 00:02:39.420
and I think there's lots of amazing new opportunities

00:02:39.500 --> 00:02:40.380
for Python as a language.

00:02:41.520 --> 00:02:44.580
We've been growing so fast and so much,

00:02:44.800 --> 00:02:47.140
and evolving as a language and as a community.

00:02:47.780 --> 00:02:54.860
This documentary, I think, is a nice way to sort of like check in and say, oh, wow, we got to here and here's the journey we've been on.

00:02:55.420 --> 00:03:02.480
And that gives us almost the space to then be a little bit more intentional about talking about where we want to go from here, which I think is something very important that we need to do as a community.

00:03:02.760 --> 00:03:06.800
So anyway, I just really liked it from philosophically speaking from that perspective.

00:03:07.660 --> 00:03:14.040
But it's also just fun just to get the perspectives like the CPython core maintainers and the BDFL and all the stuff on just the language over the years.

00:03:15.240 --> 00:03:16.620
Yeah, I thought it was really excellent.

00:03:17.320 --> 00:03:18.800
The production value is amazing.

00:03:19.220 --> 00:03:20.300
I enjoyed it tremendously.

00:03:21.140 --> 00:03:23.880
I really loved hearing all the old stories.

00:03:24.280 --> 00:03:25.700
I've been around for a long time in the community

00:03:25.900 --> 00:03:27.280
and seen all the familiar faces,

00:03:27.530 --> 00:03:30.360
and I feel like it gives a face and a level of empathy

00:03:30.450 --> 00:03:32.220
to the community that's needed.

00:03:33.140 --> 00:03:35.140
Yeah, I would say that the production quality

00:03:35.230 --> 00:03:37.680
was almost as good as Calvin's camera here.

00:03:39.740 --> 00:03:41.740
You always look great on these streams.

00:03:43.100 --> 00:03:44.260
Welcome. Tell people about yourself.

00:03:44.520 --> 00:03:46.060
Thank you, Michael. I appreciate that.

00:03:49.340 --> 00:03:54.140
well i guess i guess i can give a quick introduction um i'm calvin hendrix parker i'm cto and co-founder

00:03:54.280 --> 00:04:00.380
of six feet up we are a python and ai consulting agency who helps impactful tech leaders solve the

00:04:00.380 --> 00:04:06.160
hard problems i've been in the python community for ages um i probably don't outnumber peter in

00:04:06.300 --> 00:04:11.579
in years but at least since 2000 i've been involved i started with zoep and and then through that the

00:04:11.600 --> 00:04:16.640
plone community got very involved in the governance that open source project and now we do a lot of

00:04:16.820 --> 00:04:23.220
Django a lot of other python open source data projects like airflow for example i think that's

00:04:23.220 --> 00:04:29.460
on the list for later and so we just enjoy hanging out and being an awesome group of folks

00:04:29.470 --> 00:04:34.360
who love solving the hard problems yeah excellent yeah you've been doing it longer than me for sure

00:04:34.640 --> 00:04:40.680
i'm the baby well 2000 is about when i got involved in python as well so okay the old

00:04:40.640 --> 00:04:46.300
mail was supposed to be maybe from 99 but but basically 2000 yeah my first pike on was 2003

00:04:46.580 --> 00:04:51.500
and i think there were 250 people in the room uh it was amazing yeah you actually beat me by a

00:04:51.500 --> 00:04:57.320
couple years i went to i went to 05 was my first one at uh george washington university i think it

00:04:57.400 --> 00:05:04.279
was yeah in dc it was about 200 something people yeah they had a track and the keynote speakers

00:05:04.300 --> 00:05:04.700
Wow.

00:05:07.100 --> 00:05:10.560
I've only been doing this since 2011, so I'm just barely getting started.

00:05:12.380 --> 00:05:15.260
That used to seem pretty recent ago, but it doesn't anymore, oddly.

00:05:15.420 --> 00:05:17.960
No, it turns out it was, yeah, it's a long time ago.

00:05:18.120 --> 00:05:19.720
We're halfway through the 2020s now.

00:05:19.860 --> 00:05:20.200
It's crazy.

00:05:21.060 --> 00:05:21.220
Yeah.

00:05:21.550 --> 00:05:21.640
Yeah.

00:05:21.700 --> 00:05:25.860
When you said 2025, things that developers should learn in 2025, I was like, is this a

00:05:25.950 --> 00:05:27.300
science fiction movie we're talking about?

00:05:27.860 --> 00:05:28.220
Exactly.

00:05:28.310 --> 00:05:28.800
What is this like?

00:05:29.040 --> 00:05:30.400
It's a dystopian science fiction movie.

00:05:30.740 --> 00:05:32.260
the same crap we had to deal with in 2010.

00:05:33.720 --> 00:05:34.020
Mostly.

00:05:34.960 --> 00:05:36.720
Although async back then, it was interesting.

00:05:36.900 --> 00:05:38.940
We didn't have, you know, we had statless, I guess.

00:05:39.860 --> 00:05:42.360
There's, I don't know, 2010.

00:05:42.800 --> 00:05:43.040
There's tornado.

00:05:43.340 --> 00:05:45.720
Yeah, there were various async systems.

00:05:46.080 --> 00:05:47.060
Anyway, Celery, yeah.

00:05:47.940 --> 00:05:48.100
Wow.

00:05:48.700 --> 00:05:50.260
We've got free-threaded Python now.

00:05:50.540 --> 00:05:50.860
We do.

00:05:51.320 --> 00:05:52.020
The future is now.

00:05:52.780 --> 00:05:53.120
Yes.

00:05:53.560 --> 00:05:53.760
Almost.

00:05:54.100 --> 00:05:55.180
We almost have free-threaded Python.

00:05:55.520 --> 00:05:55.580
Yeah.

00:05:55.720 --> 00:05:55.920
Yeah.

00:05:56.440 --> 00:05:56.980
Spoiler alert.

00:05:57.240 --> 00:05:58.940
That may make an appearance in one of the topics.

00:06:00.360 --> 00:06:07.320
Well, we may not get to, yeah, we'll have 20 things, but they may not be 20 big, bold items, right?

00:06:08.200 --> 00:06:08.260
Yeah.

00:06:08.360 --> 00:06:10.320
We have a list of things we want to go through.

00:06:10.580 --> 00:06:11.020
That's right.

00:06:11.110 --> 00:06:16.020
Peter, we reserve the right to design the designation of the size of the buckets that define the things.

00:06:16.260 --> 00:06:17.100
The things, that's right.

00:06:18.600 --> 00:06:29.780
But I think the plan is we're going to just riff on some ideas we think are either emerging or current important trends or even foundational things that people should be paying attention to.

00:06:31.900 --> 00:06:34.480
in the zeitgeist right now, right?

00:06:34.630 --> 00:06:36.640
What are things that maybe you haven't necessarily

00:06:36.750 --> 00:06:37.820
been tracking or you've heard of,

00:06:37.920 --> 00:06:39.660
but you're like, ah, I haven't got time for that

00:06:39.800 --> 00:06:40.920
or it's not for me yet.

00:06:42.480 --> 00:06:43.760
So I think that'll be fun.

00:06:45.240 --> 00:06:46.160
Let's start with you, Peter.

00:06:46.840 --> 00:06:50.280
What's your first, we all gathered up a couple of things

00:06:50.420 --> 00:06:52.360
that we think might be super relevant.

00:06:52.760 --> 00:06:53.780
And yeah, what do you think?

00:06:55.760 --> 00:06:57.500
So I think, well, let's just get started with it.

00:06:57.540 --> 00:06:59.020
Let's just talk about the free threading bit.

00:06:59.660 --> 00:07:01.840
And let's really, because this is kind of,

00:07:01.860 --> 00:07:04.960
it touches the past and it also really takes us

00:07:04.980 --> 00:07:06.860
into the future and it's this thing that has taken

00:07:07.520 --> 00:07:08.580
quite some time to emerge.

00:07:09.220 --> 00:07:11.320
I think the GIL has been a topic of discussion

00:07:11.560 --> 00:07:13.020
since as long as I've been using Python.

00:07:14.700 --> 00:07:18.700
And finally we have courtesy of the team at Meta,

00:07:19.080 --> 00:07:21.900
an excellent set of patches that delivered

00:07:22.720 --> 00:07:24.100
true free threading to Python.

00:07:24.800 --> 00:07:26.900
And of course this is both a blessing and a curse, right?

00:07:26.960 --> 00:07:28.459
You should be careful what you ask for

00:07:28.480 --> 00:07:30.940
is now we end up having to deal with true free threading in Python.

00:07:32.080 --> 00:07:35.680
And for those who maybe are not so familiar with this whole topic,

00:07:36.600 --> 00:07:40.920
the global interpreter lock, we call it GIL, G-I-L for short,

00:07:41.900 --> 00:07:46.440
the global interpreter lock is how the Python virtual machine protects its innards.

00:07:46.620 --> 00:07:48.760
And so when you use Python and you write code,

00:07:49.080 --> 00:07:51.240
even if you use threading, like the threading module in Python,

00:07:52.059 --> 00:07:56.760
ultimately the CPython interpreter itself as a system-level process,

00:07:57.260 --> 00:08:03.120
it only has one real thread and it has this global interpreter lock that locks many of the internals of the interpreter.

00:08:04.320 --> 00:08:10.100
The problem is that sometimes you want to have real multi-threading and so you have to release this global interpreter lock.

00:08:10.460 --> 00:08:17.480
And doing this is hard to get right, especially if you reach into C modules and whatnot.

00:08:18.300 --> 00:08:20.920
The most popular C modules are pretty good at handling this kind of thing.

00:08:21.580 --> 00:08:22.980
NumPy and others come to mind.

00:08:23.280 --> 00:08:26.160
So we get really great performance from those when they release the GIL.

00:08:26.560 --> 00:08:30.980
But if you want to actually do a lot of Python logic in multiple threads,

00:08:31.310 --> 00:08:34.620
you end up essentially getting no lift whatsoever by using a threading module

00:08:34.990 --> 00:08:38.520
with classic single-threaded or gill-locked Python.

00:08:39.260 --> 00:08:44.360
With the free threading, you actually now are able to have threads running in parallel,

00:08:45.480 --> 00:08:47.920
touching things like free lists and stuff like that

00:08:48.200 --> 00:08:51.340
and module definitions in the interpreter itself.

00:08:52.100 --> 00:09:01.920
Now, what this means is a lot of Python modules or packages, which had been developed when Python was implicitly single threaded,

00:09:02.560 --> 00:09:10.060
they now have the potential of thread contention, race conditions, all sorts of weird and unexpected behavior when they're used in a free threaded way.

00:09:10.700 --> 00:09:15.040
So we have this patch, we have this change now for free threading in the Python interpreter itself.

00:09:15.980 --> 00:09:24.400
However, what that means is we have to make sure that all of the rest of the package ecosystem is updated and tested to work with free-threaded Python.

00:09:24.950 --> 00:09:30.020
So in Python 3.13, it was incorporated as an experimental.

00:09:30.190 --> 00:09:33.220
It was in the code base, but it was a build time feature.

00:09:33.390 --> 00:09:39.260
So you have to compile your own Python interpreter and turn on that flag to get a version of the interpreter that would be free-threaded.

00:09:39.980 --> 00:09:43.560
In 3.14, it is now supported in the interpreter.

00:09:44.280 --> 00:09:45.940
It's still not turned on by default.

00:09:46.980 --> 00:09:50.300
And then at some indeterminate date, it will be turned on by default.

00:09:52.179 --> 00:09:59.040
The classic behavior with the global turbo block will still always be there as a fallback for safety and compatibility and all that.

00:09:59.520 --> 00:10:08.540
But anyway, we're right now at the point where the core CPython team has said, hey, we're ready to take this thing to supported mode and let the bugs flow.

00:10:08.640 --> 00:10:14.660
right so like um so now if you go and install python a python build with um it actually has

00:10:14.660 --> 00:10:22.180
a different abi tag so see it's uh cp313 or 314 t for you know um for threading or free threading

00:10:23.060 --> 00:10:29.440
um so that's available through you know python.org there's a condo build for it as well and um so

00:10:29.600 --> 00:10:34.059
right now there's actually a page maybe we'll have the link for it i think in the in the show notes

00:10:34.120 --> 00:10:40.360
right but there's a page that lists what the status is think of the free threaded wheels

00:10:42.120 --> 00:10:49.220
and right now 105 out of 360 that are passing basically having the maintainers have updated them

00:10:49.820 --> 00:10:55.620
and this is out of the top like oh there that's great yeah out of the top 500 python packages

00:10:55.740 --> 00:11:00.279
something like this so you can see we have as a community a lot of work to do so the call to

00:11:00.080 --> 00:11:04.280
to action here is not only should a Python developer learn this because this is definitely

00:11:04.480 --> 00:11:09.340
coming and everyone has a multi-core machine now. So this is definitely coming, but you

00:11:09.340 --> 00:11:13.160
can also, this is a great way to give back. You know, we talk about in the open source

00:11:13.320 --> 00:11:16.620
community oftentimes, how do we get starter bugs in there for people to start becoming

00:11:16.900 --> 00:11:19.800
contributing members of the community? This is a great way to give back. If there's some

00:11:19.900 --> 00:11:24.160
packages you see here that are yellow, you're like, wait, I use AIoHTTP. Like, let me go

00:11:24.320 --> 00:11:28.699
and test that with free threading and see if I can bang, you know, just beat it up with

00:11:28.700 --> 00:11:33.440
my code in production and see like what fails there. So these are a great way for the community

00:11:33.450 --> 00:11:38.040
to really get back and help us test and make sure all this works on what is certainly to be the next

00:11:38.200 --> 00:11:43.440
generation of the Python interpreter. Yeah, there was a great talk at DjangoCon just two weeks ago

00:11:43.920 --> 00:11:50.620
by Michael Lyle. He gave a talk about using free threading in Django. And I think right now your

00:11:50.790 --> 00:11:57.519
mileage may vary was the answer. Like it kind of depends. I can only imagine going through and

00:11:57.500 --> 00:11:59.120
trying to commit and help.

00:11:59.940 --> 00:12:00.760
Threading is hard.

00:12:00.970 --> 00:12:02.180
It sounds like free threading is harder

00:12:02.900 --> 00:12:04.180
to wrap your brain around.

00:12:04.500 --> 00:12:06.740
So I think it'd be tricky for someone starting

00:12:06.810 --> 00:12:07.660
and learning something new.

00:12:07.760 --> 00:12:10.380
This may be on the more advanced edge

00:12:11.760 --> 00:12:13.160
of what someone should be learning.

00:12:13.840 --> 00:12:15.720
It's more for the advanced, crotchety,

00:12:16.050 --> 00:12:17.520
you know, like, you know, senior developers,

00:12:17.700 --> 00:12:18.920
I ain't got time to contribute to open source.

00:12:19.410 --> 00:12:21.200
You can, you can make your own life better.

00:12:21.750 --> 00:12:23.540
We can all sort of, this is the sort of stone soup

00:12:24.020 --> 00:12:25.140
or good old Amish barn raising.

00:12:25.190 --> 00:12:26.840
We should all get together and chip in.

00:12:27.360 --> 00:12:32.540
But you're right, debugging async free threading issues is definitely not a beginner kind of task.

00:12:33.480 --> 00:12:40.820
Sure, but there's a lot of people who do have that experience from probably more from other languages or C extensions who could jump in, right?

00:12:42.020 --> 00:12:48.660
Yeah, actually, you know, if you're a C++ developer who has been forced to use Python because of our success of driving the growth and adoption of the community,

00:12:49.200 --> 00:12:52.020
and you're really angry about this and you want to show other ways that Python is broken,

00:12:52.400 --> 00:12:54.100
This is a great way to show how Python is broken.

00:12:54.660 --> 00:12:58.640
It's to test really gnarly async and multi-threaded use cases.

00:12:59.680 --> 00:13:02.660
Actually, one thing about this that I will point out for the more advanced users,

00:13:03.820 --> 00:13:09.160
Dave Beasley gave a great talk years ago at PyCon about Python parallelism

00:13:09.800 --> 00:13:11.880
and are you IO bound, are you CPU bound?

00:13:12.540 --> 00:13:17.060
I think he was looking at, maybe it was actually relative to PyPy, P-Y-P-Y,

00:13:17.500 --> 00:13:20.060
and it wasn't about async in particular,

00:13:21.000 --> 00:13:22.900
but it was a rolling of distributed computing

00:13:22.940 --> 00:13:23.540
or something like this.

00:13:23.560 --> 00:13:24.460
I forget the exact title,

00:13:25.020 --> 00:13:27.720
but he did a deep analysis of when are we CPU bound

00:13:27.760 --> 00:13:30.420
or when are we IO bound and when are we CPU bound.

00:13:30.740 --> 00:13:32.580
When we get to free threading Python like this,

00:13:32.600 --> 00:13:33.980
I think we're going to, as a community,

00:13:34.060 --> 00:13:35.920
be faced with having to up-level our thinking

00:13:36.080 --> 00:13:36.800
about this kind of thing.

00:13:36.900 --> 00:13:38.400
Because so far we've done a lot of like,

00:13:38.440 --> 00:13:40.560
oh, delegating CPU bound numeric stuff

00:13:40.700 --> 00:13:42.100
to like Python or Pandas or Cython.

00:13:43.020 --> 00:13:45.060
But with this, now we can really play first class

00:13:45.380 --> 00:13:46.260
in system level code.

00:13:46.600 --> 00:13:47.980
And we have to think more deeply about

00:13:48.320 --> 00:13:49.220
how are we blocking events?

00:13:49.340 --> 00:13:50.060
How are we handling things?

00:13:50.260 --> 00:13:54.200
Is this, you know, is this a, you know, event polling kind of thing?

00:13:54.290 --> 00:13:56.480
Or is this more of a completion port thing?

00:13:56.570 --> 00:13:57.560
Like in Windows, you have different options.

00:13:58.050 --> 00:13:59.560
So this is a very interesting topic, actually.

00:13:59.610 --> 00:14:00.300
It goes quite deep.

00:14:00.820 --> 00:14:01.780
It goes very deep.

00:14:01.960 --> 00:14:09.220
And I think it's going to be a big mental lift for people in the community, generally speaking.

00:14:10.380 --> 00:14:16.080
I talk to a lot of people, as you know, from the podcast, and then also interact with a lot of people teaching.

00:14:16.360 --> 00:14:20.060
And I don't see a lot of people stressing about thread safety.

00:14:20.980 --> 00:14:22.860
or any of those kinds of things these days.

00:14:23.490 --> 00:14:25.180
And I think in general,

00:14:25.320 --> 00:14:28.580
it's just not in the collective thinking

00:14:28.790 --> 00:14:29.940
to be really worried about it.

00:14:30.340 --> 00:14:33.200
There are still cases in multi-threaded Python code

00:14:33.540 --> 00:14:34.640
where you need to take a lock

00:14:34.920 --> 00:14:37.600
because it's not one line's going to deadlock another

00:14:37.790 --> 00:14:38.600
or something like that,

00:14:38.780 --> 00:14:40.320
but you've got to take five steps.

00:14:40.610 --> 00:14:43.460
And if you get interrupted somewhere in those five steps,

00:14:44.720 --> 00:14:46.540
the guilt could still theoretically interrupt you

00:14:46.550 --> 00:14:47.540
in the middle of code, right?

00:14:48.240 --> 00:14:52.460
It still could be in a temporarily invalid state across more than one line.

00:14:53.150 --> 00:14:56.920
But I just don't see people even doing anything hardly at it at all.

00:14:57.360 --> 00:15:01.680
And when we just uncork this on them, it's going to be something.

00:15:01.810 --> 00:15:04.540
And I don't think we're going to see deadlocks as a problem first.

00:15:05.140 --> 00:15:06.360
I think we're going to see race conditions.

00:15:07.100 --> 00:15:10.320
Because deadlocks require people already having locks there that get out of order.

00:15:10.450 --> 00:15:11.760
And I just think the locks are not there.

00:15:12.280 --> 00:15:13.580
Then people are going to put the locks there.

00:15:13.580 --> 00:15:14.880
And they're like, whoa, it just stopped.

00:15:16.020 --> 00:15:16.840
It's total chaos.

00:15:17.500 --> 00:15:19.740
It's not using CPU anymore.

00:15:19.820 --> 00:15:20.260
What is it doing?

00:15:20.280 --> 00:15:21.480
Well, now you found the deadlock.

00:15:21.980 --> 00:15:23.420
You added the deadlock, right?

00:15:23.800 --> 00:15:23.900
Yeah.

00:15:24.080 --> 00:15:27.060
So it's going to be a challenge.

00:15:27.460 --> 00:15:29.900
But the potential on the other side of this,

00:15:30.000 --> 00:15:33.220
if you can get good at it, it's going to be amazing.

00:15:33.740 --> 00:15:37.120
Even on my little cheapo Mac mini, I've got 10 cores.

00:15:37.560 --> 00:15:40.860
If I run Python code, unless I do really fancy tricks

00:15:41.600 --> 00:15:44.199
or multiple processes, the best I can get is like 10%.

00:15:45.380 --> 00:15:56.920
Yeah, and I know this might be a little bit of a spicy take, but like there was, I think, a line that was being held by the CPython core team that we will accept a GIL removal or a gillectomy as it was called.

00:15:57.380 --> 00:16:03.860
We'll accept a GIL removal patch when it doesn't affect or negatively impact single core performance, right?

00:16:04.540 --> 00:16:11.340
And like when that first came out in 2000, I think that first time I heard that article was a 2005, 6, 7 timeframe.

00:16:11.910 --> 00:16:13.580
Back then, that was almost a defensible position.

00:16:14.140 --> 00:16:16.920
Nowadays, you can't find a smartphone with a single core.

00:16:17.280 --> 00:16:18.540
You know, I can't find a Raspberry Pi.

00:16:18.720 --> 00:16:20.860
A $5 Raspberry Pi has dual cores.

00:16:21.100 --> 00:16:25.440
So it's like I get the general gist of that, but, like, come on.

00:16:25.500 --> 00:16:30.160
We have, like, 90 – like, you know, John Carmack's on Twitter talking about 96-core Threadripper performance with Python.

00:16:31.220 --> 00:16:33.160
You know, we sort of need to lean into that, right?

00:16:33.240 --> 00:16:35.480
So I'm really, really bullish on this.

00:16:35.580 --> 00:16:39.540
Because, as you know, like, I'm very close to the data science and machine learning and the AI use cases.

00:16:40.160 --> 00:16:41.700
And those are all just, you know,

00:16:41.840 --> 00:16:43.000
they're looking for whatever language

00:16:43.110 --> 00:16:43.900
gives us the best performance.

00:16:44.560 --> 00:16:45.500
Right now, it happens to Python.

00:16:46.020 --> 00:16:46.840
If we as a community,

00:16:47.350 --> 00:16:48.780
and we as evangelists of that community,

00:16:49.200 --> 00:16:51.280
if we don't lead into that and those users,

00:16:51.520 --> 00:16:52.880
they will happily go somewhere else.

00:16:53.410 --> 00:16:55.660
I mean, that is bonusing people $100 million to start.

00:16:55.960 --> 00:16:57.680
They're not going to wait for your language to catch up.

00:16:57.820 --> 00:16:59.020
They'll make a new language, right?

00:16:59.380 --> 00:17:00.840
But I think if there was something in 2025

00:17:01.460 --> 00:17:03.900
that these developers should be learning along these lines

00:17:03.930 --> 00:17:05.300
would be just async programming

00:17:05.480 --> 00:17:06.740
and when it should be used.

00:17:07.300 --> 00:17:11.439
That's probably the really tactical maneuver today.

00:17:14.079 --> 00:17:14.880
Yeah, I agree.

00:17:15.189 --> 00:17:19.939
I think the ASIC and await keywords are super relevant.

00:17:20.220 --> 00:17:24.060
And the frameworks, I think, will start to take advantage of it.

00:17:24.079 --> 00:17:25.939
We're going to see what comes along with this free threading.

00:17:26.430 --> 00:17:29.180
But there's no reason you couldn't await a thread

00:17:30.290 --> 00:17:31.940
rather than await an I-O operation.

00:17:32.130 --> 00:17:32.580
You know what I mean?

00:17:33.940 --> 00:17:36.900
My background is C++ and C#.

00:17:37.500 --> 00:17:39.700
and C# is actually where async and await came from,

00:17:39.860 --> 00:17:40.940
from Anders Halsberg, I believe.

00:17:42.740 --> 00:17:46.160
And over there, you don't care if it's IO or compute bound.

00:17:46.250 --> 00:17:48.660
You just await some kind of async thing.

00:17:48.760 --> 00:17:50.120
It's not your job to care how it happens.

00:17:51.760 --> 00:17:53.520
So I think we're going to start to see that.

00:17:53.550 --> 00:17:56.840
But it's going to take time for those foundational layers

00:17:57.040 --> 00:17:58.200
to build for us to build on.

00:17:58.960 --> 00:17:59.300
Yeah.

00:18:00.740 --> 00:18:02.300
Yeah, Pamela Fox out in the audience

00:18:02.690 --> 00:18:05.759
throws out that last time she really used locks

00:18:05.780 --> 00:18:08.020
was in my code for operating system class in college.

00:18:08.220 --> 00:18:09.420
It doesn't come up much in web dev.

00:18:09.600 --> 00:18:09.900
That's true.

00:18:10.220 --> 00:18:13.500
A lot of the times the web, it's at the web framework,

00:18:13.840 --> 00:18:17.100
the web server level, the app server level, right?

00:18:17.180 --> 00:18:19.940
It's Granian or it's UVicorn or something like that.

00:18:20.000 --> 00:18:22.180
That thing does the threading and you just handle one

00:18:22.200 --> 00:18:22.760
of the requests.

00:18:23.320 --> 00:18:27.780
I literally just deadlocked and I guess probably broke the website

00:18:28.020 --> 00:18:29.620
for a couple people at Talk Python today

00:18:29.880 --> 00:18:32.140
because I have this analytics operation that's

00:18:32.380 --> 00:18:33.160
fixing up a bunch of stuff.

00:18:33.380 --> 00:18:34.920
And it ran for like 60 seconds.

00:18:35.860 --> 00:18:38.440
Even though there's multiple workers, something about the fan out,

00:18:38.620 --> 00:18:41.940
it still sent some of the traffic to the one that was bound up.

00:18:41.980 --> 00:18:43.840
And then those things were timing out after 20 seconds.

00:18:44.060 --> 00:18:45.080
I'm like, oh, no, what have I done?

00:18:46.840 --> 00:18:49.580
And if that was true threading, it wouldn't have mattered.

00:18:50.260 --> 00:18:52.280
It would have used up one of my eight cores,

00:18:52.380 --> 00:18:53.860
and the rest would have been off jamming along.

00:18:54.020 --> 00:18:54.920
It would have been fine, you know?

00:18:55.780 --> 00:18:56.820
Well, sort of, right?

00:18:56.980 --> 00:18:59.460
And I think this is – I'm really glad Pamela brought this up

00:18:59.480 --> 00:19:03.720
because when we're focused on a particular just the worker thread,

00:19:04.520 --> 00:19:06.520
it's like, okay, what am I doing?

00:19:07.220 --> 00:19:09.760
Pull this, run that, and then push this thing out.

00:19:10.840 --> 00:19:12.480
But if you start getting to more,

00:19:13.060 --> 00:19:15.960
anytime you start having either value-dependent

00:19:16.140 --> 00:19:19.340
or heterogeneous sort of workload

00:19:20.160 --> 00:19:23.340
and time boundaries for these tasks,

00:19:23.840 --> 00:19:25.520
you start having to think about thread contention.

00:19:25.640 --> 00:19:39.480
You start, you know, it's, I mean, to your point, Calvin, I think it's not so far that you have to go before you quickly find yourself thinking about things like Grand Central Dispatch, like macOS has, or IO completion ports.

00:19:39.560 --> 00:19:41.980
And like, oh, crap, I'm actually slamming.

00:19:42.120 --> 00:19:50.580
It's not under certain cases, you know, to your point about the analytics, maybe you're not doing a GPU-based analytics thing, but maybe you're slamming a bunch of stuff to disk or loading a bunch of stuff up from disk.

00:19:51.140 --> 00:19:55.180
And you start getting all these things where at some point, one of these things is the bottleneck.

00:19:55.270 --> 00:20:00.860
Is it the CPU? Is it the code itself? Is it the disk? Is it the network?

00:20:01.860 --> 00:20:04.920
And you're just slamming your code into one of these different boundaries stochastically.

00:20:05.540 --> 00:20:08.920
And as a developer, maybe as an entry-level developer, you don't have to think about it too much.

00:20:09.270 --> 00:20:13.780
But as any kind of a mid to senior developer, you're going to be running into these problems.

00:20:13.970 --> 00:20:16.360
And they are going to be stochastic. They are value-dependent.

00:20:16.420 --> 00:20:20.980
You're gonna hit them in production and you have to sort of know what,

00:20:21.290 --> 00:20:23.960
what could bite you, even if it's not biting you all the time in dev.

00:20:25.140 --> 00:20:25.800
Right. You,

00:20:26.080 --> 00:20:29.100
you remove one bottleneck and it starts to slam into a different part.

00:20:29.200 --> 00:20:32.580
Maybe you take them to the debate database and it's even a worse consequence.

00:20:32.630 --> 00:20:34.840
You never know, right? We're going to see, it's going to be interesting.

00:20:35.200 --> 00:20:36.100
But thinking about that in production,

00:20:36.260 --> 00:20:40.020
you've got new new challenges there because you may have containers and you're

00:20:40.120 --> 00:20:43.660
running in Kubernetes and you've got pods and resource limits and,

00:20:44.200 --> 00:20:45.900
and other kinds of constraints that are happening

00:20:46.120 --> 00:20:47.220
that aren't on your local machine.

00:20:47.360 --> 00:20:48.760
All of a sudden you're saturating your local machine.

00:20:48.820 --> 00:20:49.500
You're like, this is great.

00:20:49.810 --> 00:20:50.900
I'm using all the resources.

00:20:51.180 --> 00:20:51.600
Look at it go.

00:20:51.900 --> 00:20:53.080
And now you release that to production

00:20:53.340 --> 00:20:55.600
and watch Calamity and Chaos.

00:20:56.300 --> 00:20:58.340
They get killed off because you've set something.

00:20:59.520 --> 00:21:01.760
My websites and APIs and databases

00:21:02.180 --> 00:21:06.780
all have production level RAM limits

00:21:06.830 --> 00:21:07.940
and things like that

00:21:08.060 --> 00:21:09.500
so that if they go completely crazy,

00:21:09.620 --> 00:21:11.760
at least it's restricted to that one thing dying.

00:21:12.200 --> 00:21:12.820
Not everything.

00:21:13.160 --> 00:21:19.520
Yeah. Speaking of which, maybe you've got some ideas on what's next, Calvin.

00:21:20.140 --> 00:21:26.180
Sure. I mean, I've been a big believer in containers. I really got turned on to this in

00:21:26.320 --> 00:21:32.260
2020 and went down the path and now we're finally arrived where I believe developers should be

00:21:32.480 --> 00:21:39.739
learning Kubernetes, even for local development. I feel like that whole front to back story is not

00:21:39.780 --> 00:21:46.040
as complicated. The tooling has really come up to date. And so being able to use containers to get

00:21:46.440 --> 00:21:53.280
reliable, repeatable builds, being able to use tools like tilt.dev, for example, as a developer

00:21:53.540 --> 00:21:58.980
locally with my Kubernetes clusters, I can now have file systems syncing, use all my local tools.

00:22:00.040 --> 00:22:05.619
This just literally does take the pains out of, they say microservice development. I think that's

00:22:05.640 --> 00:22:07.960
a little bit of a buzzwordy explanation there.

00:22:08.160 --> 00:22:10.560
I will say that it's good for Django development.

00:22:11.120 --> 00:22:13.680
So if you check out the SCAF full stack template,

00:22:14.420 --> 00:22:14.820
are you going to go-

00:22:14.820 --> 00:22:16.180
Hold on, I'm fixing it for you here.

00:22:16.420 --> 00:22:17.040
Perfect.

00:22:17.040 --> 00:22:19.600
That's perfect.

00:22:19.600 --> 00:22:24.020
This is exactly where we can use the same tools in production

00:22:24.020 --> 00:22:25.600
that we use in development

00:22:25.600 --> 00:22:28.540
so that it's much easier to track down issues.

00:22:28.540 --> 00:22:30.480
Containers obviously unlocked a lot of those.

00:22:30.480 --> 00:22:34.120
I feel like the true superpower of Kubernetes,

00:22:34.800 --> 00:22:36.580
I think a lot of people love it for orchestration

00:22:36.660 --> 00:22:37.720
or claim it's for orchestration.

00:22:37.940 --> 00:22:39.860
I really love the fact that it's got a control plane

00:22:39.910 --> 00:22:41.780
and a URL and an API.

00:22:42.510 --> 00:22:45.160
So you can do things like continuous deployment.

00:22:45.780 --> 00:22:47.680
So being able to deliver your code,

00:22:47.960 --> 00:22:49.260
build an image, update a manifest

00:22:49.560 --> 00:22:51.180
and have things just deploy

00:22:51.950 --> 00:22:53.900
without you having to think twice about it

00:22:54.060 --> 00:22:56.020
and be able to roll back with a click of a button

00:22:56.150 --> 00:22:57.420
and using tools like Argo CD.

00:22:58.220 --> 00:23:00.560
Argo CD is a great CI CD tool.

00:23:00.710 --> 00:23:01.900
So we leverage it very heavily.

00:23:02.020 --> 00:23:03.900
If you want a good example of how to do that,

00:23:04.320 --> 00:23:06.480
you can check out that same full stack template.

00:23:06.700 --> 00:23:10.280
We have all the pieces put in there for you in GitHub

00:23:11.300 --> 00:23:12.600
to understand how that works.

00:23:12.800 --> 00:23:15.480
So I think it's real.

00:23:15.580 --> 00:23:19.220
I think developers should be embracing the container world,

00:23:19.960 --> 00:23:21.780
especially if you have more than one developer.

00:23:21.940 --> 00:23:23.000
As soon as you have a second developer,

00:23:24.040 --> 00:23:25.900
this becomes an immediate payoff

00:23:26.630 --> 00:23:29.280
in the work it took to put it in place.

00:23:30.060 --> 00:23:33.440
And so I think it hits all the environments too,

00:23:33.640 --> 00:23:36.320
like not just web dev, I think the data folks benefit

00:23:36.640 --> 00:23:39.180
from containers, especially if you look at tools like Airflow,

00:23:39.760 --> 00:23:41.160
be able to deploy that into containers,

00:23:41.880 --> 00:23:45.080
be able to manage workers that are, you know,

00:23:45.899 --> 00:23:47.240
Kubernetes based tasks.

00:23:47.860 --> 00:23:50.340
So you can like natively handle asynchronous tasks

00:23:50.600 --> 00:23:52.720
in a cluster and leverage all that power

00:23:53.300 --> 00:23:54.980
you've got under the covers and scalability

00:23:55.100 --> 00:23:56.460
and be able to scale out all the nodes.

00:23:57.059 --> 00:24:01.040
You get a lot of win for adopting a tool that

00:24:01.320 --> 00:24:03.600
I think a lot of people and me included used to consider

00:24:03.620 --> 00:24:03.860
overkill.

00:24:04.640 --> 00:24:05.380
MARK MANDEL: Yeah.

00:24:06.180 --> 00:24:08.380
Well, let's put some layers on this.

00:24:08.900 --> 00:24:09.880
First of all, preach on.

00:24:10.460 --> 00:24:10.980
Preach on.

00:24:12.240 --> 00:24:16.000
But you say containers, and you said Kubernetes,

00:24:16.640 --> 00:24:17.680
and then some other things.

00:24:19.440 --> 00:24:22.160
Do you have to know Docker and containers?

00:24:22.640 --> 00:24:24.660
Is Docker synonymous with containers for you?

00:24:24.720 --> 00:24:26.100
Do you have to know that before you're

00:24:26.340 --> 00:24:26.960
successful with Kubernetes?

00:24:27.260 --> 00:24:32.260
Like, what are-- there's a couple of layers of architecture

00:24:32.280 --> 00:24:35.620
here, where are you telling people they should pay attention to?

00:24:36.380 --> 00:24:38.860
I think you have to start with containers. Start with Docker.

00:24:40.220 --> 00:24:42.460
The dog wants me to play with the toy over here.

00:24:43.230 --> 00:24:47.140
If you start with the container, because you have to have a good container strategy,

00:24:48.060 --> 00:24:52.640
even to be able to build and work with containers inside of any kind of a,

00:24:53.030 --> 00:24:57.980
you know, whether it's Docker Compose or Swarm or, you know, using Fargate or some kind of

00:24:57.880 --> 00:24:59.820
container app service, like on DigitalOcean.

00:25:00.940 --> 00:25:02.840
- Yeah, count me down as Docker Compose, by the way.

00:25:02.880 --> 00:25:03.640
I'm a big fan of this.

00:25:03.760 --> 00:25:05.760
- Yeah, that's where we started.

00:25:06.000 --> 00:25:09.300
I really enjoyed the ability to have Compose describe

00:25:09.560 --> 00:25:13.520
my whole stack and be able to run the exact same version

00:25:13.640 --> 00:25:15.200
of the exact right version of Redis,

00:25:15.340 --> 00:25:16.580
the exact right version of Postgres,

00:25:16.620 --> 00:25:18.940
the exact right version of whatever my dependent

00:25:19.260 --> 00:25:22.020
other pieces are around me, because that matters.

00:25:23.440 --> 00:25:29.960
I don't know if folks remember the Redis 608 to 609,

00:25:30.230 --> 00:25:33.160
like a very minor release introduced a new feature

00:25:33.230 --> 00:25:36.620
that was unusable and a very minor release backward.

00:25:37.220 --> 00:25:39.060
So you want to be able to pin these things down

00:25:39.090 --> 00:25:42.920
so you aren't chasing ghosts and weird edge cases

00:25:43.620 --> 00:25:45.020
and containers enable that.

00:25:45.050 --> 00:25:47.140
And whether it's Compose or Kubernetes, it doesn't matter.

00:25:47.570 --> 00:25:49.020
You get that benefit.

00:25:49.600 --> 00:25:53.400
I feel like the Kubernetes piece just takes that to the next level

00:25:53.420 --> 00:25:58.820
a lot of the programmability of the web with an API and the fact that I'm not logging in.

00:25:59.800 --> 00:26:05.180
Our preferred way to deploy Kubernetes onto servers is actually to use Talos Linux, which has no SSH

00:26:05.320 --> 00:26:11.240
shell. There is not a way to shell into that box. It eliminates a whole class of security vulnerabilities

00:26:12.000 --> 00:26:17.320
because there is no shell on the box whatsoever. You have to interact with it programmatically via

00:26:17.340 --> 00:26:24.880
APIs and even the upgrades happen via the same, you know, API back planes. And just that level of

00:26:25.100 --> 00:26:33.740
control, security, reliability, and comfort helped me sleep really well at night knowing where I've

00:26:33.820 --> 00:26:38.300
deployed these things. Yeah. But you do need containers first. I think if you don't understand

00:26:38.390 --> 00:26:42.680
the layers of the containers, but I think that's a, that's a quick read. There's some really good

00:26:42.700 --> 00:26:48.240
resources online uh nana's tech world does a really good job of describing the changes

00:26:49.640 --> 00:26:55.540
and she does an awesome job of like bringing that down to an every person most every person level

00:26:55.840 --> 00:27:01.220
uh who would even care to touch it i i've i've some thoughts about containers and compose and

00:27:01.340 --> 00:27:07.500
stuff that i want to throw on but i do especially want to hear peter contrast your take with the

00:27:08.620 --> 00:27:10.960
you sort of say the same thing, but for data scientists,

00:27:11.380 --> 00:27:15.000
do you need to pay attention to containers in data science?

00:27:15.280 --> 00:27:15.800
Is that different?

00:27:16.140 --> 00:27:18.720
I interviewed Matthew Rockland from Coiled recently,

00:27:19.000 --> 00:27:23.140
and they've got a really interesting way to ship and produce your code

00:27:23.360 --> 00:27:25.740
without containers that you actually interact with.

00:27:26.080 --> 00:27:27.120
There's options, but.

00:27:27.580 --> 00:27:29.160
Yeah, I think, I mean,

00:27:29.860 --> 00:27:32.960
I think containers are just part of the technical landscape now,

00:27:33.100 --> 00:27:34.500
so it's good to know them.

00:27:35.060 --> 00:27:41.080
I think if we were to remove the capabilities of data science from everyone who doesn't know about containers,

00:27:41.230 --> 00:27:44.260
that we would end up with a deeply impoverished user base, right?

00:27:45.060 --> 00:27:47.360
The truth of the matter is that there are a lot of people out there today who,

00:27:48.480 --> 00:27:53.100
if you think about what containers really do from a software development and a DevOps perspective,

00:27:53.840 --> 00:27:58.800
it is a mechanism for your dog knows about to say something spicy.

00:27:59.020 --> 00:28:00.080
No, I'm not trying to be controversial.

00:28:00.260 --> 00:28:05.020
Just thinking about it on first principles, a container is a way for us to sort of format

00:28:05.040 --> 00:28:08.300
and rationalize the target deployment environment

00:28:08.640 --> 00:28:10.700
at within the boundaries of the box,

00:28:10.920 --> 00:28:12.840
within the boundaries of a particular compute node

00:28:13.340 --> 00:28:14.800
with an IP address or something like this.

00:28:16.040 --> 00:28:17.920
And then Kubernetes takes the next level up,

00:28:17.920 --> 00:28:19.760
which is, oh, if your dependencies for your application

00:28:20.380 --> 00:28:23.220
is if you have like a microservices classic sort of example,

00:28:23.720 --> 00:28:25.880
if your application is architected in such a way

00:28:25.960 --> 00:28:27.720
that you need a lot of services to be running,

00:28:28.300 --> 00:28:29.560
well, to format that,

00:28:29.700 --> 00:28:32.400
you need to actually create a cluster of services

00:28:32.420 --> 00:28:35.320
configured with particular network configuration

00:28:35.820 --> 00:28:36.980
and various kinds of things.

00:28:37.340 --> 00:28:38.640
So you're actually shipping a cluster

00:28:39.200 --> 00:28:41.360
as the first thing you land and then you land,

00:28:41.680 --> 00:28:43.200
you deploy the airport, then you land the plane.

00:28:44.160 --> 00:28:45.160
So if you need to do that,

00:28:45.580 --> 00:28:47.480
if the thing you're doing is so big,

00:28:47.760 --> 00:28:51.020
and I think that we think about the US Air Force and Army,

00:28:51.540 --> 00:28:52.920
like the reason why the American military

00:28:54.180 --> 00:28:55.480
sort of has the dominance it has

00:28:56.080 --> 00:28:57.360
is because of the logistics chain.

00:28:57.400 --> 00:29:00.740
They can land just hundreds and hundreds of tons

00:29:00.760 --> 00:29:03.400
of military hardware and food and personnel

00:29:04.090 --> 00:29:06.100
into any location on the earth inside of 24 hours.

00:29:07.120 --> 00:29:08.540
And this is sort of what Kubernetes gives you

00:29:08.620 --> 00:29:10.100
is that ability to format at that level.

00:29:10.780 --> 00:29:12.300
But at the end of the day, if you have a Jupyter Notebook,

00:29:12.700 --> 00:29:15.880
well-known data set, you know how many CPU cores,

00:29:16.300 --> 00:29:18.280
what kind of GPU you need to run a particular analytic,

00:29:19.380 --> 00:29:21.380
that can seem like overkill

00:29:21.600 --> 00:29:23.280
because you could say, spin up the CC2 box,

00:29:24.220 --> 00:29:26.440
get me in there, spin up JupyterHub,

00:29:26.940 --> 00:29:28.900
copy the thing over and now it's running, yay.

00:29:29.880 --> 00:29:32.060
So I don't think that containers are necessary,

00:29:32.700 --> 00:29:34.980
but in life we don't just do what's necessary, right?

00:29:35.120 --> 00:29:37.020
I think it is useful to know something about

00:29:37.600 --> 00:29:42.200
how to ship and work with modern IT environments

00:29:42.500 --> 00:29:43.780
and cloud native kinds of environments.

00:29:43.880 --> 00:29:44.820
So it's a useful thing to know.

00:29:46.200 --> 00:29:47.360
But then again, like I said,

00:29:47.480 --> 00:29:49.440
it's the goal for us as technologists

00:29:49.880 --> 00:29:51.020
should be empowering those

00:29:51.300 --> 00:29:54.040
who are less technically inclined than us.

00:29:54.420 --> 00:29:56.140
And so removing the complexity for them

00:29:56.480 --> 00:29:57.600
should be the thing that we should be trying to do.

00:29:57.720 --> 00:29:59.100
And this is then to the spirit

00:29:59.120 --> 00:30:04.180
I think Matt Rocklin talks to and what we on the sort of anaconda data science oriented side also

00:30:04.460 --> 00:30:09.180
hope for right is that to make as much of this disappear in the background as possible for people

00:30:09.180 --> 00:30:15.220
who don't want to learn it who don't need to know it necessarily yeah I think we want to get we all

00:30:15.320 --> 00:30:20.800
want to score well on the plays well with others scorecard right so you know if we can deploy and

00:30:20.840 --> 00:30:26.919
use containers that means it's much easier to onboard the next dev yeah and a lot of this

00:30:27.560 --> 00:30:29.580
not everyone has to be an expert at it.

00:30:30.120 --> 00:30:30.440
Correct.

00:30:30.800 --> 00:30:36.900
A couple people set up a cluster or some Docker Compose system together,

00:30:37.520 --> 00:30:38.340
and then you all get to use it.

00:30:38.400 --> 00:30:41.080
It's a little bit like the people that work on Jupyter

00:30:41.100 --> 00:30:42.900
have to do a lot of JavaScript and TypeScript,

00:30:43.420 --> 00:30:45.080
so the rest of us don't have to do so much.

00:30:45.500 --> 00:30:45.600
Right.

00:30:47.240 --> 00:30:48.940
Although you just worked out a little HTML editing,

00:30:49.080 --> 00:30:49.640
so I was pretty slick.

00:30:52.660 --> 00:30:56.780
Yeah, I think here's a good question from a little bit earlier from Pamela,

00:30:56.920 --> 00:31:00.060
but I think especially this one goes out to you, Calvin.

00:31:00.120 --> 00:31:02.180
I think you've walked this path recently.

00:31:02.920 --> 00:31:05.740
How much harder is Kubernetes versus Docker Compose

00:31:05.740 --> 00:31:06.700
to learn for a web dev?

00:31:08.080 --> 00:31:10.760
I think if you have a good template to start from,

00:31:11.560 --> 00:31:13.760
that's where this becomes a no-brainer.

00:31:14.220 --> 00:31:16.220
If you were to try and go learn all the things

00:31:16.500 --> 00:31:20.020
about the Kubernetes stack orchestrators,

00:31:21.080 --> 00:31:24.160
the storage bits, all these kind of pieces,

00:31:24.400 --> 00:31:25.480
that could be really overwhelming.

00:31:25.860 --> 00:31:28.940
Whereas Docker Compose, it's one file.

00:31:29.660 --> 00:31:30.680
It lists your services.

00:31:30.870 --> 00:31:32.220
It feels fairly readable.

00:31:32.540 --> 00:31:33.200
It's just YAML.

00:31:34.420 --> 00:31:37.080
Kubernetes is going to have a few more things going on under the covers.

00:31:37.700 --> 00:31:42.720
But again, I'll point to our SCAF example as a minimal,

00:31:43.600 --> 00:31:47.320
as little as you needed to get going version of being able to do Kubernetes

00:31:48.240 --> 00:31:50.680
locally and in a sandbox and production environment.

00:31:51.050 --> 00:31:52.680
So it scales up to all those pieces.

00:31:53.320 --> 00:31:56.420
So as a web dev, you just develop your code locally.

00:31:56.580 --> 00:31:58.780
You use your IDE, you're in PyCharm, you're in VS Code,

00:31:59.560 --> 00:32:00.780
you're editing your files locally.

00:32:01.540 --> 00:32:05.280
Tools like Tilt are kind of hiding a lot of that complexity

00:32:07.320 --> 00:32:09.700
under the covers for you and synchronizing files two-way.

00:32:10.240 --> 00:32:12.280
So if things happen in the container, for example,

00:32:12.840 --> 00:32:14.360
you probably want to be able to build,

00:32:14.660 --> 00:32:16.900
compile your dependencies with the hashes

00:32:17.440 --> 00:32:19.100
in the target container environment

00:32:19.340 --> 00:32:20.360
that you're going to release to.

00:32:20.700 --> 00:32:22.180
Because if you did it locally and you're on Windows,

00:32:22.420 --> 00:32:23.340
or on Mac or on Linux,

00:32:23.600 --> 00:32:25.320
you're gonna get potentially different hashes,

00:32:25.820 --> 00:32:27.860
different versions of different dependencies.

00:32:28.640 --> 00:32:30.820
So those kinds of things need to be able to write back

00:32:31.060 --> 00:32:32.600
from the container to your local file system

00:32:32.800 --> 00:32:35.400
and Tilt enables that and takes that whole pain away.

00:32:35.880 --> 00:32:39.640
I think Tilt was the big changing point for me,

00:32:40.140 --> 00:32:41.920
the inflection point for me when I moved over

00:32:42.460 --> 00:32:44.580
and fully embraced Kubernetes for local web dev.

00:32:45.740 --> 00:32:46.160
- Interesting.

00:32:49.160 --> 00:32:50.660
Over at Talk Python, I've got,

00:32:50.860 --> 00:32:54.980
I think last time I counted 23 different things

00:32:55.200 --> 00:32:56.540
running in Docker containers

00:32:57.760 --> 00:33:00.420
sort of managed by a handful of Docker Compose things

00:33:00.620 --> 00:33:02.460
that group them by what they're related to.

00:33:02.510 --> 00:33:03.480
And it's been awesome.

00:33:03.940 --> 00:33:07.220
It's been, it really lets you isolate things.

00:33:07.590 --> 00:33:09.000
The server doesn't get polluted

00:33:09.200 --> 00:33:12.120
with installing this version of Postgres

00:33:12.190 --> 00:33:13.140
or that version of Mongo.

00:33:13.220 --> 00:33:14.860
I think I've got two versions of Postgres,

00:33:15.440 --> 00:33:18.000
another version of MongoDB and a few other things.

00:33:18.260 --> 00:33:19.140
Yeah, and it just doesn't matter.

00:33:19.480 --> 00:33:20.580
Do I RAM and see people?

00:33:20.860 --> 00:33:27.740
20. Okay, good. And you can run in a one CPU or one server node, you don't need to have five

00:33:28.180 --> 00:33:32.520
machines running with a control plane and all the pieces you will have the control plane, but you

00:33:32.530 --> 00:33:39.200
can use like K3S is a minimal Kubernetes project that you can use to deploy, for example, on a

00:33:39.310 --> 00:33:43.720
single EC2 instance, spin that up, deploy your containers. Now you can hook it up to your GitHub

00:33:43.920 --> 00:33:48.599
actions, which I think we should also talk about is something people should learn. You hook that up

00:33:48.640 --> 00:33:53.800
and away you go, you're now releasing without logging into a server and typing git pole and

00:33:54.020 --> 00:34:03.860
potentially injecting into it unintended changes from your version control. It's a peace of mind

00:34:04.330 --> 00:34:09.200
to be able to know and audit and know what you've released is exactly what you expected to get

00:34:09.300 --> 00:34:16.379
released. So I want to wrap up this container side of things with two thoughts. First,

00:34:17.720 --> 00:34:18.659
I'm a big fan of dogs.

00:34:19.040 --> 00:34:20.040
I don't know if you guys know,

00:34:20.040 --> 00:34:21.860
but I kind of understand what dogs say a little bit.

00:34:21.919 --> 00:34:22.379
It's a little weird.

00:34:22.840 --> 00:34:25.080
I believe Calvin's dog, I don't know,

00:34:25.240 --> 00:34:26.000
Peter, back me up here.

00:34:26.000 --> 00:34:27.240
I believe Calvin's dog said,

00:34:27.760 --> 00:34:29.600
forget containers I edit in production.

00:34:30.000 --> 00:34:31.659
I think that's what the dog said when it barked.

00:34:31.659 --> 00:34:32.360
I'm not entirely sure.

00:34:33.139 --> 00:34:34.100
I mean, he is a black dog.

00:34:37.540 --> 00:34:38.340
You never know.

00:34:38.419 --> 00:34:38.800
You never know.

00:34:38.820 --> 00:34:39.700
They're known for being rebels.

00:34:40.379 --> 00:34:41.080
That's right, exactly.

00:34:41.419 --> 00:34:43.000
Not the black sheep, but the black lab.

00:34:43.080 --> 00:34:43.580
The black dog.

00:34:44.700 --> 00:34:47.220
And then the second one I want to kind of close this out with

00:34:47.240 --> 00:34:49.280
is C4Yourself out on YouTube says,

00:34:49.340 --> 00:34:52.780
I like Python for low-code ML with PyCaret.

00:34:53.399 --> 00:34:55.820
The problem is that Python is now up to 3.13.3,

00:34:56.000 --> 00:34:59.980
and very soon 3.14.0, folks, while PyCaret only supports

00:35:00.800 --> 00:35:01.600
up to 3.11.

00:35:02.220 --> 00:35:04.880
And I think this is a good place to touch on reproducibility

00:35:05.120 --> 00:35:05.680
and isolation.

00:35:06.420 --> 00:35:08.580
Like you could make a container that just runs 3.11,

00:35:09.380 --> 00:35:11.020
and it doesn't matter what your server has, right, Peter?

00:35:11.460 --> 00:35:12.040
PETER LUBBERS: Yeah.

00:35:14.320 --> 00:35:15.220
pop up the question again.

00:35:17.310 --> 00:35:20.920
I was, I think it was just that PyCaret, yeah.

00:35:21.520 --> 00:35:25.560
So this, I guess I don't really see the,

00:35:26.280 --> 00:35:28.200
I don't see the problem.

00:35:29.440 --> 00:35:31.280
Like this is a statement of fact, right?

00:35:31.380 --> 00:35:33.060
The PyCaret only supports 3.11.

00:35:33.620 --> 00:35:36.140
Are there features that you really want to see in 3.13

00:35:36.270 --> 00:35:38.120
or that you really need to use in 3.13?

00:35:38.580 --> 00:35:43.800
Or, I mean, there's, there are some changes, right?

00:35:44.220 --> 00:35:48.800
Yeah, but it could be they get a new Linux machine that's Ubuntu, and it's got 312 on it.

00:35:48.980 --> 00:35:49.120
Yep.

00:35:50.160 --> 00:35:56.280
I mean, but you never, okay, this might be where the dog barks again, but you never use the system Python.

00:35:56.720 --> 00:35:56.940
Well, right.

00:35:58.480 --> 00:36:00.180
It doesn't matter what the system ships with.

00:36:00.220 --> 00:36:01.460
I mean, what does macOS ship with?

00:36:01.460 --> 00:36:01.840
I don't know.

00:36:02.480 --> 00:36:13.200
You install, you either install a distribution like, you know, Anaconda, Miniconda, or something like this, or uv, using Python standalone, the virtual environments there, have their own, ship their own Python.

00:36:13.280 --> 00:36:20.020
on. This is now I because I am who I am like on the Anaconda side of things, we've known

00:36:20.110 --> 00:36:25.100
that you have in order to really isolate your Python installation, you really have to have

00:36:25.240 --> 00:36:30.360
the interpreter itself be built with the same tool chain and the same versions of the tool

00:36:30.460 --> 00:36:34.600
chain as all the libraries within it. And so this is what the conda universe, conda forge,

00:36:35.160 --> 00:36:39.620
bioconda, we've been doing this forever. And then with uv, I think uv is really pushed

00:36:39.640 --> 00:36:43.820
been spearheading the whole install a separate Python bit.

00:36:43.870 --> 00:36:46.900
I know that Python has been there, but it's not –

00:36:47.630 --> 00:36:49.560
I don't think it was a standard part of –

00:36:49.620 --> 00:36:50.460
Yeah, it wasn't a standard part.

00:36:50.630 --> 00:36:52.600
It wasn't a standard part of practice for people.

00:36:53.440 --> 00:36:57.240
But I'm hoping that uv helps to change minds in this way as well.

00:36:57.440 --> 00:37:01.520
But ultimately, if you do all the bits right,

00:37:02.190 --> 00:37:04.640
you actually can have a isolated and separated,

00:37:05.400 --> 00:37:10.800
perfectly isolated Python install without needing to use containerization.

00:37:11.740 --> 00:37:13.020
Not there's anything wrong with containerization,

00:37:13.160 --> 00:37:16.740
but just saying this is a solveable problem.

00:37:17.180 --> 00:37:19.120
It's just so darn complicated to try to give

00:37:19.400 --> 00:37:21.260
anyone best practices in the final packaging world.

00:37:21.900 --> 00:37:23.500
Because some guidance can be wrong for somebody.

00:37:24.340 --> 00:37:25.880
But in this case, yes,

00:37:26.210 --> 00:37:28.780
you could absolutely use containers to isolate that,

00:37:28.980 --> 00:37:33.279
or look to use conda or uv to create

00:37:33.360 --> 00:37:38.600
It's an isolated install with just that version of Python just to run than PyCaret inside it.

00:37:38.980 --> 00:37:46.620
Yeah, I feel like containers is a pure expression of an isolated environment where you can't get it messed up.

00:37:47.080 --> 00:37:50.820
If you do anything, just know that the system Python is not your Python.

00:37:51.610 --> 00:37:52.720
You shouldn't be allowed to use it.

00:37:52.770 --> 00:37:59.080
It should be almost in a user private bin path that's not usable by people.

00:37:59.780 --> 00:38:00.900
Calvin, I've been on a journey.

00:38:01.700 --> 00:38:04.500
It's a failed journey, but it was a long, solid attempt.

00:38:05.020 --> 00:38:11.260
I've been trying to remove Python from my system as a global concept, period.

00:38:11.880 --> 00:38:15.620
But I'm a big fan of Homebrew, and too many things that Homebrew wanted.

00:38:15.900 --> 00:38:20.480
And I know something's gone wrong when my app falls back to use in Python 3.9.

00:38:20.580 --> 00:38:21.660
I'm like, no, Homebrew.

00:38:22.440 --> 00:38:27.240
I deleted all my local Pythons, Pyamv and Homebrew, and any packages that depended on it.

00:38:27.400 --> 00:38:31.600
And I went fully uv and uvx for any tools that would rely on it.

00:38:32.040 --> 00:38:33.100
And then we've also moved to Nix.

00:38:33.340 --> 00:38:37.480
We've started using Nix for our package management instead of homebrew for that reason.

00:38:37.480 --> 00:38:38.240
Okay, sure.

00:38:39.980 --> 00:38:45.500
Yeah, I will say, I think maybe the next one I want to throw out there to talk about is just is uv.

00:38:46.460 --> 00:38:50.840
It was compelling when it was uv pip install and uv VENV.

00:38:51.100 --> 00:38:57.440
But I think Peter really hit the nail on the head when once it sort of jujitsu'd Python

00:38:57.630 --> 00:38:58.960
and said, okay, now here's the deal.

00:38:59.840 --> 00:39:00.680
We manage Python.

00:39:01.090 --> 00:39:02.180
Python doesn't manage us.

00:39:03.340 --> 00:39:05.140
It just uncorked the possibilities, right?

00:39:05.300 --> 00:39:11.260
Because you can say uv, VE, and V and specify a Python version, and it's not even on your

00:39:11.480 --> 00:39:11.620
machine.

00:39:12.150 --> 00:39:16.020
Two seconds later, it's both installed on your machine and you have a virtual environment

00:39:16.110 --> 00:39:16.640
based on it.

00:39:17.180 --> 00:39:18.220
And you don't have to think about it.

00:39:18.560 --> 00:39:20.300
You know, Peter, you talked about PyEmp.

00:39:20.440 --> 00:39:25.160
It's great, but it compiles Python on a machine that's super slow and error prone.

00:39:25.550 --> 00:39:28.920
Because if your build tools in your machine aren't quite right, then well, sorry.

00:39:29.020 --> 00:39:30.180
Compiling Python is no joke.

00:39:30.760 --> 00:39:31.340
No, it isn't.

00:39:31.680 --> 00:39:33.840
I used to do it for a while for talk Python in production.

00:39:34.370 --> 00:39:35.400
It was like a 10 minute deal.

00:39:37.079 --> 00:39:37.440
Yeah.

00:39:37.470 --> 00:39:38.220
I had to automate it.

00:39:38.220 --> 00:39:39.340
It was fine, but it took 10 minutes.

00:39:39.510 --> 00:39:40.220
There was no rush in there.

00:39:40.570 --> 00:39:44.120
And the great irony of this is that, again,

00:39:44.280 --> 00:40:03.900
like we in the data science world have spent years trying to convince certain folks in the sort of non-data and science Python world that you can't solve the Python packaging problem without including the management of Python itself in it.

00:40:04.500 --> 00:40:05.580
and we just got nowhere.

00:40:05.660 --> 00:40:08.920
We're just repeatedly told PyCon after PyCon,

00:40:09.160 --> 00:40:10.280
Packaging Summit after Packaging Summit.

00:40:10.920 --> 00:40:12.400
The scope of a Python package manager

00:40:12.960 --> 00:40:15.740
is to manage the things inside site packages

00:40:16.500 --> 00:40:17.820
and anything outside of that,

00:40:18.040 --> 00:40:22.800
system libraries, libping, libtif, opencv,

00:40:23.240 --> 00:40:24.320
these things are outside of scope.

00:40:25.520 --> 00:40:26.860
And many distributions,

00:40:27.440 --> 00:40:30.760
there's Linux distros like Debian or Red Hat.

00:40:31.520 --> 00:40:33.380
There's distribution vendors like us and Akana

00:40:33.380 --> 00:40:34.280
that are cross-platform.

00:40:35.320 --> 00:40:37.140
We were trying to make the case for this,

00:40:37.280 --> 00:40:40.060
but we just kept not landing that argument.

00:40:40.560 --> 00:40:41.560
UV comes along and does it,

00:40:41.830 --> 00:40:43.620
and everyone's like, oh, this is totally the way to do it.

00:40:44.359 --> 00:40:47.000
It's like, well, I guess the users finally have,

00:40:47.190 --> 00:40:48.540
you know, I think that we can follow,

00:40:48.630 --> 00:40:49.640
we can pave that cow path.

00:40:50.120 --> 00:40:51.960
And I agree, it is utterly the way to do it.

00:40:52.400 --> 00:40:53.080
And then what we're going to learn,

00:40:53.260 --> 00:40:54.240
I think on the other side of that is,

00:40:54.480 --> 00:40:58.060
oh, not only is it great to manage Python

00:40:58.190 --> 00:40:59.180
as part of the whole thing,

00:40:59.780 --> 00:41:02.120
but now we actually should care how we build that Python

00:41:02.140 --> 00:41:08.280
because your choice of clang gcc your choice of what particular libraries you link in that determines

00:41:08.350 --> 00:41:13.700
the tool chain for compiling everything else as well and especially we talk about data ai ml kinds

00:41:13.860 --> 00:41:19.480
of libraries there's there's incompatibilities that will emerge as you try to install this install

00:41:19.680 --> 00:41:25.140
that so um i gave a talk at pybay sorry to sort of toot my own horn a little bit but now go for

00:41:25.140 --> 00:41:30.880
i gave a talk at pybay last fall about the five demons of python packaging where i try to unravel

00:41:30.880 --> 00:41:34.700
Why is this perennial problem so gnarly and horrible?

00:41:36.160 --> 00:41:38.140
And it's because there's many dimensions of it.

00:41:38.170 --> 00:41:40.900
And most users only care about one and a half of those dimensions.

00:41:41.550 --> 00:41:44.540
They just really want to install the damn package and just use it.

00:41:45.180 --> 00:41:47.420
But if you're a maintainer, that's right.

00:41:48.100 --> 00:41:49.800
You got to have the obligatory Blade Runner.

00:41:51.340 --> 00:41:56.280
And anyway, so I put that talk together just to sort of get everyone on the same page to

00:41:56.300 --> 00:42:02.880
understand why we have different tools, why distribution vendors, whether it's an Anaconda

00:42:03.140 --> 00:42:05.320
or an Ubuntu, a Red Hat or Nix, right?

00:42:05.560 --> 00:42:06.040
Why homebrew?

00:42:06.090 --> 00:42:06.940
These things do matter.

00:42:07.030 --> 00:42:08.640
And there's a reason people depend on these tools.

00:42:10.039 --> 00:42:16.300
And anyway, I hope that people who care about Python packaging or want to understand more

00:42:16.480 --> 00:42:21.780
deeply go and look at this talk because I do try to put time into each of their own topics

00:42:21.930 --> 00:42:22.880
that make this so complicated.

00:42:23.220 --> 00:42:26.260
And for Python in particular, because I hear a lot of people talking about, why don't we

00:42:26.280 --> 00:42:27.500
Why isn't it as easy as Rust?

00:42:28.000 --> 00:42:29.980
Or, oh, you know, npm is so nice.

00:42:30.640 --> 00:42:31.400
Well, I don't hear that very often.

00:42:31.980 --> 00:42:32.340
Is it?

00:42:32.820 --> 00:42:33.060
No, no, no.

00:42:33.290 --> 00:42:35.200
Actually, I don't hear a lot of praise for npm.

00:42:35.600 --> 00:42:37.040
But, like, why doesn't JavaScript have this problem?

00:42:37.070 --> 00:42:40.420
It's like, well, JavaScript doesn't have a pile of Fortran involved in it, right?

00:42:41.240 --> 00:42:43.840
Many people don't know, but, you know, there's a fun thing in there.

00:42:43.840 --> 00:42:46.580
I talk about the fact that if you want to use MBConvert,

00:42:47.040 --> 00:42:49.500
if you want to use MBConvert to convert Notebook into a PDF,

00:42:50.070 --> 00:42:53.340
you need a Haskell compiler because Pandoc depends on Haskell.

00:42:53.760 --> 00:42:58.460
So like there's just things like that that are just our ecosystem is replete with these things.

00:42:59.500 --> 00:43:05.340
And most users don't have to see it if the upstream folks, the distribution folks, the packaging people are doing their jobs right.

00:43:05.690 --> 00:43:06.880
But that doesn't mean that it's not hard.

00:43:06.880 --> 00:43:09.560
It doesn't mean that it's not real labor that goes into making it work.

00:43:11.240 --> 00:43:12.020
Yeah, look in the chat.

00:43:12.320 --> 00:43:17.300
Pamela points out that even using uv, there are now multiple ways, which is tricky.

00:43:18.280 --> 00:43:21.720
And I would refer to myself as one of the old school people.

00:43:22.280 --> 00:43:26.900
I still use uv kind of in a agnostic way.

00:43:27.140 --> 00:43:29.440
Like if people don't want to use uv and they take one of my projects,

00:43:29.620 --> 00:43:32.460
they can still use pip and they can still use pip-tools.

00:43:32.580 --> 00:43:38.040
And so I'll use things like uv, V, ENV or uv pip install or,

00:43:38.560 --> 00:43:42.040
you know, pip compile, especially to build out the pin requirements.

00:43:42.300 --> 00:43:45.560
But if you don't like it, you just pip install -r requirements.txt

00:43:45.660 --> 00:43:47.840
instead of using what I was doing, right?

00:43:48.240 --> 00:43:48.560
Mm-hmm.

00:43:49.700 --> 00:43:50.960
And then there's this other way of embracing,

00:43:51.680 --> 00:43:55.380
like let it sort of manage your PyProject.tomal entirely

00:43:55.700 --> 00:43:56.980
and create it and so on.

00:43:58.160 --> 00:44:01.460
So I think there is a little bit of confusion,

00:44:01.580 --> 00:44:02.360
but I think...

00:44:02.760 --> 00:44:03.560
Yeah, it's probably good.

00:44:03.600 --> 00:44:05.440
It's a huge support for the Python packaging community,

00:44:05.460 --> 00:44:05.760
for sure.

00:44:06.020 --> 00:44:07.980
It's good they made that compatibility path, though.

00:44:08.260 --> 00:44:10.200
It helps people be comfortable

00:44:10.400 --> 00:44:11.320
because change is hard.

00:44:11.640 --> 00:44:13.600
As humans, we don't like change,

00:44:14.160 --> 00:44:15.320
but this is a really good change.

00:44:15.560 --> 00:44:18.540
That speed is a feature that Charlie talks about.

00:44:19.340 --> 00:44:21.080
that 100% I'm on the onboard.

00:44:22.440 --> 00:44:23.520
Yeah, I agree.

00:44:23.670 --> 00:44:25.760
And it's changed even my Docker stuff.

00:44:26.420 --> 00:44:32.340
Now, one of my Docker layers is just uv Python install.

00:44:32.620 --> 00:44:34.300
Really, I think it just creates a virtual environment,

00:44:34.410 --> 00:44:35.580
which also installs the Python.

00:44:35.690 --> 00:44:37.900
And that's a two-second, one-time deal.

00:44:38.190 --> 00:44:39.260
And it's off to the races.

00:44:39.460 --> 00:44:40.180
It's really, really nice.

00:44:41.040 --> 00:44:41.260
All right.

00:44:42.140 --> 00:44:44.640
We have probably time for a few more topics.

00:44:45.050 --> 00:44:47.160
However, if I put this out into the world,

00:44:47.800 --> 00:44:52.460
it may consume all of the time as it does pretty much all of the GPUs.

00:44:54.680 --> 00:44:58.220
What are your thoughts on Agenda coding?

00:44:58.480 --> 00:45:03.840
What are your thoughts on LLMs and Agenda coding AI and that whole soup of

00:45:04.000 --> 00:45:04.300
craziness?

00:45:04.900 --> 00:45:09.120
I'm shocked how many people are not diving in headfirst on this.

00:45:09.340 --> 00:45:15.160
I literally started talking to a developer last week and I was like,

00:45:15.280 --> 00:45:16.480
hey, we tried Claude Code.

00:45:17.180 --> 00:45:18.760
And they were like, no, what's that?

00:45:18.880 --> 00:45:19.900
I was like, oh my.

00:45:20.500 --> 00:45:21.460
Like, yeah, exactly.

00:45:21.859 --> 00:45:22.980
Well, we've got Copilot.

00:45:23.200 --> 00:45:24.620
I think the issue is in the enterprise,

00:45:25.080 --> 00:45:28.420
a lot of people have opted to purchase Copilot

00:45:28.710 --> 00:45:31.940
because it's a checkbox and a one-click purchase.

00:45:32.130 --> 00:45:32.600
So it's easy.

00:45:33.600 --> 00:45:35.160
But they're not giving them Copilot Studio,

00:45:35.360 --> 00:45:36.540
which is the agentic version of it.

00:45:36.920 --> 00:45:38.480
They're just like, yeah, you've got your LLMs now.

00:45:39.130 --> 00:45:39.680
Go have fun.

00:45:40.390 --> 00:45:42.260
I think they're really missing out on the true power

00:45:42.580 --> 00:45:45.240
of like a tool that can inspect your file system

00:45:45.260 --> 00:45:50.400
a tool that can like look at things and do actions now obviously that introduces risk so a lot of

00:45:50.520 --> 00:45:56.360
these security people in these environments are not excited about that level of risk um i don't have a

00:45:56.370 --> 00:46:00.540
good answer for that other than if you're a developer and you're going to turn on jt coding

00:46:00.650 --> 00:46:06.360
you kind of have to like sign up and be accountable for what it's going to do i've got some ideas and

00:46:06.440 --> 00:46:10.619
some concrete recommendations for you but peter i want to hear what you have to say first we so

00:46:10.640 --> 00:46:16.120
first of all i think um vibe coding is simultaneously oversold at the same time

00:46:16.940 --> 00:46:24.420
um i'm very bullish on where this can go the ultimately the transformers models and that style

00:46:25.080 --> 00:46:31.780
of um current era ai has some structural mathematical limitations and the recent open ai

00:46:31.960 --> 00:46:36.820
paper about hallucinations are inevitable and sort of part of the math uh sort of shows that yeah we're

00:46:36.800 --> 00:46:40.800
We're going to end up, it is to some extent glorious high dimensional autocomplete.

00:46:41.120 --> 00:46:42.980
But oh my God, is it glorious when it's right.

00:46:43.420 --> 00:46:44.420
So it is steerable.

00:46:44.720 --> 00:46:49.360
It's like trying to fly a very, very awkward airplane before we've really figured out aerodynamics.

00:46:50.140 --> 00:46:51.240
But it kind of still does work.

00:46:51.780 --> 00:46:57.020
So people should absolutely 100% be looking at what this can do for them.

00:46:57.500 --> 00:47:05.580
And thinking really right now, like I would say actually the limitations, the known, the visible limitations of Vibe Coding,

00:47:06.300 --> 00:47:08.680
should actually, you should be grateful for that

00:47:08.870 --> 00:47:11.080
because that gives us time and space to think about

00:47:11.610 --> 00:47:13.600
how would we design projects?

00:47:15.160 --> 00:47:18.800
Because I know for myself, the way I code is I write doc strings

00:47:19.340 --> 00:47:21.840
and comments and sort of class structures first.

00:47:22.370 --> 00:47:24.500
And then I think about what needs to play with what

00:47:24.920 --> 00:47:25.800
and you're writing documentation.

00:47:26.600 --> 00:47:29.240
And if I can just have the code itself just get filled out with that,

00:47:29.370 --> 00:47:31.820
like, holy crap, like, of course, right?

00:47:32.280 --> 00:47:35.340
So everyone should be doing this so they can think about it

00:47:35.360 --> 00:47:37.240
and really think about where this stuff will go

00:47:37.620 --> 00:47:39.140
because it's definitely going to get better.

00:47:40.040 --> 00:47:42.200
But if you're worried about the data leakage

00:47:42.740 --> 00:47:44.540
and the compliance and all this other stuff,

00:47:45.180 --> 00:47:46.120
use local models.

00:47:46.320 --> 00:47:49.060
Go and buy, expense a couple of GPUs.

00:47:49.160 --> 00:47:51.640
3090s actually work fine with the newer, smaller models.

00:47:52.260 --> 00:47:54.580
If you work for a richer employer,

00:47:54.820 --> 00:47:56.560
maybe you can get a couple of 5090s.

00:47:57.400 --> 00:47:58.980
Sacrifice a gaming PC, come on.

00:47:59.780 --> 00:48:00.920
It's also a gaming PC.

00:48:01.020 --> 00:48:02.120
It's also a gaming PC.

00:48:03.580 --> 00:48:06.640
I have an M4 Mac with 64 gig of RAM, and it's wonderful.

00:48:06.920 --> 00:48:08.020
I've got DevStroll running.

00:48:08.200 --> 00:48:10.420
I've got the OSS GPT running.

00:48:10.960 --> 00:48:14.180
All those tools run on a just base model.

00:48:14.190 --> 00:48:14.780
Well, a base model.

00:48:15.130 --> 00:48:16.340
A little super tough Mac.

00:48:16.820 --> 00:48:20.620
Yeah, I have a 32 gig Mac mini running here,

00:48:20.780 --> 00:48:24.900
and I'm running the 20 billion parameter OpenAI model on it

00:48:25.000 --> 00:48:27.160
just to be shared with all my computers and my laptop.

00:48:28.060 --> 00:48:30.380
Yeah, and there's also, you know,

00:48:30.440 --> 00:48:33.860
the Chinese models are really freaking good, you know?

00:48:33.940 --> 00:48:36.400
And the, I mean, I think, I don't know what,

00:48:36.540 --> 00:48:38.240
we'll see what happens with CES next year,

00:48:38.620 --> 00:48:41.640
but I feel like this year was the year of small models.

00:48:41.790 --> 00:48:42.960
This year was the year, I mean,

00:48:42.990 --> 00:48:44.620
we started the year with DeepSeek, right?

00:48:45.120 --> 00:48:47.100
And so it's like not just Chinese labs saying,

00:48:47.170 --> 00:48:48.840
we don't need your stinking whatever,

00:48:49.600 --> 00:48:51.640
but over the course of the year, we got Kimi,

00:48:51.650 --> 00:48:53.820
we got Quinn, we got GLM, we got,

00:48:54.180 --> 00:48:55.240
we're just going to keep getting these.

00:48:55.330 --> 00:48:56.880
And that's not even, that's just on the code

00:48:57.260 --> 00:48:58.300
and the text prompting side.

00:48:58.420 --> 00:48:59.780
That's not even on image generation.

00:49:00.320 --> 00:49:03.200
Some of the Chinese image and video generation models are just jaw-droppingly good.

00:49:03.570 --> 00:49:06.960
So I think what we're going to see here is by the beginning of next year,

00:49:07.480 --> 00:49:09.520
well, this is a 25-26 podcast, right?

00:49:10.040 --> 00:49:15.460
So in 26, you probably have no excuse to say, why are you not?

00:49:16.600 --> 00:49:20.100
You know, professional CAD and engineering people have big workstations.

00:49:20.150 --> 00:49:22.140
As a dev, maybe you just have a big workstation now

00:49:22.730 --> 00:49:26.380
or a fat 128-gig unified memory M4 Mac,

00:49:26.860 --> 00:49:29.340
but you're just going to have that as your coding station

00:49:29.360 --> 00:49:30.260
and everything is local,

00:49:31.040 --> 00:49:32.640
you're going to be careful with tool use, of course,

00:49:32.780 --> 00:49:34.420
but still, like, you just run it all locally.

00:49:34.800 --> 00:49:35.920
I think as a developer,

00:49:37.580 --> 00:49:39.900
one of the key skills you should learn

00:49:40.060 --> 00:49:41.280
is going to be context engineering

00:49:41.880 --> 00:49:44.420
and using sub-processes.

00:49:44.890 --> 00:49:46.680
Like, the models now support, you know,

00:49:46.840 --> 00:49:49.360
basically spinning off parallel instances of themselves.

00:49:49.570 --> 00:49:50.740
And you can spin off parallel instances

00:49:51.160 --> 00:49:53.100
with a limited amount of context

00:49:53.380 --> 00:49:56.800
to kind of really shape how they understand things.

00:49:58.319 --> 00:50:00.760
Because Google reintroduced the Gemini

00:50:00.810 --> 00:50:03.380
with like a 1 million token context window limit.

00:50:03.960 --> 00:50:04.400
So what?

00:50:04.530 --> 00:50:05.300
What are you going to do with that?

00:50:05.340 --> 00:50:08.500
It's really not useful to just feed a million tokens into it

00:50:09.130 --> 00:50:09.960
because it can't,

00:50:10.400 --> 00:50:12.080
it's just as much as you tried to like-

00:50:12.230 --> 00:50:13.940
Well, it tapers off at the end as well.

00:50:13.940 --> 00:50:16.020
It's not really a million tokens.

00:50:16.020 --> 00:50:17.200
Right, you don't get a million tokens.

00:50:17.200 --> 00:50:18.220
And it's also,

00:50:18.220 --> 00:50:19.880
it's just going to be thoroughly confused

00:50:19.880 --> 00:50:21.300
by all the context you just threw at it.

00:50:21.300 --> 00:50:24.740
But if you can give a really narrow focus context,

00:50:24.800 --> 00:50:25.000
small diffs,

00:50:25.000 --> 00:50:26.080
that's one of the things I liked about Aider Chat.

00:50:26.380 --> 00:50:31.400
If you've not checked out AdrChat, it has a diff mode that really limits the amount of tokens it consumes.

00:50:31.460 --> 00:50:37.400
So actually it's a little more efficient on tokens than like cloud code, even if you're using the anthropic models the same way.

00:50:38.120 --> 00:50:40.520
Because it'll do diffs and send smaller context.

00:50:41.120 --> 00:50:51.820
And if you can leverage that with like sub models or sub prompts and Goose, the chat agent from Block has recipes that actually operate in like a sub model.

00:50:52.000 --> 00:50:53.640
So it's basically like you're building your own little tools

00:50:54.220 --> 00:50:56.300
that are just descriptions of like what MCP pieces

00:50:56.410 --> 00:50:57.880
it should use, what tools should be available

00:50:58.540 --> 00:51:01.420
and use this context and only pass me back that bit

00:51:01.430 --> 00:51:03.780
and throw away the extra context once you're done.

00:51:03.870 --> 00:51:06.140
So you're not polluting your context window

00:51:06.700 --> 00:51:08.740
with a whole bunch of unneeded operation.

00:51:09.190 --> 00:51:10.700
And now you get back really what's needed

00:51:10.790 --> 00:51:12.020
for whatever you're trying to work on.

00:51:13.000 --> 00:51:13.140
Yeah.

00:51:13.980 --> 00:51:16.040
So I wanna kind of take it a little bit higher level

00:51:16.200 --> 00:51:16.720
back real quick.

00:51:18.140 --> 00:51:18.800
Calvin, I'm with you.

00:51:18.860 --> 00:51:20.120
If you have not seen this,

00:51:20.260 --> 00:51:26.460
And I've talked to a lot of really smart devs who are like, yeah, I tried Copilot or I tried one of these things.

00:51:27.040 --> 00:51:30.880
And their experience is largely, I think, with the multi-line autocomplete.

00:51:31.520 --> 00:51:33.320
And to me, that, I just turn that off.

00:51:33.440 --> 00:51:33.920
That's like garbage.

00:51:35.040 --> 00:51:38.420
It's not garbage, but it's, let me put it more kindly.

00:51:38.980 --> 00:51:42.120
Half of the time, hitting tab is glorious.

00:51:42.660 --> 00:51:46.220
And the other half, I'm like, I want the first half, but the second half is wrong.

00:51:46.470 --> 00:51:48.600
So do I hit tab and then go down and delete it again?

00:51:48.860 --> 00:51:49.280
You know what I mean?

00:51:49.380 --> 00:51:52.040
I got to like, it's giving me too much and it's not quite right.

00:51:52.390 --> 00:51:55.920
But the agentic tool using part is stunning.

00:51:56.460 --> 00:51:59.900
Not with the cheap models, but with the models that cost like $20 a month.

00:52:00.700 --> 00:52:02.600
It's a huge difference from the very cheap model.

00:52:02.860 --> 00:52:05.440
Which is like, that's not even a latte a week, right?

00:52:05.620 --> 00:52:10.020
Like we're talking to an audience of probably mostly professional developers, right?

00:52:10.980 --> 00:52:18.440
You know, a hundred bucks a month, $200 a month for what literally is transforming the future of your entire industry is worth it.

00:52:18.560 --> 00:52:22.180
like why would you not subscribe to your and your employer should be paying for this like they should

00:52:22.180 --> 00:52:26.080
be handing you all well but if they do actually so here's the thing i'm actually two minds of this

00:52:26.310 --> 00:52:29.840
i think every dev for their own purposes for their own applications you're paying for their own

00:52:30.150 --> 00:52:34.520
because the employers will have limitations on what they're allowed to use they may have to sign up for

00:52:34.520 --> 00:52:40.300
an enterprise thing which has then data you know data retention policies yada yada yada and you

00:52:40.400 --> 00:52:45.779
want to just go full blast what is absolute cutting edge being released by various things yes but i

00:52:45.800 --> 00:52:50.920
would still again my little you know nerd like open source heart would not be uh stated unless

00:52:51.020 --> 00:52:57.760
i've made the comment here please play with local models please like have do work in a data

00:52:58.060 --> 00:53:04.340
sovereignty mode um because this is actually the closest the first time i think we've had real tech

00:53:04.780 --> 00:53:11.040
that could potentially move people away from a centralized computing model which has been i think

00:53:10.940 --> 00:53:14.280
so deleterious to our world, actually.

00:53:14.660 --> 00:53:16.520
And the last thing that we don't have time for,

00:53:16.530 --> 00:53:18.280
but the last thing I was going to just throw a shout out for

00:53:18.360 --> 00:53:19.380
was for people to check out Beware,

00:53:19.940 --> 00:53:22.880
because that is the way that we can build Python mobile applications

00:53:23.760 --> 00:53:26.760
and really be shipping applications that don't necessarily,

00:53:26.950 --> 00:53:28.260
like we should be deploying to mobile.

00:53:29.360 --> 00:53:31.340
So many Python developers are web devs,

00:53:31.800 --> 00:53:33.060
storing state in the Postgres somewhere,

00:53:33.410 --> 00:53:35.980
and we're part of that data concentration, data gravity problem.

00:53:36.799 --> 00:53:38.820
Whereas if we flip the bit and just learn for ourselves,

00:53:38.980 --> 00:53:41.960
How do we Vibecode an actual iOS platformer?

00:53:42.420 --> 00:53:43.580
Like, let's go do that, right?

00:53:43.700 --> 00:53:45.180
Or an Android thing, which is a little bit easier to deal with.

00:53:46.200 --> 00:53:47.640
These are things that we can actually do as Python.

00:53:47.680 --> 00:53:48.300
Totally doable, though, yeah.

00:53:48.500 --> 00:53:49.120
Yeah, totally doable.

00:53:49.780 --> 00:53:56.640
I want to give a shout out to you, Peter, and Anaconda in general for all the support for Beware and some of the PyScript and some of those other projects.

00:53:56.800 --> 00:54:00.000
Those are important ones, and yeah, good work.

00:54:01.100 --> 00:54:02.060
Just try to fight the good fight.

00:54:02.720 --> 00:54:03.200
Yeah, for sure.

00:54:03.300 --> 00:54:03.560
Thank you.

00:54:04.100 --> 00:54:05.900
I'm not quite done with this AI thing, though.

00:54:06.240 --> 00:54:11.800
I do want to say, I do want to point out this thing called Klein that recently came out.

00:54:11.960 --> 00:54:13.300
That's really pretty interesting.

00:54:13.800 --> 00:54:14.520
Have you guys heard of this?

00:54:14.800 --> 00:54:15.000
Yep.

00:54:15.300 --> 00:54:15.380
Yep.

00:54:15.720 --> 00:54:15.820
Yep.

00:54:16.000 --> 00:54:16.060
Yeah.

00:54:16.260 --> 00:54:17.260
So it's open source.

00:54:17.520 --> 00:54:21.380
It's kind of like cursor, but the big difference is they don't charge for inference.

00:54:21.460 --> 00:54:26.420
You just put in an API key and, or you put in a link to a URL to a local model.

00:54:26.800 --> 00:54:27.800
So you can use local with it.

00:54:27.840 --> 00:54:27.980
Yeah.

00:54:28.320 --> 00:54:28.380
Yeah.

00:54:28.780 --> 00:54:28.840
Yeah.

00:54:29.120 --> 00:54:33.179
And I mean, if you're using local models and you really want to go all in on the data sovereignty

00:54:33.200 --> 00:54:38.200
pieces use tools like little snitch on your mac to know if it's sending something someplace you're

00:54:38.460 --> 00:54:43.720
you didn't request it to send to you can be totally eyes wide open and maybe exercise a

00:54:43.800 --> 00:54:48.020
little more reckless abandon if you know that the tool like that can catch an outbound connection

00:54:48.600 --> 00:54:56.220
that you didn't expect yeah i think i'll give you how many how many how much i don't want to burn

00:54:56.160 --> 00:55:02.440
I will give you guys an example that I think probably will--

00:55:03.110 --> 00:55:06.320
if you've done a lot of web development and web design

00:55:07.040 --> 00:55:09.480
mixed, this will probably catch your attention.

00:55:11.100 --> 00:55:15.980
So I want to add some new features to talkpython.fm.

00:55:15.980 --> 00:55:19.220
I got some cool whole sections coming and announcements.

00:55:20.020 --> 00:55:24.219
But talkpython.fm was originally created and designed

00:55:24.240 --> 00:55:25.980
in 2015 on Bootstrap.

00:55:26.560 --> 00:55:29.420
Do you know how out of date 2015 Bootstrap is with

00:55:29.620 --> 00:55:30.720
modern day front end frameworks?

00:55:31.360 --> 00:55:34.760
A lot. But there's like 10,000 lines of HTML

00:55:35.720 --> 00:55:38.060
designed in Bootstrap, early Bootstrap.

00:55:38.180 --> 00:55:38.700
I'm like, oh.

00:55:40.080 --> 00:55:43.020
Hey, it still renders great on my phone though.

00:55:43.240 --> 00:55:45.760
And the LLMs are very aware of old Bootstrap documentation.

00:55:46.120 --> 00:55:46.380
I know.

00:55:46.720 --> 00:55:47.160
And issues.

00:55:48.040 --> 00:55:50.060
Peter, it looks great and it works well.

00:55:50.160 --> 00:55:51.820
But here's the thing, I want to add a whole bunch of

00:55:51.820 --> 00:55:53.440
new features and sections to it.

00:55:53.560 --> 00:55:54.980
Now, I've got to design that from scratch.

00:55:55.200 --> 00:55:57.300
I'm like, oh, I can't do this in Bootstrap 3.

00:55:57.420 --> 00:55:58.920
I just don't have the willpower for it.

00:55:59.540 --> 00:56:01.420
It's going to make it so hard, you know?

00:56:02.120 --> 00:56:04.520
And so I'm like, well, I really should redesign it,

00:56:04.520 --> 00:56:05.900
but that's got to be weeks of work.

00:56:07.060 --> 00:56:09.460
And one evening around 4 o'clock, I'm just hanging out,

00:56:10.920 --> 00:56:13.680
you know, enjoying the outside, sitting, working on my computer,

00:56:13.840 --> 00:56:16.040
trying to take in a little more summer before it's gone.

00:56:16.060 --> 00:56:16.680
And I'm like, you know what?

00:56:16.680 --> 00:56:21.559
I bet Claude Sonnet and I bet we could do this quicker than

00:56:22.980 --> 00:56:33.480
two hours later the entire site 5,000 lines of CSS 10,000 lines of template HTML files all

00:56:33.700 --> 00:56:40.320
rewritten in Bulma modern clean doesn't look at all different except for the few parts I'm like

00:56:40.320 --> 00:56:44.580
oh I don't like that rewrite that actually to the point where you just take a screenshot of what you

00:56:44.580 --> 00:56:48.140
do want throw it in there and go make it look like this oh yeah okay I see the picture let's make it

00:56:48.120 --> 00:56:54.080
look like that. And it's just a couple hours. That would be pulling your hair out the most tedious,

00:56:54.460 --> 00:57:00.280
painful work for a week or two. And now it's, if I want to add something to the site, it's just,

00:57:00.380 --> 00:57:04.400
oh yeah, it's just modern Bulma. Off it goes. Or I could have chose Tailwind or whatever.

00:57:06.160 --> 00:57:09.500
I think Bulma works a little better with AIs because it doesn't have build steps and all

00:57:09.580 --> 00:57:14.080
that kind of stuff. It's a little more straightforward. But those are the kinds of

00:57:14.180 --> 00:57:17.780
things that like, literally I wrote down a markdown plan. I said, here's what we're going to do.

00:57:17.900 --> 00:57:19.160
and I planned it out with AI.

00:57:19.300 --> 00:57:20.760
Then I said, okay, step one, step two.

00:57:20.820 --> 00:57:22.560
And we just worked it through until it was done.

00:57:22.680 --> 00:57:23.840
There's a few little glitches.

00:57:23.840 --> 00:57:24.620
I'm like, this looks weird.

00:57:24.860 --> 00:57:25.360
Here's a screenshot.

00:57:25.620 --> 00:57:25.860
Fix it.

00:57:25.960 --> 00:57:26.040
Okay.

00:57:26.820 --> 00:57:28.720
AI is really good at these kinds of tasks.

00:57:29.020 --> 00:57:29.100
Yeah.

00:57:29.260 --> 00:57:31.060
And if people have not seen this in action,

00:57:31.180 --> 00:57:32.340
I think it just doesn't.

00:57:32.660 --> 00:57:34.840
They're like, I tried to use ChatGPT

00:57:34.940 --> 00:57:35.700
and it gave me an answer,

00:57:35.760 --> 00:57:36.800
but it doesn't help that much.

00:57:36.820 --> 00:57:37.520
I could write that.

00:57:37.680 --> 00:57:39.720
Or I used a free cheat model

00:57:39.840 --> 00:57:40.420
and it got it wrong

00:57:40.580 --> 00:57:42.280
and I had to fix more than it helped me.

00:57:42.800 --> 00:57:44.680
There are these new tools that are crazy.

00:57:45.539 --> 00:57:46.960
There's something that people don't,

00:57:47.160 --> 00:57:52.020
think have an intuitive feeling for because they're they're encountering a cognitive reactive system

00:57:52.060 --> 00:57:58.320
for the first time uh i'm not saying sentient or conscious by the way but just cognitive and so

00:57:58.700 --> 00:58:07.980
it's sort of like it's it it's going to be as deep as how you probe it so if you ask it a dumb shallow

00:58:08.140 --> 00:58:12.920
thing it will give you a dumb shallow response if you you know but if you get really deep or nerdy

00:58:13.000 --> 00:58:17.120
and i was using it to i was using early incarnations actually a couple years back i

00:58:17.140 --> 00:58:22.400
first figured out this effect, I was reading some philosophy books, as one does. And I was thinking,

00:58:22.540 --> 00:58:26.900
well, I could use this as a co-reading tutor. And I noticed I would just ask for some reason,

00:58:27.000 --> 00:58:29.960
give me some summaries. I'm like, well, that's reasonable, but you know, okay, whatever.

00:58:30.540 --> 00:58:34.980
But then as I got deeper into some of the content and I was asking for contrasting opinions about

00:58:35.160 --> 00:58:38.860
different, from different other perspectives and some critiques and all this stuff. And I started

00:58:38.980 --> 00:58:44.240
getting into it, it would go very deep. And this is like GPT, just 4.0, it just come out kind of

00:58:44.260 --> 00:58:49.080
thing like time frame so i think the same thing is true now especially with like gpt5 research i've

00:58:49.080 --> 00:58:55.060
had feedback from friends who are like yeah some people say five is a nothing burger but five

00:58:55.340 --> 00:59:00.180
research is a thing because i'm able to do this is this other person not me but this other

00:59:00.250 --> 00:59:05.620
person i quote i'm able to get graduate level feedback like stuff that is deeply researched in

00:59:05.860 --> 00:59:12.260
arcane parts of mathematics and i check it and i mean i use claude to check the gpt5 and it basically

00:59:12.280 --> 00:59:14.300
is correct from as far as I can tell.

00:59:14.900 --> 00:59:18.400
So I think the thing to go to these people with is like,

00:59:18.560 --> 00:59:19.820
if you're not getting anything out of it,

00:59:19.820 --> 00:59:21.760
it's because you're not squeezing hard enough, right?

00:59:21.860 --> 00:59:24.000
Approach it as if it were a super intelligence

00:59:24.560 --> 00:59:26.800
and see how little it disappoints you

00:59:27.200 --> 00:59:28.720
because it will not disappoint you that often

00:59:28.760 --> 00:59:29.860
if you really get into it.

00:59:30.500 --> 00:59:32.760
Yeah, I want to take a slightly different take,

00:59:32.760 --> 00:59:33.740
but I 100% agree.

00:59:35.320 --> 00:59:36.960
I think you should treat it a little bit

00:59:37.080 --> 00:59:40.700
like a junior developer who knows 80% of what you want,

00:59:40.820 --> 00:59:43.060
but they just kind of gas into that last 20%.

00:59:45.839 --> 00:59:48.460
And if you gave this work to a junior dev

00:59:48.960 --> 00:59:50.680
and they got it 95% wrong

00:59:50.800 --> 00:59:52.060
and there's a little mistake

00:59:52.240 --> 00:59:52.900
and you had to go and say,

00:59:53.020 --> 00:59:53.980
hey, really good,

00:59:54.140 --> 00:59:55.460
but this part you got to fix up a little bit,

00:59:55.900 --> 00:59:57.780
that would be a success for that junior developer.

00:59:58.440 --> 01:00:00.920
I don't know why we expect 100% perfection

01:00:01.060 --> 01:00:02.560
if there's any kind of flaw whatsoever

01:00:03.480 --> 01:00:05.460
from such a creation process

01:00:06.560 --> 01:00:07.820
that like, well, it's broken, it's junk.

01:00:08.260 --> 01:00:10.780
Like you're expected to make a few mistakes

01:00:11.020 --> 01:00:12.180
and you've got to be there to guide it.

01:00:12.190 --> 01:00:15.340
But the huge amount it gets right is so valuable.

01:00:15.640 --> 01:00:18.500
This doesn't negate the standard software development lifecycle process

01:00:18.590 --> 01:00:19.260
of having code review.

01:00:19.960 --> 01:00:22.100
You still need to have those kinds of things in place.

01:00:22.280 --> 01:00:26.460
And the code review is you with your junior developer who's an LLM now.

01:00:27.090 --> 01:00:28.980
Well, yeah, the SCLC isn't negated.

01:00:29.210 --> 01:00:31.980
But the thing I think that's deeply counterintuitive is we're used to,

01:00:32.320 --> 01:00:33.080
I mean, the modality.

01:00:33.170 --> 01:00:35.420
Think about how this manifests.

01:00:35.640 --> 01:00:38.140
We're typing things still into a text window, right?

01:00:38.680 --> 01:00:44.420
And so we as developers are used to that being a very precise, predictable input, output transformational process.

01:00:45.500 --> 01:00:49.280
We're not used to the idea of coding with a semantic paintbrush, right?

01:00:49.420 --> 01:00:54.900
Like a Chinese or Japanese calligrapher doesn't care exactly which horsehair got ink on which part of the paper.

01:00:55.340 --> 01:00:57.060
They got a brush and they're like doing their calligraphy.

01:00:57.660 --> 01:01:05.340
And I think we have to get over ourselves and think about I'm painting with a semantic paintbrush, splattering it, certainly using my fingers with keyboard.

01:01:05.460 --> 01:01:07.040
But soon it'll be dictation, right?

01:01:07.500 --> 01:01:13.860
And so we're really splattering ideas into this canvas, and it's auto-rendering this stuff for us into a formal system.

01:01:14.520 --> 01:01:22.460
And I think just the modality of, wow, you can see the clouds are going over the sun, and my color temperature changes in my video.

01:01:23.700 --> 01:01:24.800
Is the AI doing it?

01:01:25.020 --> 01:01:27.160
The AI is doing it because they're passionate about this, right?

01:01:28.480 --> 01:01:39.900
So no, but I think that's the key thing is we are used to this modality of fingers on keyboard textual input being an input to a formal system, not an informal probabilistic system, which is what these things are.

01:01:40.180 --> 01:01:44.840
So once you make that mental bit flip, then then it's like you just learn to embrace it. Right.

01:01:45.560 --> 01:01:47.260
Yeah, I think voice is a great option here.

01:01:48.240 --> 01:01:51.680
We use Fireflies for our meeting recording bot.

01:01:52.140 --> 01:01:55.660
But you can also just open up your phone and launch the Fireflies app and start talking to it.

01:01:55.960 --> 01:01:56.980
And it has an MCP server.

01:01:57.820 --> 01:01:59.340
You can go into Claude Code and be like,

01:01:59.500 --> 01:02:03.800
grab the last transcript where I was just talking about this and pull it in,

01:02:04.100 --> 01:02:06.060
or have a discussion about the specifications,

01:02:06.440 --> 01:02:08.020
about the journey, the epic,

01:02:08.190 --> 01:02:09.900
the customer's story,

01:02:10.360 --> 01:02:13.280
and bring those in as artifacts really, really quickly now.

01:02:14.900 --> 01:02:18.500
Yeah. Holder ballgame.

01:02:19.060 --> 01:02:20.360
It is a crazy ballgame.

01:02:20.550 --> 01:02:21.180
The whole new ballgame.

01:02:21.700 --> 01:02:23.600
Yeah. All right.

01:02:23.700 --> 01:02:27.360
Anything else that is burning on your list of topics

01:02:27.460 --> 01:02:29.420
that we should do a lightning round

01:02:29.480 --> 01:02:30.300
'cause we're out of time on.

01:02:30.340 --> 01:02:31.700
- We should lightning round on DuckDB.

01:02:32.300 --> 01:02:33.600
I think I agree.

01:02:33.600 --> 01:02:36.260
- Okay, you two riffed on it 'cause I'm knowledgeable,

01:02:36.600 --> 01:02:38.080
but you all are the ones who use it.

01:02:38.560 --> 01:02:39.660
- If you've not played with it,

01:02:39.740 --> 01:02:43.500
it is an incredible little embedded,

01:02:44.200 --> 01:02:46.380
like kind of SQL lite, but way more.

01:02:47.200 --> 01:02:48.900
And if you've got files on a disk someplace,

01:02:49.400 --> 01:02:50.260
they're now your database.

01:02:51.000 --> 01:02:53.260
If you've got stuff in an S3 bucket someplace,

01:02:53.680 --> 01:02:58.320
that's now your database like it it's incredibly flexible it's got so many like cool extensions

01:02:58.580 --> 01:03:03.480
built into it like it can do geospatial stuff it's got json capabilities that are like really

01:03:03.620 --> 01:03:08.180
incredible i mean the speed is a little bit mind-blowing it's kind of like the first time

01:03:08.180 --> 01:03:14.600
you use uv or rough like how is that so fast and then you use duckdb and it's it's really i i think

01:03:14.700 --> 01:03:18.680
folks should go check it out and learn a little more because it's it may change how you think

01:03:18.600 --> 01:03:25.060
think about deploying an at edge thing or a little local thing or even a big data analysis piece,

01:03:25.330 --> 01:03:29.900
you may actually be able to fit that into memory on your machine and DuckDB and get some incredible

01:03:30.080 --> 01:03:35.700
results out of it. I'm sure Peter has way more to talk about this than I do, but I don't use it that

01:03:35.860 --> 01:03:41.480
much. But man, if I had a use case for it, I would be 100% picking that tool up. Yeah, DuckDB is a

01:03:41.580 --> 01:03:47.359
fantastic little piece of technology. I don't mean little in a pejorative sense here, but

01:03:47.320 --> 01:03:54.600
But at a technical level, I would say it is a highly portable, very efficient, and very versatile database engine.

01:03:55.620 --> 01:04:00.560
So the name is almost wrong because it exactly liberates you from databases.

01:04:00.980 --> 01:04:09.920
We are used to thinking of databases at places where data goes to, well, not die, but to be housed at rest and have an extreme amount of gravity attracted to it.

01:04:10.040 --> 01:04:12.480
And then DuckDB takes the opposite of that.

01:04:12.480 --> 01:04:21.040
It says any data representation you have should be searchable or queryable if only you had the right engine.

01:04:21.880 --> 01:04:26.680
And it inverts the whole thing, which is the brilliant piece of it.

01:04:28.020 --> 01:04:31.920
And again, what data isn't just representation.

01:04:32.080 --> 01:04:34.660
It's someone on a disk or over a network or a network.

01:04:35.740 --> 01:04:38.620
So it pairs very nicely with the PyData stack of tools.

01:04:39.740 --> 01:04:43.040
And so I know one of the topics we had on here as well was Arrow.

01:04:43.470 --> 01:04:47.340
So if you care about representation for a variety of reasons, then Arrow is great.

01:04:47.840 --> 01:04:57.620
If you want a query interface, you want a SQL-style query interface that's agnostic as to representation, that's your DuckDB.

01:04:58.190 --> 01:05:06.620
And, of course, the fact that it plays so well with WebAssembly means Edge, Cloudflare workers or whatever, or PyScript and WebAssembly workers.

01:05:07.260 --> 01:05:09.760
We have some demonstration examples using PyScript

01:05:09.980 --> 01:05:11.700
where you have an entire analytics stack

01:05:12.030 --> 01:05:13.340
running entirely within the browser,

01:05:14.040 --> 01:05:17.000
full on, you got pandas and psychic image,

01:05:17.160 --> 01:05:19.120
scikit-learn, map plotlib stuff going on.

01:05:19.530 --> 01:05:21.040
And you've got, you're hitting S3 buckets

01:05:22.520 --> 01:05:25.040
with full blown SQL queries using DuckDB

01:05:25.560 --> 01:05:26.880
because it all runs in WebAssembly.

01:05:27.680 --> 01:05:29.160
And this is just a taste.

01:05:29.190 --> 01:05:30.440
I mean, none of this is mainstream yet.

01:05:30.630 --> 01:05:32.600
I think some of these use cases are a little bit on the edge,

01:05:33.100 --> 01:05:35.600
but the vision this takes us to as a world

01:05:35.620 --> 01:05:41.680
where we really are just we're living a much more portable world so your things can just move you

01:05:41.680 --> 01:05:45.880
can give someone a web page a static web page it's a full-blown app and actually if you look at web

01:05:46.080 --> 01:05:52.260
gpu and transformers js web lm kinds of stuff you can fit a little tiny model in there actually and

01:05:52.280 --> 01:05:59.240
you have a totally local entirely client-side experience uh with with ai in it so yeah i'm

01:05:59.320 --> 01:06:03.480
very excited about this and ducktp is really part of that equation yeah bring your query engine to

01:06:03.380 --> 01:06:04.100
where your data is.

01:06:04.560 --> 01:06:04.660
Exactly.

01:06:04.860 --> 01:06:06.820
As opposed to the other way around, which always takes time.

01:06:08.880 --> 01:06:09.020
Yeah.

01:06:09.620 --> 01:06:09.720
Excellent.

01:06:10.780 --> 01:06:12.200
I know people are very excited about it.

01:06:12.340 --> 01:06:16.000
It's got the built into your program.

01:06:16.080 --> 01:06:19.780
You don't have to run another server aspect, which I think is good as well.

01:06:19.960 --> 01:06:21.280
And the WebAssembly stuff.

01:06:23.820 --> 01:06:29.980
Maybe there won't be local DB and local SQL or WebSQL, all those things that we just do

01:06:30.040 --> 01:06:32.120
DuckDB in the browser with WebAssembly.

01:06:33.680 --> 01:06:33.980
be nice

01:06:35.660 --> 01:06:35.760
so

01:06:37.330 --> 01:06:37.960
very interesting

01:06:39.140 --> 01:06:40.740
we barely scratched the surface

01:06:40.890 --> 01:06:42.880
you guys like there's a more there's more people

01:06:43.060 --> 01:06:44.840
need to know but I think these are probably

01:06:44.910 --> 01:06:46.440
some of the hotter topics

01:06:47.360 --> 01:06:48.700
we may have to do a part two

01:06:48.920 --> 01:06:50.860
but a 2026 edition that's just

01:06:50.940 --> 01:06:52.080
a continuation but

01:06:53.020 --> 01:06:54.360
if people take the time

01:06:54.980 --> 01:06:56.760
invest in putting some energy into these

01:06:56.940 --> 01:06:59.020
things it's going to make a big difference I think

01:07:01.559 --> 01:07:02.500
thanks for being on the show

01:07:04.860 --> 01:07:05.100
and

01:07:05.580 --> 01:07:06.760
yeah it's been great

01:07:07.420 --> 01:07:08.920
yeah this was awesome thank you so much for having us

01:07:09.320 --> 01:07:11.060
yeah thanks Michael I enjoy talking about all the

01:07:11.120 --> 01:07:11.860
cool new tech and tools

01:07:12.660 --> 01:07:13.160
bye guys

