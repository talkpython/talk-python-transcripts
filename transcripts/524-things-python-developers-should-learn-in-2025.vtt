WEBVTT

00:00:00.020 --> 00:00:01.880
Python in 2025 is different.

00:00:02.400 --> 00:00:04.460
Threads are really about to run in parallel.

00:00:05.100 --> 00:00:09.320
Installs, finish before your coffee cools, and containers are the default.

00:00:10.040 --> 00:00:14.000
In this episode, we count down 38 things to learn this year.

00:00:14.560 --> 00:00:25.180
Free-threaded CPython, uv for packaging, Docker and Compose, Kubernetes with Tilt, DuckDB and Arrow, PyScript at the Edge, plus MCP for sane AI workflows.

00:00:25.660 --> 00:00:28.260
Expect practical wins and migration paths.

00:00:28.560 --> 00:00:31.300
No buzzword bingo, just what pays off in real apps.

00:00:31.770 --> 00:00:37.000
Join me, along with Peter Wang and Calvin Hendrix Parker, for a fun, fast-moving conversation.

00:00:37.740 --> 00:00:43.520
This is Talk Python To Me, episode 524, recorded September 22nd, 2025.

00:01:01.280 --> 00:01:05.440
Welcome to Talk Python To Me, the number one podcast for Python developers and data scientists.

00:01:06.080 --> 00:01:11.060
This is your host, Michael Kennedy. I'm a PSF fellow who's been coding for over 25 years.

00:01:11.700 --> 00:01:12.880
Let's connect on social media.

00:01:13.300 --> 00:01:16.540
You'll find me and Talk Python on Mastodon, Bluesky, and X.

00:01:16.870 --> 00:01:18.620
The social links are all in the show notes.

00:01:19.220 --> 00:01:23.320
You can find over 10 years of past episodes at talkpython.fm.

00:01:23.800 --> 00:01:27.060
And if you want to be part of the show, you can join our recording live streams.

00:01:27.620 --> 00:01:27.960
That's right.

00:01:28.120 --> 00:01:31.700
We live stream the raw, uncut version of each episode on YouTube.

00:01:32.340 --> 00:01:36.800
Just visit talkpython.fm/youtube to see the schedule of upcoming events.

00:01:37.300 --> 00:01:41.660
and be sure to subscribe and press the bell so you'll get notified anytime we're recording.

00:01:42.820 --> 00:01:53.700
This episode is brought to you by Sentry. Don't let those errors go unnoticed. Use Sentry like we do here at Talk Python. Sign up at talkpython.fm/sentry. And it's brought to you by Agency.

00:01:54.280 --> 00:02:14.520
Discover agentic AI with Agency. Their layer lets agents find, connect, and work together, any stack, anywhere. Start building the internet of agents at talkpython.fm/agency, spelled A-G-N-T-C-Y. Hello, hello, Peter and Calvin. Welcome back to Talk Python To Me, to both of you. It's great to be here. Great to be here. Thanks for having us.

00:02:14.980 --> 00:02:33.580
Yeah, I know you both are very passionate technologists and Pythonistas, and we're going to dive into some really exciting things. What do people need to know as developers and data scientists in 2025? And I'm going to take a wild guess and bet that these trends, most of them carry over to 2026.

00:02:34.050 --> 00:02:34.880
You know, we're just a few months.

00:02:36.580 --> 00:02:43.580
So let's just really quickly have both of you introduce yourselves just because not everyone has had a chance to listen to every episode.

00:02:43.730 --> 00:02:45.920
And even if they did, they may not remember.

00:02:46.400 --> 00:02:47.620
So Peter, welcome.

00:02:47.670 --> 00:02:48.040
Who are you?

00:02:48.360 --> 00:02:49.400
Hi, I'm Peter Wang.

00:02:49.660 --> 00:02:54.020
I'm a founder of Anaconda and the creator of the PyData community.

00:02:54.440 --> 00:03:35.600
And I'm sort of leading the advocacy, at least, and been at the center of evangelism for the use of Python in the data science and machine learning world for over 12 years now. I think 13 years at this point. But my day job is at Anaconda. I'm the chief AI officer. So I work on open source community projects, innovation, looking at AI things and how that impacts our community and our users and what good could look like there for us. I mean, there's a lot of discussion on AI, of course, good, bad, and ugly. And I'm really trying to figure out if we as responsible open source community stewards, you know, want to have something meaningful to say here, what are the right things to do? So that's what I spend a lot of my time focused on. Yeah, that's really good

00:03:35.740 --> 00:03:50.940
work. Yeah, it's really good work. And congrats with all the access you've had at Anaconda. It's thank you made a serious dent. I you were on you were featured in or you were part of the Python documentary, right? That's right. Yeah, that was really great. I really appreciated your words in

00:03:51.020 --> 00:03:54.620
there. Thank you. Thank you. Yeah, that was great. Really honor to be included in that.

00:03:55.020 --> 00:04:00.560
Well, tell people, I haven't technically talked about it on the documentary or the documentary on the podcast very much.

00:04:00.740 --> 00:04:04.080
So you just give people a quick rundown on what that is and why they should check it out.

00:04:04.480 --> 00:04:12.480
Well, well, anyone who's listening to this podcast should absolutely watch the documentary because it is just got a cast of characters telling the story about how our favorite programming language came to be.

00:04:13.290 --> 00:04:14.400
All of the not all.

00:04:14.560 --> 00:04:15.100
OK, not all.

00:04:15.200 --> 00:04:23.240
But some of the travails that have challenged us as a community over over the period of time since since its inception, you know, 30 years ago at this point.

00:04:24.160 --> 00:04:29.400
And so it's just a really fun, nice, you know, I think it's weird because Python has been around forever.

00:04:29.820 --> 00:04:33.800
Right. And yet in many respects, we are still the world is changing.

00:04:33.980 --> 00:04:36.840
And I think there's lots of amazing new opportunities for Python as a language.

00:04:37.600 --> 00:04:43.080
And we've been growing, growing so fast and so much and evolving as a language and as a community.

00:04:44.000 --> 00:04:49.140
This documentary, I think, is a nice way to sort of like check in and say, oh, wow, we got to here.

00:04:49.420 --> 00:04:50.600
And here's the journey we've been on.

00:04:50.960 --> 00:04:57.860
And that gives us almost the space to then be a little bit more intentional about talking about where we want to go from here, which I think is something very important that we need to do as a community.

00:04:58.120 --> 00:05:02.000
So anyway, I just really liked it from philosophically speaking from that perspective.

00:05:02.700 --> 00:05:08.940
But it's also just fun just to get the perspectives of like the CPython core maintainers and the BDFL and all the stuff on just the language over the years.

00:05:09.360 --> 00:05:10.720
Yeah, I thought it was really excellent.

00:05:11.120 --> 00:05:13.720
Yeah, I enjoyed it tremendously.

00:05:14.310 --> 00:05:17.280
Like I really loved hearing all the old stories.

00:05:17.700 --> 00:05:20.580
I've been around for a long time in the community and seen all the familiar faces.

00:05:20.860 --> 00:05:25.200
And I feel like it gives a face and a level of empathy to the community that's needed.

00:05:25.480 --> 00:05:25.720
Yeah.

00:05:25.860 --> 00:05:30.100
I would say that the production quality was almost as good as Calvin's camera here.

00:05:31.980 --> 00:05:33.940
You always look great on these streams.

00:05:35.360 --> 00:05:35.580
Welcome.

00:05:35.800 --> 00:05:36.400
Tell people about yourself.

00:05:36.740 --> 00:05:37.240
Thank you, Michael.

00:05:37.580 --> 00:05:38.160
I appreciate that.

00:05:40.580 --> 00:05:42.160
Well, I guess I can give a quick introduction.

00:05:43.060 --> 00:05:44.020
I'm Calvin Hendryx-Parker.

00:05:44.160 --> 00:05:45.900
I'm CTO and co-founder of Six Feet Up.

00:05:46.000 --> 00:05:51.960
We are a Python and AI consulting agency who helps impactful tech leaders solve the hard problems.

00:05:52.300 --> 00:05:54.280
I've been in the Python community for ages.

00:05:55.360 --> 00:06:00.180
I probably don't outnumber Peter in years, but at least since 2000, I've been involved.

00:06:00.250 --> 00:06:05.540
I started with Zope and then through that, the Plone community got very involved in the governance open source project.

00:06:06.040 --> 00:06:13.100
Now we do a lot of Django, a lot of other Python open source data projects like Airflow, for example.

00:06:13.130 --> 00:06:14.340
I think that's on the list for later.

00:06:14.700 --> 00:06:20.320
And so we just enjoy hanging out and being an awesome group of folks who love solving the hard problems.

00:06:20.660 --> 00:06:21.360
Yeah, excellent.

00:06:21.580 --> 00:06:23.420
You've been doing it longer than me for sure.

00:06:23.740 --> 00:06:24.260
I'm the baby.

00:06:25.560 --> 00:06:28.080
Well, 2000 is about when I got involved in Python as well.

00:06:28.300 --> 00:06:32.240
So the old manual was supposed to be maybe from 99, but basically 2000.

00:06:32.870 --> 00:06:37.220
Yeah, my first Python was 2003, and I think there were 250 people in the room.

00:06:38.380 --> 00:06:39.000
It was amazing.

00:06:39.110 --> 00:06:40.360
Yeah, you actually beat me by a couple of years.

00:06:40.440 --> 00:06:45.200
I went to, I went to 05 was my first one at George Washington University.

00:06:45.440 --> 00:06:45.960
I think it was.

00:06:46.260 --> 00:06:46.440
Yeah.

00:06:46.720 --> 00:06:47.020
In DC.

00:06:47.520 --> 00:06:48.420
And it was about 200 something people.

00:06:48.800 --> 00:06:51.180
They had a track in the keynote speakers.

00:06:51.420 --> 00:06:51.700
Wow.

00:06:52.880 --> 00:06:54.760
I've only been doing this since 2011.

00:06:55.100 --> 00:06:56.560
So I'm just barely getting started.

00:06:57.120 --> 00:06:59.640
That used to seem pretty recent ago, but it doesn't anymore.

00:06:59.860 --> 00:07:00.100
Oddly.

00:07:00.260 --> 00:07:02.760
No, it turns out it was, yeah, it's a long time ago.

00:07:02.900 --> 00:07:04.520
We're halfway through the 2020s now.

00:07:04.640 --> 00:07:05.000
It's crazy.

00:07:05.400 --> 00:07:05.520
I know.

00:07:05.620 --> 00:07:05.760
Yeah.

00:07:05.980 --> 00:07:06.040
Yeah.

00:07:06.040 --> 00:07:11.760
When you said 2025 things that developers should learn in 2025, I was like, is this a science fiction movie we're talking about.

00:07:12.420 --> 00:07:12.600
Exactly.

00:07:12.660 --> 00:07:13.340
What is this like then?

00:07:13.500 --> 00:07:14.760
It's a dystopian science fiction movie.

00:07:14.880 --> 00:07:16.620
It's the same crap we had to deal with in 2010.

00:07:18.020 --> 00:07:18.380
Mostly.

00:07:19.280 --> 00:07:20.960
Although async back then, it was interesting.

00:07:21.170 --> 00:07:23.340
We didn't have, you know, we had staff list, I guess.

00:07:24.020 --> 00:07:25.320
There's a, I don't know.

00:07:26.040 --> 00:07:26.400
2010.

00:07:26.740 --> 00:07:26.980
There's tornado.

00:07:27.290 --> 00:07:27.380
Yeah.

00:07:27.520 --> 00:07:29.300
There were various async systems.

00:07:29.660 --> 00:07:30.300
Anyway, salary.

00:07:30.580 --> 00:07:30.660
Yeah.

00:07:31.000 --> 00:07:31.180
Wow.

00:07:31.420 --> 00:07:32.960
We've got, we've got free threaded Python.

00:07:33.180 --> 00:07:33.940
Now we do.

00:07:34.440 --> 00:07:35.000
Futures now.

00:07:35.460 --> 00:07:35.820
Yes.

00:07:36.240 --> 00:07:36.560
Almost.

00:07:36.870 --> 00:07:37.980
We almost have free threaded Python.

00:07:38.230 --> 00:07:38.320
Yeah.

00:07:38.480 --> 00:07:38.560
Yeah.

00:07:38.890 --> 00:07:39.020
Yeah.

00:07:39.080 --> 00:07:39.700
Spoiler alert.

00:07:39.840 --> 00:07:41.580
That may make an appearance in one of the topics.

00:07:42.360 --> 00:07:48.660
Well, we may not get to 20 things, but they may not be 20 big, bold items, right?

00:07:49.480 --> 00:07:49.560
Yeah.

00:07:49.700 --> 00:07:51.660
We have a list of things we want to go through.

00:07:51.860 --> 00:07:52.300
That's right.

00:07:52.390 --> 00:07:57.400
Peter, we reserve the right to design the designation of the size of the buckets that define the things.

00:07:57.620 --> 00:07:58.460
The things, that's right.

00:07:59.600 --> 00:08:08.440
But I think the plan is we're going to just riff on some ideas we think are either emerging or current important trends or even foundational things.

00:08:08.540 --> 00:08:13.400
that people should be paying attention to in the zeitgeist right now, right?

00:08:13.560 --> 00:08:19.760
What are things that maybe you haven't necessarily been tracking or you heard of, but you're like, ah, I haven't got time for that, or it's not for me yet.

00:08:20.460 --> 00:08:21.380
So I think that'll be fun.

00:08:22.320 --> 00:08:23.400
Let's start with you, Peter.

00:08:24.039 --> 00:08:25.140
What's your first...

00:08:25.260 --> 00:08:29.120
We all gathered up a couple of things that we think might be super relevant.

00:08:29.520 --> 00:08:30.480
And yeah, what do you think?

00:08:31.760 --> 00:08:33.500
So I think, well, let's just get started with it.

00:08:33.580 --> 00:08:35.020
Let's just talk about the free threading bit.

00:08:35.280 --> 00:08:40.840
And let's really, because this is a kind of, it touches the past, and it also really takes us into the future.

00:08:41.110 --> 00:08:43.760
And it's this thing that has taken quite some time to emerge.

00:08:44.240 --> 00:08:47.980
I think the GIL has been a topic of discussion since as long as I've been using Python.

00:08:49.240 --> 00:08:58.300
And finally, we have, courtesy of the team at Meta, an excellent set of patches that delivered true free threading to Python.

00:08:58.800 --> 00:09:00.880
And of course, this is both a blessing and a curse, right?

00:09:00.880 --> 00:09:01.960
You should be careful what you ask for.

00:09:02.210 --> 00:09:04.760
Because now we end up having to deal with true free threading in Python.

00:09:05.300 --> 00:09:13.540
And for those who maybe are not so familiar with this whole topic, the global interpreter lock, we call it GIL, G-I-L for short.

00:09:14.280 --> 00:09:18.620
The global interpreter lock is how the Python virtual machine protects its innards.

00:09:18.820 --> 00:09:30.500
And so when you use Python and you write code, even if you use threading, like the threading module in Python, ultimately the CPython interpreter itself as a system level process, it only has one real thread.

00:09:30.880 --> 00:09:34.660
And it has this global interpreter lock that locks many of the internals of the interpreter.

00:09:35.000 --> 00:09:38.020
The problem is that sometimes you want to have real multi-threading.

00:09:38.450 --> 00:09:44.020
And so you have to release this global interpreter lock and doing this, doing this is hard to

00:09:44.160 --> 00:09:47.100
get right, especially if you reach into C modules and whatnot.

00:09:47.780 --> 00:09:50.560
The most popular C modules are pretty good at handling this kind of thing.

00:09:51.200 --> 00:09:52.520
NumPy and others come to mind.

00:09:52.810 --> 00:09:55.720
So we get really great performance from those when they release the gil.

00:09:56.020 --> 00:10:50.680
But if you want to actually do a lot of Python logic in multiple threads, you end up essentially getting no lift whatsoever by using a threading module with classic single threaded or GIL locked python with the free threading you actually now are able to have threads running in parallel touching things like free lists and stuff like that and and and you know module definitions in the interpreter itself now what this means is a lot of python modules or packages which had been developed when Python was, you know, implicitly single threaded, they now have the potential of thread contention, race conditions, all sorts of weird and unexpected behavior when they're used in a free threaded way. So we have this patch, we have this change now for free threading in the Python interpreter itself. That means that however, what that means is we have to make sure that all of the rest of the package ecosystem is updated and tested to work with free threaded Python.

00:10:51.180 --> 00:10:56.020
So in Python 3.13, it was incorporated as an experimental.

00:10:56.200 --> 00:10:59.220
It was in the code base, but it was a build time feature.

00:10:59.380 --> 00:11:05.140
So you have to compile your own Python interpreter and turn on that flag to get a version of the interpreter that would be free-threaded.

00:11:05.360 --> 00:11:08.780
In 3.14, it is now supported in the interpreter.

00:11:09.440 --> 00:11:11.000
It's still not turned on by default.

00:11:11.600 --> 00:11:14.940
And then at some indeterminate date, it will be turned on by default.

00:11:16.020 --> 00:11:23.320
The classic behavior with the global interblock will still always be there as a fallback for safety and compatibility and all that.

00:11:23.660 --> 00:11:32.520
But anyway, we're right now at the point where the core CPython team has said, hey, we're ready to take this thing to supported mode and let the bugs flow, right?

00:11:32.760 --> 00:11:36.440
So now if you go and install Python,

00:11:36.650 --> 00:11:39.180
a Python build with, it actually has a different ABI tag.

00:11:39.470 --> 00:11:45.420
So it's CP313 or 314T for threading or free threading.

00:11:45.740 --> 00:12:16.800
um so that's available through you know python.org there's a condo bill for it as well and um so right now there's actually a page maybe we'll have the link for it i think in the in the show notes right but there's a page that lists what the status is think of the free threaded wheels um and uh right now 105 out of 360 that are passing basically having the maintainers have updated them um and this is out of the top like oh there it is great yeah out of the top 500 um Python packages, something like this.

00:12:17.200 --> 00:12:19.460
So you can see we have, as a community, a lot of work to do.

00:12:19.980 --> 00:12:26.860
So the call to action here is not only should a Python developer learn this, because this is definitely coming and everyone has a multi-core machine now.

00:12:27.860 --> 00:12:28.640
So this is definitely coming.

00:12:29.020 --> 00:12:30.980
But you can also, this is a great way to give back.

00:12:31.620 --> 00:12:37.340
You know, we talk about in the open source community oftentimes, how do we get starter bugs in there for people to start becoming contributing members of the community?

00:12:37.680 --> 00:12:38.680
This is a great way to give back.

00:12:38.840 --> 00:12:42.580
If there's some packages you see here that are yellow, you're like, wait, I use AIoHttp.

00:12:42.980 --> 00:12:50.500
Like, let me go and test that with free threading and see if I can bang, you know, just beat up my beat up with my code in production and see like what what fails there.

00:12:50.750 --> 00:12:58.020
So these are this is a great way for the community to really get back and help us test and make sure all this works on what is certainly to be the next generation of the Python interpreter.

00:12:58.260 --> 00:13:03.280
Yeah, there was a great talk at DjangoCon just two weeks ago by Michael Lyle.

00:13:03.580 --> 00:13:06.480
He gave a talk about using free threaded, free threading in Django.

00:13:07.110 --> 00:13:10.220
And I think right now your mileage may vary was the answer.

00:13:10.440 --> 00:13:11.600
Like it kind of depends.

00:13:12.700 --> 00:13:15.780
I can only imagine going through and trying to commit and help.

00:13:16.080 --> 00:13:16.860
Threading is hard.

00:13:17.060 --> 00:13:19.900
It sounds like free threading is harder to wrap your brain around.

00:13:20.200 --> 00:13:23.380
So I think it'd be tricky for someone starting and learning something new.

00:13:23.460 --> 00:13:28.480
This may be on the more advanced edge of what someone should be learning.

00:13:28.840 --> 00:13:32.520
It's more for the advanced crotchety, you know, senior developers.

00:13:32.760 --> 00:13:33.980
I ain't got time to contribute to open source.

00:13:34.400 --> 00:13:34.940
You can.

00:13:35.180 --> 00:13:36.180
You can make your own life better.

00:13:36.620 --> 00:13:39.840
We can all sort of, this is the sort of stone soup or good old Amish barn raising.

00:13:39.900 --> 00:13:41.520
We should all get together and chip in.

00:13:41.980 --> 00:13:42.440
But you're right.

00:13:42.700 --> 00:13:46.860
Debugging async free threading issues is definitely not a beginner kind of task.

00:13:47.440 --> 00:13:47.500
Sure.

00:13:47.580 --> 00:13:54.560
But there's a lot of people who do have that experience from probably more from other languages or C extensions who could jump in, right?

00:13:54.860 --> 00:13:54.960
Yeah.

00:13:55.140 --> 00:14:11.260
Actually, if you're a C++ developer who has been forced to use Python because of our success of driving the growth and adoption of the community, and you're really angry about this and you want to show other ways that Python is broken, this is a great way to show how Python has broken is to test really gnarly async and multi-threaded use cases.

00:14:11.800 --> 00:14:21.760
Actually, one thing about this that I will point out for the more advanced users, Dave Beasley gave a great talk years ago at PyCon about Python parallelism and are you IO bound?

00:14:21.860 --> 00:14:22.740
Are you CPU bound?

00:14:23.100 --> 00:14:23.420
You know what?

00:14:23.540 --> 00:14:33.660
I think he was looking at maybe it was actually relative to PyPy, P-Y-P-Y, and it wasn't about async in particular, but it was a rolling your own distributed computing or something like this.

00:14:33.760 --> 00:14:34.620
I forget the exact title.

00:14:35.140 --> 00:14:38.860
but he did a deep analysis of when are we CPU bound or when are we IO bound?

00:14:39.320 --> 00:14:40.360
And when are we CPU bound?

00:14:40.660 --> 00:14:46.700
When we get to free threading Python like this, I think we're going to, as a community, be faced with having to up-level our thinking about this kind of thing.

00:14:46.800 --> 00:14:51.880
Because so far we've done a lot of like, oh, delegating CPU bound numeric stuff to like Python or Pandas or Cython.

00:14:52.260 --> 00:14:55.420
But with this, now we can really play first class in system level code.

00:14:55.760 --> 00:14:58.320
And we have to think more deeply about how are we blocking events?

00:14:58.380 --> 00:14:59.160
How are we handling things?

00:14:59.520 --> 00:15:03.000
Is this, you know, is this a, you know, event polling kind of thing?

00:15:03.120 --> 00:15:05.180
or is this more of a completion port thing?

00:15:05.230 --> 00:15:06.260
Like in Windows, you have different options.

00:15:06.720 --> 00:15:08.200
So this is a very interesting topic, actually.

00:15:08.270 --> 00:15:08.960
It goes quite deep.

00:15:09.120 --> 00:15:09.820
It goes very deep.

00:15:10.020 --> 00:15:15.680
And I think it's going to be a big mental lift for people in the community, generally speaking.

00:15:16.260 --> 00:15:21.660
I talk to a lot of people, as you know from the podcast, and then also interact with a lot of people teaching.

00:15:21.940 --> 00:15:27.500
And I don't see a lot of people stressing about thread safety or any of those kinds of things these days.

00:15:27.960 --> 00:15:33.120
And I think in general, it's just not in the collective thinking to be really worried about it.

00:15:33.480 --> 00:15:36.240
There are still cases in multi-threaded Python code

00:15:36.640 --> 00:15:43.100
where you need to take a lock because it's not one line's gonna deadlock another or something like that, but you've gotta take five steps.

00:15:43.390 --> 00:15:46.240
And if you get interrupted somewhere in those five steps,

00:15:46.740 --> 00:15:48.500
the guilt could still theoretically interrupt you

00:15:48.510 --> 00:15:49.480
in the middle of code, right?

00:15:50.140 --> 00:15:54.360
It still could be in a temporarily invalid state across more than one line.

00:15:54.670 --> 00:15:58.480
But I just don't see people even doing anything hardly at it at all.

00:15:58.900 --> 00:16:00.580
And when we just uncork this on them,

00:16:01.440 --> 00:16:03.240
it's going to be, it's going to be something.

00:16:03.420 --> 00:16:05.960
And I don't think we're going to see deadlocks as a problem first.

00:16:06.480 --> 00:16:11.400
I think we're going to see race conditions because deadlocks require people already having locks there that get out of order.

00:16:11.600 --> 00:16:12.840
And I just think the locks are not there.

00:16:13.440 --> 00:16:15.960
Then people are going to put the locks there and they're like, Whoa, it's just stopped.

00:16:17.100 --> 00:16:17.980
It's total chaos.

00:16:19.399 --> 00:16:21.340
Yeah. It's not using CPU anymore. What is it doing?

00:16:21.440 --> 00:16:24.460
Well, now you found the deadlock. You, you, you added the deadlock, right?

00:16:25.000 --> 00:16:27.240
So it's going to be, it's going to be a challenge,

00:16:27.580 --> 00:16:33.080
but the potential on the other side of this, if you can get good at it, it's going to be amazing.

00:16:33.540 --> 00:16:36.800
You know, even on my little cheapo Mac mini, I've got 10 cores.

00:16:37.200 --> 00:16:43.800
If I run Python code, unless I do really fancy tricks or multiple processes, the best I can get is like 10%.

00:16:43.960 --> 00:16:55.260
Yeah, and I know this might be a little bit of a spicy take, but like there was, I think, a line that was being held by the CPython core team that we will accept a GIL removal or a gillectomy as it was called.

00:16:55.500 --> 00:17:01.720
We'll accept a GIL removal patch when it doesn't affect or negatively impact single core performance, right?

00:17:02.120 --> 00:17:08.520
And like when that first came out in 2000, I think that first time I heard that article was a 2005, six, seven timeframe.

00:17:08.959 --> 00:17:10.620
Back then that was almost a defensible position.

00:17:11.060 --> 00:17:13.860
Nowadays, you can't find a smartphone with a single core.

00:17:14.199 --> 00:17:17.780
You know, I can't find a Raspberry Pi, a $5 Raspberry Pi has dual core.

00:17:17.980 --> 00:17:26.800
So it's like, I get the general gist of that, but like, come on, we have like 90, like, you know, John Carmacks on Twitter talking about 96 core Threadripper performance with Python.

00:17:27.260 --> 00:17:29.180
We, you know, we sort of need to lean into that, right?

00:17:29.340 --> 00:17:35.380
So I'm really, really bullish on this because as you know, like I'm very close to the data science and machine learning and the AI use cases.

00:17:35.880 --> 00:17:39.600
And those are all just, you know, they're looking for whatever language gives us the best performance.

00:17:40.160 --> 00:17:41.000
Right now, it happens to be Python.

00:17:41.520 --> 00:17:48.200
If we as a community and we as evangelists of that community, if we don't lead into that and those users, they will happily go somewhere else.

00:17:48.590 --> 00:17:50.860
I mean, that is bonusing people $100 million to start.

00:17:51.180 --> 00:17:52.860
They're not going to wait for your language to catch up.

00:17:53.000 --> 00:17:53.740
They'll make a new language.

00:17:54.050 --> 00:17:54.140
Right.

00:17:54.460 --> 00:18:01.920
But I think there was something in 2025 that these developers should be learning along these lines would be just async programming and when it should be used.

00:18:02.260 --> 00:18:06.220
That's why the really tactical maneuver today.

00:18:06.660 --> 00:18:07.460
Yeah, I agree.

00:18:07.750 --> 00:18:11.420
I think the async and await keywords are super relevant.

00:18:11.640 --> 00:18:14.360
And the frameworks, I think, will start to take advantage of it.

00:18:14.380 --> 00:18:16.240
We're going to see what comes along with this free threading.

00:18:16.560 --> 00:18:21.660
But there's no reason you couldn't await a thread rather than await an IO operation.

00:18:21.980 --> 00:18:22.320
You know what I mean?

00:18:22.980 --> 00:18:29.640
My background is C++ and C#, and C# is actually where async and await came from, from Anders Halsberg, I believe.

00:18:30.080 --> 00:18:33.440
And over there, you don't care if it's IO or compute bound.

00:18:33.530 --> 00:18:35.880
You just await some kind of async thing.

00:18:35.960 --> 00:18:37.280
It's not your job to care how it happens.

00:18:38.270 --> 00:18:44.520
So I think we're going to start to see that, but it's going to take time for those foundational layers to build for us to build on.

00:18:44.860 --> 00:18:44.960
Yeah.

00:18:47.300 --> 00:18:50.580
This portion of Talk Python To Me is brought to you by Centuries Seer.

00:18:51.040 --> 00:18:54.080
I'm excited to share a new tool from Sentry, Seer.

00:18:54.620 --> 00:19:01.940
Seer is your AI-driven pair programmer that finds, diagnoses, and fixes code issues in your Python app faster than ever.

00:19:02.440 --> 00:19:06.120
If you're already using Sentry, you are already using Sentry, right?

00:19:06.640 --> 00:19:10.920
Then using Seer is as simple as enabling a feature on your already existing project.

00:19:11.720 --> 00:19:15.040
Seer taps into all the rich context Sentry has about an error.

00:19:15.540 --> 00:19:19.600
Stack traces, logs, commit history, performance data, essentially everything.

00:19:20.240 --> 00:19:24.340
Then it employs its agentic AI code capabilities to figure out what is wrong.

00:19:24.940 --> 00:19:28.720
It's like having a senior developer pair programming with you on bug fixes.

00:19:29.500 --> 00:19:34.720
Seer then proposes a solution, generating a patch for your code and even opening a GitHub pull request.

00:19:35.380 --> 00:19:40.000
This leaves the developers in charge because it's up to them to actually approve the PR.

00:19:40.440 --> 00:19:44.240
But it can reduce the time from error detection to fix dramatically.

00:19:45.000 --> 00:19:50.580
Developers who've tried it found it can fix errors in one shot that would have taken them hours to debug.

00:19:51.200 --> 00:19:55.600
SEER boasts a 94.5% accuracy in identifying root causes.

00:19:56.160 --> 00:20:02.860
SEER also prioritizes actionable issues with an actionability score, so you know what to fix first.

00:20:03.480 --> 00:20:10.380
This transforms sentry errors into actionable fixes, turning a pile of error reports into an ordered to-do list.

00:20:11.220 --> 00:20:20.200
If you could use an always-on-call AI agent to help track down errors and propose fixes before you even have time to read the notification, check out Sentry's Seer.

00:20:20.840 --> 00:20:24.820
Just visit talkpython.fm/Seer, S-E-E-R.

00:20:25.470 --> 00:20:27.300
The link is in your podcast player's show notes.

00:20:27.840 --> 00:20:30.000
Be sure to use our code, Talk Python.

00:20:30.600 --> 00:20:31.400
One word, all caps.

00:20:32.080 --> 00:20:34.080
Thank you to Sentry for supporting Talk Python To Me.

00:20:35.180 --> 00:21:27.580
Pamela Fox out in the audience throws out that last time she really used locks was in my code for operating system class in college. It doesn't come up much in web dev. That's true. A lot of the times the web, it's at the web framework, the web server level, the app server level, right? It's Granian or it's UVicorn or something like that. That thing does the threading and you just handle one of the requests. I literally just deadlocked and I guess probably broke the website for a couple of people at Talk Python today because I have this analytics operation that's fixing up a stuff and it it ran for like 60 seconds even though there's multiple workers something about the fan out it's still it still sent some of the traffic to the one that was bound up and then those things were timing out after 20 seconds i'm like oh no what have i done and if that was true threading it wouldn't have mattered it would have used up one of my eight cores and the rest would have been off jamming along it would have been fine you know well sort of right and i think this is i think it's

00:21:27.590 --> 00:21:38.800
i'm really glad pamela brought this up because um we do when we're focused on a particular just the worker thread, it's like, okay, what am I doing? Pull this, run that, and then push this thing out.

00:21:39.480 --> 00:21:53.400
But if you start getting to more, anytime you start having either value dependent or heterogeneous workload and time boundaries for these tasks, you start having to think about thread contention.

00:21:55.920 --> 00:22:00.500
To your point, Calvin, I think it's not so far that you have to go before you quickly find

00:22:00.520 --> 00:22:06.320
yourself thinking about things like grand central dispatch like iowa like mac os has or io completion

00:22:06.500 --> 00:22:29.360
ports and like oh crap i'm actually slamming it's not under certain cases you know to your point about the analytics maybe you're not doing a gpu-based analytics thing but maybe you're slamming a bunch of stuff to disk or loading a bunch of stuff up from disk and you start getting all these things where at some point one of these things the bottleneck is the cpu is it the you know the code itself is it the disk is the network um and you're just slamming your code into one of

00:22:29.320 --> 00:22:30.800
these different boundaries stochastically.

00:22:31.340 --> 00:22:40.880
And as a developer, maybe as an entry-level developer, you don't have to think about it too much, but as any kind of a mid to senior developer, you're going to be running into these problems and they are going to be stochastic.

00:22:41.020 --> 00:22:41.820
They are value dependent.

00:22:42.040 --> 00:22:49.120
You're going to hit them in production and you have to sort of know what could bite you, even if it's not biting you all the time in dev.

00:22:49.400 --> 00:22:51.420
Right, you remove one bottleneck

00:22:51.440 --> 00:22:53.160
and it starts to slam into a different part.

00:22:53.280 --> 00:22:56.660
Maybe you take them to the database and it's even a worse console.

00:22:56.700 --> 00:22:57.320
You never know, right?

00:22:57.380 --> 00:22:57.520
That's right.

00:22:57.760 --> 00:22:58.280
We're going to see.

00:22:58.360 --> 00:22:58.900
It's going to be interesting.

00:22:59.300 --> 00:23:10.720
But thinking about that in production, you've got new challenges there because you may have containers and you're running in Kubernetes and you've got pods and resource limits and other kinds of constraints that are happening that aren't on your local machine.

00:23:10.940 --> 00:23:12.360
All of a sudden you're saturating your local machine.

00:23:12.360 --> 00:23:13.120
You're like, this is great.

00:23:13.440 --> 00:23:14.480
I'm using all the resources.

00:23:14.740 --> 00:23:15.140
Look at it go.

00:23:15.480 --> 00:23:19.180
And now you release that to production and watch calamity and chaos.

00:23:19.860 --> 00:23:21.840
They get killed off because you've set some.

00:23:22.900 --> 00:23:34.160
My websites and APIs and databases all have production level RAM limits and things like that, so that if they go completely crazy, at least it's restricted to that one thing dying.

00:23:34.480 --> 00:23:34.660
Yeah.

00:23:34.790 --> 00:23:34.920
Everything.

00:23:35.900 --> 00:23:39.800
Speaking of which, have you got some ideas on what's next, Calvin?

00:23:40.160 --> 00:23:40.380
Sure.

00:23:40.920 --> 00:23:43.280
I've been a big believer in containers.

00:23:43.740 --> 00:23:48.120
I really got turned on to this in 2020 and went down the path.

00:23:48.120 --> 00:23:54.300
And now we're finally arrived where I believe developers should be learning Kubernetes, even for local development.

00:23:54.530 --> 00:23:59.080
I feel like that whole front to back story is not as complicated.

00:23:59.220 --> 00:24:01.720
The tooling has really come up to date.

00:24:01.980 --> 00:24:17.160
And so being able to use containers to get reliable, repeatable builds, being able to use tools like Tilt.dev, for example, as a developer locally with my Kubernetes clusters, I can now have file systems syncing, use all my local tools.

00:24:18.180 --> 00:24:21.760
This just literally does take the pains out of, they say microservice development.

00:24:22.100 --> 00:24:25.720
I think that's a little bit of a buzzwordy explanation there.

00:24:25.880 --> 00:24:28.240
I will say that it's good for Django development.

00:24:28.340 --> 00:24:31.160
So if you check out the SCAF full stack template,

00:24:31.690 --> 00:24:32.860
are you going to change it for me?

00:24:33.220 --> 00:24:34.600
Perfect, that's perfect.

00:24:35.620 --> 00:24:43.500
This is exactly where we can use the same tools in production that we use in development so that it's much easier to track down issues.

00:24:44.620 --> 00:24:46.440
Containers obviously unlocked a lot of those.

00:24:46.600 --> 00:24:53.640
I feel like the true superpower of Kubernetes, I think a lot of people love it for orchestration or claim it's for orchestration.

00:24:53.860 --> 00:25:00.940
I really love the fact that it's got a control plane and a URL and an API so you can do things like continuous deployment.

00:25:01.280 --> 00:25:12.620
So being able to deliver your code, build an image, update a manifest and have things just deploy without you having to think twice about it and be able to roll back with a click of a button and using tools like Argo CD.

00:25:13.220 --> 00:25:15.480
Argo CD is a great CI CD tool.

00:25:15.640 --> 00:25:16.820
So we leverage it very heavily.

00:25:16.960 --> 00:25:21.220
If you want a good example of how to do that, you can check out that same full stack template.

00:25:21.340 --> 00:25:26.600
We have all the pieces put in there for you in GitHub to understand how that works.

00:25:26.800 --> 00:25:28.920
So I think it's real.

00:25:29.080 --> 00:25:35.060
I think developers should be embracing the container world, especially if you have more than one developer.

00:25:35.200 --> 00:25:42.340
As soon as you have a second developer, this becomes an immediate payoff in the work it took to put it in place.

00:25:42.580 --> 00:25:46.820
And so I think it hits all the environments too, like not just web dev.

00:25:46.980 --> 00:25:49.640
I think the data folks benefit from containers,

00:25:50.120 --> 00:25:51.540
especially if you look at tools like Airflow,

00:25:51.920 --> 00:26:15.520
be able to deploy that into containers, being able to manage workers that are Kubernetes-based tasks so you can natively handle asynchronous tasks in a cluster and leverage all that power you've got under the covers and scalability of being able to scale out all the nodes, you get a lot of win for adopting a tool that I think a lot of people, and me included, used to consider overkill.

00:26:15.760 --> 00:26:15.920
Yeah.

00:26:16.620 --> 00:26:18.760
Well, let's put some layers on this.

00:26:19.220 --> 00:26:21.100
First of all, preach on, preach on.

00:26:21.880 --> 00:26:23.260
But you say containers.

00:26:23.920 --> 00:26:26.140
You said Kubernetes, these some other things.

00:26:26.760 --> 00:26:29.460
Do you have to know Docker and containers?

00:26:30.000 --> 00:26:31.960
Is Docker synonymous with containers for you?

00:26:31.960 --> 00:26:34.260
Do you have to know that before you're successful with Kubernetes?

00:26:36.380 --> 00:26:39.720
There's a couple of layers of architecture here.

00:26:40.519 --> 00:26:42.920
Where are you telling people they should pay attention to?

00:26:43.240 --> 00:26:44.780
I think you have to start with containers.

00:26:45.340 --> 00:26:45.800
Start with Docker.

00:26:46.280 --> 00:26:48.620
The dog wants me to play with the toy over here.

00:26:49.400 --> 00:26:50.600
If you start with the container,

00:26:50.780 --> 00:27:04.800
because you have to have a good container strategy, even to be able to build and work with containers inside of any kind of a, whether it's Docker Compose or Swarm or using Fargate or some kind of container app service, like on DigitalOcean.

00:27:05.680 --> 00:27:07.680
- Yeah, count me down as Docker Compose, by the way.

00:27:07.700 --> 00:27:08.460
I'm a big fan of this.

00:27:08.640 --> 00:27:09.400
- Yeah, you're a big fan.

00:27:09.960 --> 00:27:10.640
That's where we started.

00:27:10.900 --> 00:27:26.400
I really enjoyed the ability to have Compose describe my whole stack and be able to run the exact same version of the exact right version of Redis, the exact right version of Postgres, the exact right version of whatever my dependent other pieces are around me, because that matters.

00:27:27.540 --> 00:27:48.100
I don't remember folks remember the Reddit 608 to 609, like a very minor release introduced a new feature that was unusable in a very minor release backward. So you want to be able to pin these things down so you aren't chasing ghosts and weird edge cases and containers enable that.

00:27:48.320 --> 00:27:50.220
whether it's Compose or Kubernetes, it doesn't matter.

00:27:50.780 --> 00:27:52.100
You get that benefit.

00:27:52.700 --> 00:28:01.580
I feel like the Kubernetes piece just takes that to the next level and gives you a lot of the programmability of the web with an API and the fact that I'm not logging in.

00:28:02.760 --> 00:28:08.160
Our preferred way to deploy Kubernetes onto servers is actually to use Talos Linux, which has no SSH shell.

00:28:08.460 --> 00:28:10.640
There is not a way to shell into that box.

00:28:10.840 --> 00:28:16.500
There eliminates a whole class of security vulnerabilities because there is no shell on the box whatsoever.

00:28:17.160 --> 00:28:19.920
You have to interact with it programmatically via APIs.

00:28:20.600 --> 00:28:25.240
And even the upgrades happen via the same API backplanes.

00:28:25.740 --> 00:28:35.780
And just that level of control, security, reliability, and comfort helped me sleep really well at night knowing where I've deployed these things.

00:28:36.140 --> 00:28:37.520
But you do need containers first.

00:28:37.740 --> 00:28:41.620
I think if you don't understand the layers of the containers, I think that's a quick read.

00:28:42.060 --> 00:28:43.840
There's some really good resources online.

00:28:45.340 --> 00:28:48.040
Nana's tech world does a really good job of describing containers.

00:28:48.260 --> 00:28:49.280
She does a really good job.

00:28:49.500 --> 00:28:56.840
And she does an awesome job of bringing that down to an every person, most every person level who would even care to want to touch it.

00:28:57.060 --> 00:29:00.700
I have some thoughts about containers and components and stuff that I want to throw in.

00:29:00.700 --> 00:29:08.580
But I do especially want to hear, Peter, contrast your take with the, you sort of say the same thing, but for data scientists.

00:29:08.940 --> 00:29:11.880
Do you need to pay attention to containers and data science?

00:29:12.260 --> 00:29:12.680
Is that different?

00:29:12.980 --> 00:29:21.760
I interviewed Matthew Rockland from Coiled recently, and they've got a really interesting way to ship and produce your code without containers that you actually interact with.

00:29:22.380 --> 00:29:23.680
There's options, but what do you think?

00:29:23.790 --> 00:29:30.240
Yeah, I think containers are just part of the technical landscape now, so it's good to know them.

00:29:30.680 --> 00:29:38.860
I think if we were to remove the capabilities of data science from everyone who doesn't know about containers, that we would end up with a deeply impoverished user base.

00:29:39.920 --> 00:29:52.980
The truth of the matter is that there are a lot of people out there today who, if you think about what containers really do from a software development and a DevOps perspective, it is a mechanism for your dog knows about to say something spicy.

00:29:53.080 --> 00:29:54.140
No, I'm not trying to be controversial.

00:29:54.320 --> 00:30:08.500
Just thinking about it on first principles, a container is a way for us to sort of format and rationalize the target deployment environment within the boundaries of the box, within the boundaries of a particular compute node with an IP address or something like this.

00:30:09.400 --> 00:30:20.940
And then Kubernetes takes the next level up, which is, oh, if your dependencies for your application is if you have like a microservices classic sort of example, if your application is architected in such a way that you need a lot of services to be running.

00:30:21.280 --> 00:30:29.680
Well, to format that, you need to actually create a cluster of services configured with particular network configuration and various kinds of things.

00:30:30.000 --> 00:30:35.800
So you're actually shipping a cluster as the first thing you land and then you land, you deploy the airport, then you land the plane.

00:30:36.520 --> 00:30:48.580
So if you need to do that, if the thing you're doing is so big, and I think that when you think about the U.S. Air Force and Army, the reason why the American military sort of has the dominance it has is because of the logistics chain.

00:30:48.610 --> 00:30:57.120
They can land just hundreds and hundreds of tons of military hardware and food and personnel into any location on the earth inside of 24 hours.

00:30:57.860 --> 00:31:00.860
And this is sort of what Kubernetes gives you is that ability to format at that level.

00:31:01.240 --> 00:31:11.060
But at the end of the day, if you have a Jupyter Notebook, well-known data set, you know how many CPU cores, what kind of GPU you need to run a particular analytic, that can seem like overkill.

00:31:11.200 --> 00:31:17.760
Because you could say, spin up the CC2 box, get me in there, spin up Jupyter Hub, copy the thing over, and now it's running.

00:31:18.040 --> 00:31:18.480
You know, yay.

00:31:19.180 --> 00:31:21.380
So I don't think that containers are necessary.

00:31:21.880 --> 00:31:24.060
But in life, we don't just do what's necessary, right?

00:31:24.260 --> 00:31:32.620
I think it is useful to know something about how to ship and work with modern IT environments and cloud native kinds of environments.

00:31:32.730 --> 00:31:33.720
So it's a useful thing to know.

00:31:34.510 --> 00:31:42.320
But then, like I said, it's the goal for us as technologists should be empowering those who are less technically inclined than us.

00:31:42.650 --> 00:31:45.840
And so removing the complexity for them should be the thing that we should be trying to do.

00:31:45.850 --> 00:31:48.580
And this is the spirit of what I think Matt Rockland talks to.

00:31:48.980 --> 00:31:59.560
And what we on the sort of Anaconda data science oriented side also hope for, right, is that to make as much of this disappear into the background as possible for people who don't want to learn it, who don't need to know it necessarily.

00:31:59.920 --> 00:32:01.180
Yeah, I think we want to get it.

00:32:01.240 --> 00:32:04.720
We all want to score well on the plays well with others scorecard.

00:32:04.980 --> 00:32:10.480
And so if we can deploy and use containers, that means it's much easier to onboard the next dev.

00:32:10.900 --> 00:32:10.960
Yeah.

00:32:11.040 --> 00:32:14.040
And a lot of this, not everyone has to be an expert at it.

00:32:14.440 --> 00:32:14.580
Correct.

00:32:14.960 --> 00:32:21.900
A couple of people set up a cluster or some Docker compose system together, and then you all get to use it.

00:32:21.980 --> 00:32:28.140
It's a little bit like the people that work on Jupyter have to do a lot of JavaScript and TypeScript, so the rest of us don't have to do so much.

00:32:28.520 --> 00:32:28.620
Right.

00:32:28.910 --> 00:32:31.300
Although you just whipped out a little HTML editing, so I was pretty slick.

00:32:33.680 --> 00:32:40.280
Yeah, I think here's a good question from a little bit earlier from Pamela, but I think especially this one goes out to you, Calvin.

00:32:40.370 --> 00:32:42.400
I think you've walked this path recently.

00:32:43.020 --> 00:32:46.720
How much harder is Kubernetes versus Docker Compose to learn for a web dev?

00:32:47.040 --> 00:32:52.160
I think if you have a good template to start from, that's where this becomes a no-brainer.

00:32:52.530 --> 00:33:03.260
If you were to try and go learn all the things about the Kubernetes stack orchestrators, the storage bits, all these kind of pieces, that could be really overwhelming.

00:33:03.490 --> 00:33:10.400
And whereas Docker Compose, it's one file, it lists your services, it feels fairly readable, It's just YAML.

00:33:11.520 --> 00:33:14.080
Kubernetes is going to have a few more things going on under the covers.

00:33:14.390 --> 00:33:26.520
But again, I'll point to our SCAF example as a minimal, as little as you needed to get going version of being able to do Kubernetes locally and in a sandbox and production environment.

00:33:27.010 --> 00:33:28.620
So it scales up to all those pieces.

00:33:29.120 --> 00:33:31.800
So as a web dev, you just develop your code locally.

00:33:31.960 --> 00:33:32.620
You use your IDE.

00:33:32.880 --> 00:33:33.480
You're in PyCharm.

00:33:33.640 --> 00:33:34.160
You're in VS Code.

00:33:34.880 --> 00:33:36.180
You're editing your files locally.

00:33:36.720 --> 00:33:44.580
Tools like Tilt are kind of hiding a lot of that complexity out under the covers for you and synchronizing files two-way.

00:33:44.860 --> 00:33:54.640
So if things happen in the container, for example, you probably want to be able to build, compile your dependencies with the hashes in the target container environment that you're going to release to.

00:33:55.000 --> 00:34:02.040
Because if you did it locally and you're on Windows or on Mac or on Linux, you're going to get potentially different hashes, different versions of different dependencies.

00:34:02.800 --> 00:34:09.139
So those kinds of things need to write back from the container to your local file system and Tilt enables that and takes that whole pain away.

00:34:09.580 --> 00:34:14.659
I think Tilt was the big changing point for me, the inflection point for me when I moved over

00:34:15.139 --> 00:34:17.200
and fully embraced Kubernetes for local web dev.

00:34:17.700 --> 00:34:17.919
Interesting.

00:34:18.560 --> 00:34:30.919
Over at Talk Python, I've got, I think last time I counted 23 different things running in Docker containers were managed by a handful of Docker Compose things that grew them by what they're related to.

00:34:30.950 --> 00:34:31.919
And it's been awesome.

00:34:32.300 --> 00:34:34.480
It's been, it really lets you isolate things.

00:34:34.860 --> 00:34:37.280
The server doesn't get polluted with installing

00:34:37.419 --> 00:34:39.780
this version of Postgres or that version of Mongo.

00:34:39.879 --> 00:34:41.500
I think I've got two versions of Postgres,

00:34:41.940 --> 00:34:44.500
another version of MongoDB and a few other things.

00:34:44.720 --> 00:34:45.639
Yeah, and it just doesn't matter.

00:34:45.800 --> 00:34:47.200
Do I RAM and CPU for it?

00:34:47.379 --> 00:34:47.520
Plenty.

00:34:47.600 --> 00:34:48.220
Okay, good.

00:34:48.340 --> 00:34:52.600
And you can run in a one CPU or one server node.

00:34:52.720 --> 00:34:57.780
You don't need to have five machines running with a control plane and all the pieces.

00:34:57.880 --> 00:35:06.440
You will have the control plane, You can use like K3S is a minimal Kubernetes project that you can use to deploy, for example, on a single EC2 instance.

00:35:07.000 --> 00:35:08.600
Spin that up, deploy your containers.

00:35:08.900 --> 00:35:12.840
Now you can hook it up to your GitHub actions, which I think we should also talk about as something people should learn.

00:35:13.840 --> 00:35:15.400
You hook that up and away you go.

00:35:15.520 --> 00:35:26.520
You're now releasing without logging into a server and typing git pole and potentially injecting into it unintended changes from your version control.

00:35:27.020 --> 00:35:34.120
I mean, it's a peace of mind to be able to know and audit and know what you've released is exactly what you expected to get released.

00:35:34.960 --> 00:35:38.620
So I want to wrap up this container side of things with two thoughts.

00:35:39.039 --> 00:35:40.720
First, I'm a big fan of dogs.

00:35:41.090 --> 00:35:43.940
I don't know if you guys know, but I kind of understand what dogs say a little bit.

00:35:44.000 --> 00:35:44.460
It's a little weird.

00:35:44.730 --> 00:35:47.880
I believe Calvin's dog, I don't know, Peter, back me up here.

00:35:47.880 --> 00:35:51.460
I believe Calvin's dog said, forget containers I edit in production.

00:35:51.900 --> 00:35:53.540
I think that's what the dog said when it barked.

00:35:53.540 --> 00:35:54.240
I'm not entirely sure.

00:35:54.900 --> 00:35:55.900
I mean, he is a black dog.

00:35:56.480 --> 00:36:02.360
you can only. Yeah, you never know. You never know. They're known for being rebels. That's right.

00:36:02.540 --> 00:36:25.800
Exactly. Not the black sheep, but the black lab. The black dog. And then the second one I want to kind of close this out with is see for yourself out on YouTube says, I like Python for low code ML with PyCaret. The problem is that Python is now up to 313.3 and very soon 314.0 folks, while PyCaret only supports up to 311. And I think this is a good place to touch on reproducibility and isolation, right?

00:36:25.940 --> 00:36:30.080
Like you could make a container that just runs 3.11 and it doesn't matter what your server has, right, Peter?

00:36:30.880 --> 00:36:38.040
Yeah, I mean, the, is the, sorry, if you could pop up the question again, I was, I think it was just that PyCaret, yeah.

00:36:38.640 --> 00:36:44.360
So this, I guess I don't really see the, I don't see the problem.

00:36:45.400 --> 00:36:47.220
Like this is a statement of fact, right?

00:36:47.340 --> 00:36:49.000
The PyCaret only supports 3.11.

00:36:49.580 --> 00:36:57.480
Are there features that you really want to see in 3.13 or that you really need to use in 3.13 or, I mean, there's...

00:36:57.700 --> 00:36:58.880
It could be that they work.

00:36:59.400 --> 00:37:01.380
Yeah, but it could be they get a new Linux machine

00:37:01.920 --> 00:37:02.680
that's Ubuntu

00:37:02.850 --> 00:37:04.040
and it's got 3.12 on it.

00:37:04.220 --> 00:37:05.860
Yeah, that's true.

00:37:05.940 --> 00:37:07.460
But you never...

00:37:07.680 --> 00:37:11.220
Okay, this might be where the dog barks again, but you never use the system Python.

00:37:11.620 --> 00:37:11.900
Well, right.

00:37:13.279 --> 00:37:15.100
It doesn't matter what the system ships with.

00:37:15.220 --> 00:37:16.400
What does macOS ship with?

00:37:16.400 --> 00:37:16.780
I don't know.

00:37:17.939 --> 00:37:27.760
You either install a distribution like Anaconda, Miniconda or something like this or uv using Python standalone, the virtual environments there have their own, ship their own Python.

00:37:28.900 --> 00:37:45.860
This is now, because I am who I am, like on the Anaconda side of things, we've known that you have, in order to really isolate your Python installation, you really have to have the interpreter itself be built with the same tool chain and the same versions of the tool chain as all the libraries within it.

00:37:45.880 --> 00:37:49.920
And so this is what the Conda universe, Conda Forge, BioConda, we've been doing this forever.

00:37:50.720 --> 00:37:56.960
And then with uv, I think uv has really pushed the spearheading the whole like install a separate Python bit.

00:37:57.000 --> 00:38:02.120
I know that Pyan has been there, but like it's not, I don't think it was a standard part of,

00:38:02.340 --> 00:38:04.080
it was considered best practice, right.

00:38:04.440 --> 00:38:05.280
For people.

00:38:05.980 --> 00:38:09.740
But, but I'm hoping that, you know, that, that uv helps to change minds in this way as well.

00:38:09.880 --> 00:38:34.380
But ultimately for, for if you actually, if you do all the bits, right, you actually can have a isolated and separated perfectly isolated Python install without needing to, use containerization. Not that there's anything wrong with containerization, but just sort of saying like, this is a solvable problem. It's just so darn complicated to try to give anyone best practices in the Python packaging world because some guidance can be wrong for somebody, right?

00:38:34.680 --> 00:38:40.860
But in this case, yes, you could absolutely use containers to isolate that or look to use

00:38:41.360 --> 00:38:47.400
conda or uv to create an isolated install with just that version of Python just to run

00:38:47.420 --> 00:39:08.640
then PyCaret inside it. Yeah. I feel like containers is a pure expression of an isolated environment where you can't get it messed up. Like the, if you, if you do anything, just know that the system Python is not your Python. It's near not, you shouldn't be allowed to use it. It should be almost in a user private bin, you know, path that's not usable by people. Calvin, I've been on

00:39:08.640 --> 00:39:18.320
a journey and it's a failed journey, but it was a long, long, solid attempt. I've been trying to remove Python from my system as a global concept, period.

00:39:18.770 --> 00:39:21.960
But I'm a big fan of Homebrew and too many things that Homebrew wanted.

00:39:22.150 --> 00:39:26.360
And I know something's gone wrong when my app falls back to using Python 3.9.

00:39:26.460 --> 00:39:27.540
I'm like, no, Homebrew.

00:39:27.650 --> 00:39:33.140
I deleted all my local Pythons, Pyamv and Homebrew, in any packages that depended on it.

00:39:33.210 --> 00:39:37.480
And I went fully uv and uvx for any tools that would rely on it.

00:39:37.740 --> 00:39:38.820
And we've also moved to Nix.

00:39:39.020 --> 00:39:42.780
We've started using Nix for our package management instead of Homebrew for that reason.

00:39:43.200 --> 00:39:43.360
Okay.

00:39:45.560 --> 00:39:48.060
This portion of Talk Python To Me is brought to you by Agency.

00:39:48.570 --> 00:39:53.720
Build the future of multi-agent software with Agency, spelled A-G-N-T-C-Y.

00:39:54.010 --> 00:39:58.400
Now an open source Linux foundation project, Agency is building the internet of agents.

00:39:58.880 --> 00:40:05.160
Think of it as a collaboration layer where AI agents can discover, connect, and work across any framework.

00:40:05.780 --> 00:40:07.320
Here's what that means for developers.

00:40:07.940 --> 00:40:13.620
The core pieces engineers need to deploy multi-agent systems now belong to everyone who builds on agency.

00:40:13.960 --> 00:40:20.080
You get robust identity and access management, so every agent is authenticated and trusted before it interacts.

00:40:20.700 --> 00:40:33.120
You get open, standardized tools for agent discovery, clean protocols for agent-to-agent communication, and modular components that let you compose scalable workflows instead of wiring up brittle glue code.

00:40:33.560 --> 00:40:35.000
Agency is not a walled garden.

00:40:35.480 --> 00:40:44.100
You'll be contributing alongside developers from Cisco, Dell Technologies, Google Cloud, Oracle, Red Hat, and more than 75 supporting companies.

00:40:44.660 --> 00:40:45.540
The goal is simple.

00:40:45.980 --> 00:40:53.200
Build the next generation of AI infrastructure together in the open so agents can cooperate across tools, vendors, and runtimes.

00:40:53.900 --> 00:40:57.720
Agencies dropping code, specs, and services with no strings attached.

00:40:58.380 --> 00:40:58.820
Sound awesome?

00:40:59.360 --> 00:41:02.940
Well, visit talkpython.fm/agency to contribute.

00:41:03.480 --> 00:41:57.940
that's talkpython.fm/agntcy the link is in your podcast player show notes and on the episode page thank you as always to agency for supporting talk python to me maybe the next one i want to throw out there to talk about is just is uv it's yeah it was compelling when it was uv pip install and uv venv but i think peter really hit the nail on the head when once it's it sort of jujitsu'd Python and said, okay, now here's the deal. We manage Python. Python doesn't manage us. It just uncorked the possibilities, right? Because you can say uv venv and specify a Python version, and it's not even on your machine. Two seconds later, it's both installed on your machine and you have a virtual environment based on it. And you don't have to think about it. You know, Peter, you talked about PyM. It's great, but it compiles Python on a machine that's super slow and error prone. Because if your build tools in your machine aren't quite right,

00:41:57.980 --> 00:42:06.800
then well oh yeah no compiling python is no joke no it isn't i've done i used to do it for a while for talk python in production it it was like a 10 minute deal yeah i had it automated it was fine

00:42:06.820 --> 00:42:11.560
but it took 10 minutes there was no rush and the great you don't need to spend the great irony of

00:42:11.560 --> 00:42:59.240
this is that again like we in the data science world have spent years trying to convince um certain folks in the um sort of non non data and science python world that you can't solve the Python packaging problem without including the management of Python itself in it. And we just got nowhere. We're just repeatedly told PyCon after PyCon, packaging summit after packaging summit. The scope of a Python package manager is to manage the things inside site packages and anything outside of that system libraries, lib ping, lib tiff, you know, open CV, these things are outside of scope. And, you know, many distributions, there's, you know, Linux distros like Debian or Red Hat, there's distribution vendors of like us and a content that are cross-platform.

00:43:00.040 --> 00:43:04.380
We were trying to make the case for this, but we just kept not landing that argument.

00:43:04.740 --> 00:43:05.820
UV comes along and does it.

00:43:06.060 --> 00:43:07.840
And I was like, oh, this is totally the way to do it.

00:43:08.680 --> 00:43:13.620
It's like, well, I guess the users finally have, you know, I think that we can follow, we can pave that cow path.

00:43:14.020 --> 00:43:15.860
And I agree, it is utterly the way to do it.

00:43:16.100 --> 00:43:33.320
And then we're going to learn, I think on the other side of that is, oh, not only is it great to manage Python as part of the whole thing, But now we actually should care how we build that Python, because your choice of clang, GCC, your choice of what particular libraries you link in, that determines the tool chain for compiling everything else as well.

00:43:33.710 --> 00:43:41.680
And especially to talk about data, AI, ML kinds of libraries, there's incompatibilities that will emerge as you try to install this, install that.

00:43:42.060 --> 00:43:46.720
So I gave a talk at PyBay, sorry to sort of toot my own horn a little bit, but I gave

00:43:46.720 --> 00:43:55.880
a talk at PyBay last fall about the five demons of Python packaging, where I try to unravel why is this perennial problem so gnarly and horrible?

00:43:56.960 --> 00:43:58.940
And it's because there's many dimensions of it.

00:43:58.960 --> 00:44:01.760
And most users only care about one and a half of those dimensions.

00:44:02.140 --> 00:44:05.120
They just really want to install the damn package and just use it.

00:44:05.380 --> 00:44:07.520
But if you're a maintainer, that's right.

00:44:08.220 --> 00:44:09.920
You got to have the obligatory Blade Runner.

00:44:11.200 --> 00:44:24.660
And anyway, so I put that talk together just to sort of get everyone on the same page to understand why we have different tools, why distribution vendors, whether it's an Anaconda or an Ubuntu, a Red Hat, or Nix, right?

00:44:24.880 --> 00:44:25.380
Why homebrew?

00:44:25.520 --> 00:44:26.280
These things do matter.

00:44:26.410 --> 00:44:27.980
And there's a reason people depend on these tools.

00:44:29.179 --> 00:44:41.180
And anyway, I hope the people who care about Python packaging or want to understand more deeply go and look at this talk because I do try to put time at each of their own topics that make this so complicated.

00:44:41.520 --> 00:44:45.440
And for Python in particular, because I hear a lot of people talking about, why isn't it as easy as Rust?

00:44:46.020 --> 00:44:48.000
Or, oh, you know, npm is so nice.

00:44:48.500 --> 00:44:49.260
Well, I don't hear that very often.

00:44:49.540 --> 00:44:50.180
Is it?

00:44:50.640 --> 00:44:50.940
No, no, no.

00:44:51.160 --> 00:44:53.120
Actually, I don't hear a lot of praise for MPM.

00:44:53.480 --> 00:44:54.920
Well, like, why doesn't JavaScript have this problem?

00:44:55.000 --> 00:44:58.080
It's like, well, JavaScript doesn't have a pile of Fortran involved in it, right?

00:44:58.800 --> 00:45:01.400
Many people don't know, but, you know, there's a fun thing in there.

00:45:01.400 --> 00:45:09.680
I talk about the fact that if you want to use MB convert, if you want to use MB convert to convert notebook into a PDF, you need a Haskell compiler because Pandoc depends on Haskell.

00:45:10.100 --> 00:45:14.360
So, like, there's just things like that that are just our ecosystem is replete with these things.

00:45:15.040 --> 00:45:21.000
And most users don't have to see it if the upstream folks or the distribution folks and packaging people are doing their jobs right.

00:45:21.310 --> 00:45:22.420
But that doesn't mean that it's not hard.

00:45:22.470 --> 00:45:25.060
It doesn't mean that it's not real labor that goes into making it work.

00:45:25.280 --> 00:45:26.120
Yeah, look in the chat.

00:45:26.420 --> 00:45:31.100
Pamela points out that even using uv, there are now multiple ways, which is tricky.

00:45:31.860 --> 00:45:35.300
And I would refer to myself as one of the old school people.

00:45:35.780 --> 00:45:39.300
I still use uv kind of in a agnostic way.

00:45:39.500 --> 00:46:06.380
Like if people don't want to use uv and they take one of my projects, they can still use pip and they can still use pip-tools and so i'll use things like uv venv or uv pip install or you know pip compile especially to build out the pin requirements but if you don't like it you just pip install -r requirements.txt instead of using what i was doing right and then there's this other way of embracing like let it sort of manage your pyproject.tomal entirely and create it and

00:46:06.570 --> 00:46:15.780
so on so i think there is some a little bit of confusion but i think yeah it's probably good It's good they made that compatibility path, though.

00:46:16.340 --> 00:46:19.060
It helps people be comfortable because change is hard.

00:46:19.590 --> 00:46:21.340
As humans, we don't like change.

00:46:21.700 --> 00:46:22.800
But this is a really good change.

00:46:23.260 --> 00:46:26.020
That speed is a feature that Charlie talks about.

00:46:26.380 --> 00:46:28.140
It's 100% I'm on board.

00:46:28.540 --> 00:46:29.320
Yeah, I agree.

00:46:29.540 --> 00:46:31.520
And it's changed even my Docker stuff.

00:46:32.000 --> 00:46:36.980
Now, one of my Docker layers is just uv Python install.

00:46:37.260 --> 00:46:40.240
Really, I think it just creates a virtual environment, which also installs the Python.

00:46:40.460 --> 00:46:42.200
That's a two second, one time deal.

00:46:42.540 --> 00:46:43.540
And it's after the races.

00:46:43.740 --> 00:46:44.400
It's really, really nice.

00:46:44.800 --> 00:46:44.960
All right.

00:46:45.180 --> 00:46:47.660
We have probably time for a few more topics.

00:46:48.070 --> 00:46:54.700
However, if I put this out into the world, it may consume all of the time as it does pretty much all of the GPUs.

00:46:55.820 --> 00:46:59.360
What are your thoughts on Agenda coding?

00:46:59.640 --> 00:47:00.480
What are your thoughts on them?

00:47:00.840 --> 00:47:05.440
On LLMs and Agenda coding AI and that whole soup of craziness.

00:47:05.940 --> 00:47:09.980
I'm shocked how many people are not diving in headfirst on this.

00:47:10.340 --> 00:47:13.720
I literally started talking to some developer last week.

00:47:14.280 --> 00:47:16.340
And I was like, hey, we tried Claude Code.

00:47:16.860 --> 00:47:18.380
And they were like, no, what's that?

00:47:18.560 --> 00:47:19.520
I was like, oh my.

00:47:19.530 --> 00:47:19.660
What?

00:47:20.220 --> 00:47:20.880
Yeah, exactly.

00:47:21.460 --> 00:47:22.560
Well, we've got Copilot.

00:47:22.780 --> 00:47:24.180
I think the issue is in the enterprise,

00:47:24.640 --> 00:47:31.120
a lot of people have opted to purchase Copilot because it's a checkbox and a one-click purchase.

00:47:31.310 --> 00:47:31.780
So it's easy.

00:47:32.220 --> 00:47:35.100
But they're not giving them Copilot Studio, which is the agentic version of it.

00:47:35.320 --> 00:47:36.940
They're just like, yeah, you've got your LLMs now.

00:47:37.520 --> 00:47:38.120
Go have fun.

00:47:38.540 --> 00:47:45.520
I think they're really missing out on the true power of like a tool that can inspect your file system, a tool that can like look at things and do actions.

00:47:45.860 --> 00:47:47.540
Now, obviously that introduces risk.

00:47:47.870 --> 00:47:51.740
So a lot of these security people in these environments are not excited about that level of risk.

00:47:52.640 --> 00:48:00.620
I don't have a good answer for that other than if you're a developer and you're going to turn on energetic coding, you kind of have to like sign up and be accountable for what it's going to do.

00:48:00.880 --> 00:48:03.600
I've got some ideas and some concrete recommendations for you.

00:48:03.700 --> 00:48:05.540
But Peter, I want to hear what you have to say first.

00:48:06.180 --> 00:48:11.760
So first of all, I think vibe coding is simultaneously oversold at the same time.

00:48:13.000 --> 00:48:15.900
I'm very bullish on where this can go.

00:48:15.900 --> 00:48:25.180
The ultimately the Transformers models and that style of current era AI has some structural mathematical limitations.

00:48:25.560 --> 00:48:31.660
The recent open AI paper about hallucinations are inevitable and sort of part of the math sort of shows that, yeah, we're going to end up.

00:48:32.000 --> 00:48:35.100
It is to some extent glorious high dimensional autocomplete.

00:48:35.360 --> 00:48:37.180
But oh my God, is it glorious when it's right.

00:48:37.640 --> 00:48:38.480
So it is steerable.

00:48:38.780 --> 00:48:43.420
It's like trying to fly a very, very awkward airplane before we've really figured out aerodynamics.

00:48:43.820 --> 00:48:44.920
But it kind of still does work.

00:48:45.420 --> 00:48:50.600
So people should absolutely 100% be looking at what this can do for them.

00:48:50.980 --> 00:49:00.880
And thinking really right now, like I would say actually the limitations, the known, the visible limitations of vibe coding should actually, you should be grateful for that.

00:49:01.020 --> 00:49:05.960
because that gives us time and space to think about how would we design projects?

00:49:06.840 --> 00:49:13.040
Because I know for myself, the way I code is I write doc strings and comments and sort of class structures first.

00:49:13.580 --> 00:49:16.860
And then I think about what needs to play with what and you're writing documentation.

00:49:17.420 --> 00:49:22.300
And if I can just have the code itself just get filled out with that, like, holy crap, like, of course, right?

00:49:22.400 --> 00:49:29.140
So everyone should be doing this so they can think about it and really think about where this stuff will go because it's definitely going to get better.

00:49:29.560 --> 00:49:35.380
But if you're worried about the data leakage and the compliance and all this other stuff, use local models.

00:49:35.560 --> 00:49:38.240
Go and buy expense a couple of GPUs.

00:49:38.340 --> 00:49:40.740
3090s actually work fine with the newer, smaller models.

00:49:41.200 --> 00:49:45.480
If you work for a richer employer, maybe you can get a couple of 5090s.

00:49:46.200 --> 00:49:47.280
Sacrifice a gaming PC.

00:49:47.580 --> 00:49:47.740
Come on.

00:49:48.560 --> 00:49:49.660
It's also a gaming PC.

00:49:49.760 --> 00:49:50.840
It's also a gaming PC.

00:49:51.160 --> 00:49:52.440
An M4 Mac with 64.

00:49:52.640 --> 00:49:54.500
I have an M4 Mac with 64 gig of RAM.

00:49:54.840 --> 00:49:55.320
And it's wonderful.

00:49:55.600 --> 00:49:56.700
I've got DevStroll running.

00:49:56.900 --> 00:49:59.140
I've got the OSS GPT running.

00:49:59.460 --> 00:50:07.480
all those tools run on a on just yeah base model on base model i have mac yeah i have a 32 gig

00:50:08.020 --> 00:50:17.500
mac mini running here and i'm running the 20 billion parameter open ai model on it just to be shared with all my computers my laptop yeah and there's and there's um there's there's also

00:50:17.700 --> 00:50:37.180
you know the chinese models are um really freaking good you know and and the i mean i think i don't know we'll see what happens with ces next year but i feel like this year was the year of small models. This year was the year, I mean, we started the year with DeepSeq, right? And so it's like, not just Chinese labs saying, we don't need your stinking whatever. But over the course of the

00:50:37.280 --> 00:50:41.620
year, we got Kimi, we got Quinn, we got GLM, we got, we're just going to keep getting these.

00:50:41.760 --> 00:50:49.400
And that's not even, that's just on the code and the text prompting side. That's not even on image generation. So the Chinese image and video generation models are just jaw-droppingly good.

00:50:49.640 --> 00:51:15.320
So I think what we're going to see here is by the beginning of next year, well, this is a 25 slash 26 podcast, right? So in 26, you probably have no excuse to say, why are you not, you know, like you're, you're, you know, professional CAD and engineering people have big workstations as a dev. Maybe you just have a big workstation now, or a fat 128 gig, you know, a unified memory for Mac, but like, you're just going to have that as your coding station and everything is local.

00:51:15.720 --> 00:51:19.100
You're going to be careful with tool use, of course, but still like you just run all locally.

00:51:19.500 --> 00:51:25.200
I think as a developer, one of the key skills you should learn is going to be context engineering

00:51:25.640 --> 00:51:27.800
and using sub processes.

00:51:28.700 --> 00:51:32.800
The models now support basically spinning off parallel instances of themselves.

00:51:33.000 --> 00:51:39.780
And you can spin off parallel instances with a limited amount of context to kind of really shape how they understand things.

00:51:39.940 --> 00:51:45.860
Because Google introduced the Gemini with like a 1 million token context window limit.

00:51:46.260 --> 00:51:46.700
So what?

00:51:46.800 --> 00:51:47.600
What are you going to do with that?

00:51:47.700 --> 00:51:53.960
it's really not useful to just feed a million tokens into it because it can't, it just as much as you try to like stuff your brain.

00:51:53.960 --> 00:51:55.660
Well, it tapers off at the end as well.

00:51:55.740 --> 00:51:57.520
It's not really a million tokens.

00:51:57.740 --> 00:51:58.640
Right, you don't get a million tokens.

00:51:58.750 --> 00:52:02.680
And it's also, it's just going to be thoroughly confused by all the context you just threw at it.

00:52:02.880 --> 00:52:07.780
But if you can give a really narrow focus context, small diffs, that's one of the things I liked about AiderChat.

00:52:08.000 --> 00:52:10.420
If you've not checked out AiderChat, it has a diff mode

00:52:10.780 --> 00:52:12.720
that really limits the amount of tokens it consumes.

00:52:12.900 --> 00:52:21.380
So actually it's a little more efficient on tokens than like cloud code, even if you're using the anthropic models the same way, because it'll do diffs and send smaller context.

00:52:21.860 --> 00:52:32.060
And if you can leverage that with like sub models or sub prompts and Goose, the chat agent from Block has recipes that actually operate in like a sub model.

00:52:32.220 --> 00:52:43.860
So it's basically like you're building your own little tools that are just descriptions of like what MCP pieces it should use, what tools should be available and use this context and only pass me back that bit and throw away the extra context once you're done.

00:52:44.020 --> 00:52:48.540
So you're not polluting your context window with a whole bunch of unneeded operation.

00:52:48.980 --> 00:52:50.480
And now you get back really what's needed

00:52:50.660 --> 00:52:51.820
for whatever you're trying to work on.

00:52:52.060 --> 00:52:52.220
Yeah.

00:52:52.940 --> 00:52:55.680
So I want to kind of take it a little bit higher level back real quick.

00:52:55.920 --> 00:52:56.560
How about I'm with you?

00:52:56.700 --> 00:53:08.560
If you have not seen this, and I've talked to a lot of really smart devs who are like, yeah, I tried Copilot or I tried one of these things and their experience is largely, I think with the multi-line autocomplete.

00:53:08.980 --> 00:53:10.700
And to me, that, I just turn that off.

00:53:10.780 --> 00:53:11.260
That's like garbage.

00:53:12.160 --> 00:53:15.400
I mean, it's not garbage, But it's, I'll put it, let me put it more kindly.

00:53:15.660 --> 00:53:18.720
Like half of the time hitting tab is glorious.

00:53:19.320 --> 00:53:22.900
And the other half, I'm like, I want the first half, but the second half is wrong.

00:53:23.110 --> 00:53:25.220
So do I hit tab and then go down and delete it again?

00:53:25.500 --> 00:53:25.940
Like, you know what I mean?

00:53:25.970 --> 00:53:28.700
I got to like, it's giving me too much and it's not quite right.

00:53:29.100 --> 00:53:32.380
But the agentic tool using part is stunning.

00:53:32.900 --> 00:53:36.340
Not with the cheap models, but with the models that cost like $20 a month.

00:53:36.660 --> 00:53:38.840
It's a huge difference from the very cheap model to like.

00:53:38.880 --> 00:53:41.500
Which is like, that's not even a latte a week, right?

00:53:41.640 --> 00:53:46.020
Like just like we're talking to an audience of probably mostly professional developers, right?

00:53:46.340 --> 00:53:46.680
Yes.

00:53:47.020 --> 00:53:53.700
You know, a hundred bucks a month, $200 a month for what literally is transforming the future of your entire industry is worth it.

00:53:53.820 --> 00:53:56.740
Like, why would you not subscribe to your employer and your employer should be paying for this?

00:53:56.880 --> 00:53:58.320
Like they should be handing you all.

00:53:58.600 --> 00:53:59.440
Well, but if they do actually.

00:53:59.740 --> 00:54:00.300
So here's the thing.

00:54:00.440 --> 00:54:01.360
I'm actually two minds of this.

00:54:01.680 --> 00:54:08.540
I think every dev for their own purposes, for their own application, you're paying for their own because the employers will have limitations on what they're allowed to use.

00:54:08.900 --> 00:54:14.140
They may have to sign up for an enterprise thing, which has then data, you know, data retention policies, yada, yada, yada.

00:54:14.520 --> 00:54:16.000
And you want to just go full blast.

00:54:16.110 --> 00:54:19.360
What is absolute cutting edge being released by various things?

00:54:19.520 --> 00:54:19.640
Yes.

00:54:19.900 --> 00:54:26.680
But I would still, again, my little, you know, nerd like open source heart would not be stated unless I've made the comment here.

00:54:27.020 --> 00:54:28.360
Please play with local models.

00:54:28.840 --> 00:54:32.560
Please like have do work in a data sovereignty mode.

00:54:33.600 --> 00:54:46.920
Because this is actually the closest, the first time I think we've had real tech that could potentially move people away from a centralized computing model, which has been, I think, so deleterious to our world, actually.

00:54:47.300 --> 00:55:00.200
And the last thing that we don't have time for, but the last thing I was going to just throw a shout out for was for people to check out Beware, because that is the way that we can build Python mobile applications and really be shipping applications that don't necessarily, like, we should be deploying to mobile.

00:55:00.880 --> 00:55:07.520
so many Python developers are web devs, storing state in the Postgres somewhere, and we're part of that data concentration, data gravity problem.

00:55:08.079 --> 00:55:13.060
Whereas if we flip the bit and just learn for ourselves, how do we Vibecode an actual iOS platformer?

00:55:13.380 --> 00:55:14.620
Like, let's go do that, right?

00:55:14.680 --> 00:55:16.220
Or an Android thing, which is a little bit easier to deal with.

00:55:16.980 --> 00:55:18.480
These are things that we can actually do as Python.

00:55:18.580 --> 00:55:19.080
Totally doable, though, yeah.

00:55:19.260 --> 00:55:19.940
Yeah, totally doable.

00:55:20.260 --> 00:55:27.060
I want to give a shout out to you, Peter, and Anaconda in general for all the support for Beware and some of the PyScript and some of those other projects.

00:55:27.300 --> 00:55:29.680
Those are important ones, and yeah, good work.

00:55:30.020 --> 00:55:30.420
Yeah, thank you.

00:55:30.440 --> 00:55:31.080
Start to fight the good fight.

00:55:31.520 --> 00:55:32.060
Yeah, for sure.

00:55:32.210 --> 00:55:32.440
Thank you.

00:55:32.960 --> 00:55:34.760
I'm not quite done with this AI thing, though.

00:55:35.000 --> 00:55:42.160
I do want to say, I do want to point out this thing called Klein that recently came out that's really pretty interesting.

00:55:42.540 --> 00:55:43.380
Have you guys heard of this?

00:55:43.740 --> 00:55:43.880
Yep.

00:55:44.200 --> 00:55:44.280
Yep.

00:55:44.490 --> 00:55:44.600
Yep.

00:55:44.760 --> 00:55:44.820
Yeah.

00:55:45.060 --> 00:55:46.100
So it's open source.

00:55:46.280 --> 00:55:47.080
It's kind of like Cursor.

00:55:47.560 --> 00:55:49.660
But the big difference is they don't charge for inference.

00:55:49.770 --> 00:55:54.280
You just put in an API key or you put in a link to a URL to a local model.

00:55:54.680 --> 00:55:55.620
So you use local with it.

00:55:55.740 --> 00:55:55.860
Yeah.

00:55:56.200 --> 00:55:56.300
Yeah.

00:55:56.500 --> 00:55:56.720
Yeah.

00:55:57.060 --> 00:56:07.560
I recommend if you're using local models and you really want to go all in on the data sovereignty pieces, use tools like Little Snitch on your Mac to know if it's sending something someplace you didn't request it to send to.

00:56:07.790 --> 00:56:16.540
You can be totally eyes wide open and maybe exercise a little more reckless abandon if you know that a tool like that can catch an outbound connection that you didn't expect.

00:56:16.820 --> 00:56:21.180
Yeah, I think I'll give you how many, how much I know what I'm doing on this.

00:56:21.420 --> 00:56:32.200
I will give you guys an example that I think probably will, if, let me put it this way, if you've done a lot of web development and web design mix, this will probably catch your attention.

00:56:33.500 --> 00:56:37.820
So I want to add some new features to talkpython.fm.

00:56:37.820 --> 00:56:47.140
I got some cool whole sections coming and announcements, but talkpython.fm was originally created and designed in 2015 on Bootstrap.

00:56:47.480 --> 00:56:52.980
Do you know how out of date 2015 Bootstrap is with modern day front end frameworks a lot.

00:56:53.520 --> 00:56:58.820
But there's like 10,000 lines of HTML designed in Bootstrap, early Bootstrap.

00:57:00.660 --> 00:57:02.740
It still renders great on my phone though.

00:57:02.900 --> 00:57:06.700
And the LLMs are very aware of old Bootstrap documentation and issues.

00:57:07.880 --> 00:57:10.360
Peter, it looks great and it works well, but here's the thing.

00:57:10.440 --> 00:57:13.060
I want to add a whole bunch of new features and sections to it.

00:57:13.120 --> 00:57:14.560
And I've got to design that from scratch.

00:57:14.700 --> 00:57:16.900
I'm like, oh, I can't do this in Bootstrap 3.

00:57:17.040 --> 00:57:18.520
I just don't have the willpower for it.

00:57:19.080 --> 00:57:21.020
It's going to make it so hard, you know?

00:57:21.640 --> 00:57:25.500
And so I'm like, well, I really should redesign it, but that's got to be weeks of work.

00:57:25.790 --> 00:57:33.980
And one evening around four o'clock, I'm just hanging out, you know, enjoying the outside, sitting, working on my computer, trying to take in a little more summer before it's gone.

00:57:34.080 --> 00:57:34.620
And I'm like, you know what?

00:57:34.620 --> 00:57:58.600
I bet, I bet, Claude Sonnet and I bet we could do this quicker than two hours later, the entire site, 5,000 lines of CSS, 10,000 lines of template, HTML files, all rewritten in Bulma. Modern, clean, doesn't look at all different except for the few parts. I'm like, oh, I don't like that. Rewrite that actually. To the point where you just take a screenshot of what you do want, throw it in there and go, make it look like this. Oh yeah. Okay. I see the picture.

00:57:58.760 --> 00:58:08.780
Let's make it look like that. And it's just a couple hours. That would be pulling your hair out the most tedious, painful work for a week or two. And now it's, if I want to add something to the

00:58:08.920 --> 00:58:13.059
site, it's just, oh yeah, it's just modern Bulma. Off it goes. Or I could have chose Tailwind or

00:58:13.060 --> 00:58:24.240
whatever. I think Bulma works a little better with AIs because it doesn't have build steps and all that kind of stuff. It's a little more straightforward. But those are the kinds of things that like literally I wrote down a markdown plan. I said, here's what we're going to do.

00:58:24.550 --> 00:58:28.260
And I planned it out with AI. Then I said, okay, step one, step two. And then we just worked it

00:58:28.310 --> 00:58:31.540
through till it was done. There's a few little glitches. I'm like, this looks weird. Here's a

00:58:31.580 --> 00:58:36.140
screenshot. Fix it. Okay. AI is really good at these kinds of tasks. Yeah. And if people have

00:58:36.250 --> 00:58:50.580
not seen this in action, I think it just doesn't. They're like, I tried to use ChatGPT and it gave me an answer, but it doesn't help that much. I could write that. Or I used a free cheap model and it got it wrong and I had to fix more than it helped me. There are these tools that are crazy.

00:58:51.200 --> 00:59:09.880
There's something that people don't, I think, have an intuitive feeling for because they're encountering a cognitive reactive system for the first time. I'm not saying sentient or conscious, by the way, but just cognitive. And so it's going to be as deep as how you probe it.

00:59:09.920 --> 00:59:13.000
So if you ask it a dumb, shallow thing, it will give you a dumb, shallow response.

00:59:13.740 --> 00:59:19.540
If you, you know, but if you get really deep or nerdy, and I was using it to, I was using early incarnations actually a couple of years back.

00:59:19.590 --> 00:59:23.920
I remember when I first figured out this effect, I was reading some philosophy books, as one does.

00:59:24.240 --> 00:59:26.980
And I was thinking, well, I could use this as a co-reading tutor.

00:59:27.540 --> 00:59:32.120
And I noticed I would just ask for some reason, give me some summaries and like, well, that's reasonable, but you know, okay, whatever.

00:59:32.720 --> 00:59:42.520
But then I still got deeper into some of the content and I was asking for contrasting opinions from different other perspectives and some critiques and all this stuff, and I started getting into it, it would go very deep.

00:59:42.780 --> 00:59:46.400
And this is like GPT just 4.0, it just come out kind of thing, like timeframe.

00:59:47.040 --> 00:59:49.940
So I think the same thing is true now, especially with like GPT-5 research.

00:59:50.300 --> 00:59:57.440
I've had feedback from friends who are like, yeah, some people say 5.0 is a nothing burger, but 5.0 research is a thing.

00:59:57.800 --> 01:00:00.440
Because I'm able to do, this is this other person, not me,

01:00:00.560 --> 01:00:07.700
but this other person saying, quote, I'm able to get graduate level feedback, like stuff that is deeply researched in arcane parts of mathematics.

01:00:08.360 --> 01:00:09.100
And I check it.

01:00:09.120 --> 01:00:11.280
And I mean, I use Claude to check the GPT-5.

01:00:11.920 --> 01:00:14.620
And it basically is correct as far as I can tell.

01:00:14.960 --> 01:00:21.280
So I think the thing to go to these people with is like, if you're not getting anything out of it, it's because you're not squeezing hard enough, right?

01:00:21.400 --> 01:00:23.560
Approach it as if it were a super intelligence

01:00:23.960 --> 01:00:26.020
and see how little it disappoints you.

01:00:26.340 --> 01:00:29.000
Because it will not disappoint you that often if you really get into it.

01:00:29.200 --> 01:00:32.280
Yeah, I want to take a slightly different take, but I 100% agree.

01:00:32.760 --> 01:00:40.120
I think you should treat it a little bit like a junior developer who knows 80% of what you want, but he's kind of guessing that last 20%.

01:00:40.820 --> 01:00:50.280
And if you gave this work to a junior dev and they got it 95% wrong and there's a little mistake and you had to go and say, hey, really good, but this part you got to fix up a little bit.

01:00:50.560 --> 01:00:52.440
That would be a success for that junior developer.

01:00:52.700 --> 01:00:52.900
Yeah.

01:00:53.210 --> 01:00:55.460
I don't know why we expect 100% perfection.

01:00:55.650 --> 01:01:01.180
If there's any kind of flaw whatsoever from such a creation process, they're like, well, it's broken.

01:01:01.320 --> 01:01:01.660
It's a joke.

01:01:02.160 --> 01:01:08.640
You're expected to make a few mistakes and you've got to be there to guide it, but the huge amount it gets right is so valuable.

01:01:08.960 --> 01:01:12.560
This doesn't negate the standard software development lifecycle process of having code review.

01:01:13.040 --> 01:01:19.440
You still need to have those kinds of things in place and the code review is you with your junior developer who's an LLM now.

01:01:19.740 --> 01:01:25.680
Well, yeah, the SCLC isn't negated, but the thing I think that's deeply counterintuitive is we're used to, I mean, the modality.

01:01:25.880 --> 01:01:28.020
Think about how this manifests.

01:01:28.240 --> 01:01:30.760
We're typing things still into a text window, right?

01:01:31.100 --> 01:01:36.740
And so we as developers are used to that being a very precise, predictable input, output, transformational process.

01:01:37.840 --> 01:01:41.360
We're not used to the idea of coding with a semantic paintbrush, right?

01:01:41.460 --> 01:01:46.940
Like a Chinese or Japanese calligrapher doesn't care exactly which horsehair got ink on which part of the paper.

01:01:47.400 --> 01:01:49.120
They got a brush and they're like doing their calligraphy.

01:01:49.460 --> 01:01:56.940
And I think we have to get over ourselves and think about I'm painting with a semantic paintbrush, splattering it, certainly using my fingers with keyboard.

01:01:57.060 --> 01:01:58.480
But soon it'll be dictation, right?

01:01:58.840 --> 01:02:05.200
And so we're really splattering ideas into this canvas and it's auto rendering the stuff for us into a formal system.

01:02:05.600 --> 01:02:12.880
And I think just the modality of, wow, you can see the clouds are going over the sun and like my color temperature changes in my video.

01:02:14.040 --> 01:02:15.040
It's the AI doing it.

01:02:15.360 --> 01:02:17.520
The AI is doing it because I'm getting passionate about this, right.

01:02:18.620 --> 01:02:20.300
So no, but I think that's the key thing.

01:02:20.340 --> 01:02:29.720
We are used to this modality of fingers on keyboard textual input being an input to a formal system, not an informal probabilistic system, which is what these things are.

01:02:30.000 --> 01:02:34.580
So once you make that mental bit flip, then it's like you just learn to embrace it, right?

01:02:34.860 --> 01:02:36.620
Yeah, I think voice is a great option here.

01:02:37.760 --> 01:02:40.980
We use Fireflies for our meeting recording bot.

01:02:41.460 --> 01:02:44.980
You can also just open up your phone and launch the Fireflies app and start talking to it.

01:02:45.160 --> 01:02:46.200
And it has an MCP server.

01:02:46.600 --> 01:03:01.680
So you can go into Claude Code and be like, grab the last transcript where I was just talking about this and pull it in or have a discussion about the specifications, about the journey, the epic, the customer's story, and bring those in as artifacts really, really quickly now.

01:03:02.020 --> 01:03:03.140
Yeah. Older than ballgame.

01:03:03.680 --> 01:03:04.240
It is a crazy ballgame.

01:03:04.240 --> 01:03:05.540
That's what I learned. It's a whole new ballgame.

01:03:05.880 --> 01:03:06.100
Yeah.

01:03:07.440 --> 01:03:13.020
All right. Anything else that is burning on your list of topics that we should do a lightning round because we're out of time on?

01:03:13.140 --> 01:03:14.500
We should lightning round on DuckDB.

01:03:15.660 --> 01:03:20.560
Okay, you two riffed on it because I'm knowledgeable, but you all are the ones who use it.

01:03:20.780 --> 01:03:28.260
If you've not played with it, it is an incredible little embedded, like kind of SQLite, but way more.

01:03:28.960 --> 01:03:32.000
And if you've got files on a disk someplace, they're now your database.

01:03:32.460 --> 01:03:36.280
If you've got stuff in an S3 bucket someplace, that's now your database.

01:03:36.840 --> 01:03:38.200
It's incredibly flexible.

01:03:38.440 --> 01:03:40.620
It's got so many cool extensions built into it.

01:03:40.740 --> 01:03:42.060
It can do geospatial stuff.

01:03:42.320 --> 01:03:45.340
It's got JSON capabilities that are really incredible.

01:03:45.800 --> 01:04:16.120
I mean the speed is a little bit mind-blowing it's kind of like the first time you use uv or rough like how is that so fast and then you use duck db and it's it's really I think folks should go check it out and learn a little more because it's it may change how you think about deploying a an at edge thing or a little local thing or even a big data analysis piece you may actually be able to fit that into memory on your machine and duck db and get some incredible results out of it I'm sure Peter has way more to talk about this than I do, but I don't use it that much.

01:04:16.190 --> 01:04:19.780
But man, if I had a use case for it, I would be 100% picking that tool up.

01:04:20.060 --> 01:04:22.400
Yeah, it's a fantastic little piece of technology.

01:04:23.040 --> 01:04:33.800
I don't mean little in a pejorative sense here, but at a technical level, I would say it is a highly portable, very efficient and very versatile database engine.

01:04:34.300 --> 01:04:39.160
So the name is almost wrong because it's exactly it liberates you from databases.

01:04:39.560 --> 01:04:47.880
We are used to thinking of databases at places where data goes to, well, not die, but to be housed at rest and have an extreme amount of gravity attracted to it.

01:04:47.940 --> 01:04:50.220
And then DuckDB takes the opposite of that.

01:04:50.260 --> 01:04:58.420
It says any data representation you have should be searchable or queryable if only you had the right engine.

01:04:58.980 --> 01:05:03.620
And it inverts the whole thing, which is the brilliant piece of it.

01:05:05.000 --> 01:05:08.400
And again, what data isn't just representation.

01:05:08.540 --> 01:05:11.080
It's somewhere on a disk or over a network or a memory.

01:05:11.560 --> 01:05:14.380
So it pairs very nicely with the PyData stack of tools.

01:05:15.300 --> 01:05:18.660
And so I know one of the topics we had on here as well was Arrow.

01:05:19.020 --> 01:05:22.800
So if you care about representation for a variety of reasons, then Arrow is great.

01:05:23.180 --> 01:05:32.080
If you want a query interface, you want a SQL-style query interface that's agnostic as to representation, that's your DuckDB.

01:05:32.700 --> 01:05:40.940
And of course, the fact that it plays so well with WebAssembly means Edge, you know, worker, Cloudflare workers or whatever, or PyScript and WebAssembly workers.

01:05:41.370 --> 01:05:48.540
You know, we have some we have some demonstration examples using PyScript where you have an entire analytics stack running entirely within the browser full on.

01:05:48.870 --> 01:05:53.140
You know, you got pandas and psychic image, scikit-learn, you know, map, hot lip stuff going on.

01:05:53.460 --> 01:06:00.660
And you've got you're hitting S3 buckets through with query with real full blown SQL queries using DuckDB because it all runs on WebAssembly.

01:06:01.060 --> 01:06:29.920
and this is just a taste i mean none of this is mainstream yet i think some of these use cases are a little bit on the edge but the vision this takes us to as a world where we really are just we were living a much more portable world so your fees can just move and give someone a web page a static web page it's a full-blown app and actually if you look at web gpu and transformers js web lm kinds of stuff you can fit a little tiny model in there actually and you have a totally local entirely client-side experience with AI in it.

01:06:30.120 --> 01:06:32.100
So I'm very excited about this.

01:06:32.280 --> 01:06:33.860
And DuckDB is really part of that equation.

01:06:33.890 --> 01:06:35.980
Yeah, bring your query engine to where your data is.

01:06:36.490 --> 01:06:36.600
Exactly.

01:06:37.020 --> 01:06:38.700
That way around, which always takes time.

01:06:39.520 --> 01:06:40.340
Yeah, excellent.

01:06:41.440 --> 01:06:42.860
I know people are very excited about it.

01:06:42.980 --> 01:06:46.320
It's got the built-in your program.

01:06:46.410 --> 01:06:49.780
You don't have to run another server aspect, which I think is good as well.

01:06:49.920 --> 01:07:00.580
And the WebAssembly stuff, maybe there won't be local DB and local SQL or WebSQL, all those things that we can just do DuckDB in the browser with WebAssembly.

01:07:00.940 --> 01:07:01.260
Be nice.

01:07:01.700 --> 01:07:03.300
So very interesting.

01:07:03.780 --> 01:07:05.680
We barely scratched the surface, you guys.

01:07:05.960 --> 01:07:07.960
Like there's more people need to know,

01:07:08.100 --> 01:07:10.920
but I think these are probably some of the hotter topics.

01:07:11.760 --> 01:07:14.500
We may have to do a part two, but a 2026 edition.

01:07:14.740 --> 01:07:15.720
That's just a continuation.

01:07:16.120 --> 01:07:21.620
But if people take the time, invest in putting some energy into these things, it's going to make a big difference, I think.

01:07:22.340 --> 01:07:23.060
Thanks for being on the show.

01:07:23.520 --> 01:07:24.920
And yeah, it's been great.

01:07:25.200 --> 01:07:25.740
Yeah, this was awesome.

01:07:25.980 --> 01:07:26.680
Thank you so much for having us.

01:07:27.050 --> 01:07:27.500
Yeah, thanks, Michael.

01:07:27.610 --> 01:07:29.600
I enjoy talking about all the cool new tech and tools.

01:07:29.960 --> 01:07:30.060
Yep.

01:07:30.270 --> 01:07:30.520
Bye, guys.

01:07:31.660 --> 01:07:33.800
This has been another episode of Talk Python To Me.

01:07:34.400 --> 01:07:35.080
Thank you to our sponsors.

01:07:35.480 --> 01:07:36.860
Be sure to check out what they're offering.

01:07:37.140 --> 01:07:38.460
It really helps support the show.

01:07:39.360 --> 01:07:40.740
Take some stress out of your life.

01:07:41.120 --> 01:07:46.560
Get notified immediately about errors and performance issues in your web or mobile applications with Sentry.

01:07:47.060 --> 01:07:51.440
Just visit talkpython.fm/sentry and get started for free.

01:07:51.910 --> 01:07:55.100
And be sure to use the promo code TALKPYTHON, all one word.

01:07:55.760 --> 01:08:23.080
Agency. Discover agentic AI with agency. Their layer lets agents find, connect, and work together, any stack, anywhere. Start building the internet of agents at talkpython.fm/agency, spelled A-G-N-T-C-Y. If you or your team needs to learn Python, we have over 270 hours of beginner and advanced courses on topics ranging from complete beginners to async code, Flask, Django, HTMLX, and even LLMs.

01:08:23.580 --> 01:08:25.660
Best of all, there's not a subscription in sight.

01:08:26.120 --> 01:08:27.920
Browse the catalog at talkpython.fm.

01:08:28.339 --> 01:08:29.740
Be sure to subscribe to the show.

01:08:30.180 --> 01:08:34.220
Open your favorite podcast player app, search for Python, we should be right at the top.

01:08:34.640 --> 01:08:38.140
If you enjoy the Geeky Rap theme song, you can download the full track.

01:08:38.359 --> 01:08:39.980
The link is your podcast player's show notes.

01:08:40.600 --> 01:08:41.900
This is your host, Michael Kennedy.

01:08:42.359 --> 01:08:43.500
Thank you so much for listening.

01:08:43.660 --> 01:08:44.520
I really appreciate it.

01:08:44.980 --> 01:08:46.580
Now get out there and write some Python code.

01:08:58.420 --> 01:09:01.220
I'm out.

